{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c1f4172",
   "metadata": {},
   "source": [
    "1. Extract semantic chunks from your JSON (e.g., abstract, claim, paragraph).\n",
    "\n",
    "2. Use HuggingFaceEmbeddings or OpenAIEmbeddings.\n",
    "\n",
    "3. Store structured info as metadata.\n",
    "\n",
    "4. Use metadata filtering + vector similarity for retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f62f4251-df00-493c-b405-361abc6ada42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Current directory: /app/notebooks\n",
      "üìÅ Project root: /app\n",
      "üìÅ Source directory: /app/src\n",
      "‚úÖ Python paths configured\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "import json\n",
    "\n",
    "# Add source directories to Python path\n",
    "current_dir = Path.cwd()\n",
    "project_root = current_dir.parent  # Go up one level from notebooks to project root\n",
    "src_dir = project_root / \"src\"\n",
    "\n",
    "# Add paths\n",
    "sys.path.append(str(src_dir / \"data_pipline\"))\n",
    "sys.path.append(str(src_dir / \"EU_XML_data_loader\"))\n",
    "\n",
    "\n",
    "from data_pipline import DataPipeline\n",
    "import get_raw_data_paths_EPO  \n",
    "from xml_loader_EPO import process_xml_files_list\n",
    "\n",
    "\n",
    "print(f\"üìÅ Current directory: {current_dir}\")\n",
    "print(f\"üìÅ Project root: {project_root}\")\n",
    "print(f\"üìÅ Source directory: {src_dir}\")\n",
    "print(f\"‚úÖ Python paths configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d61903a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import os\n",
    "from langchain.docstore.document import Document\n",
    "import sys\n",
    "# Prepare .json file for embedding\n",
    "def extract_documents(json_data):\n",
    "    bibliographic = json_data.get(\"bibliographic_data\", {})\n",
    "    doc_id = bibliographic.get(\"doc_id\", \"UNKNOWN\")\n",
    "    documents = []\n",
    "\n",
    "    # Common metadata to propagate\n",
    "    common_meta = {\n",
    "        \"doc_id\": doc_id,\n",
    "        \"language\": bibliographic.get(\"language\"),\n",
    "        \"country\": bibliographic.get(\"country\"),\n",
    "        \"doc_number\": bibliographic.get(\"doc_number\"),\n",
    "        \"application_number\": bibliographic.get(\"application_number\"),\n",
    "        \"publication_date\": bibliographic.get(\"publication_date\"),\n",
    "        \"ipc_classes\": bibliographic.get(\"ipc_classes\", []),\n",
    "    }\n",
    "\n",
    "    # Title (en preferred)\n",
    "    title_dict = bibliographic.get(\"title\", {})\n",
    "    title = title_dict.get(\"en\") or next(iter(title_dict.values()), \"\")\n",
    "    if title:\n",
    "        documents.append(Document(\n",
    "            page_content=title,\n",
    "            metadata={**common_meta, \"section\": \"title\"}\n",
    "        ))\n",
    "\n",
    "    # Abstract\n",
    "    abstract = bibliographic.get(\"abstract\")\n",
    "    if abstract:\n",
    "        documents.append(Document(\n",
    "            page_content=abstract,\n",
    "            metadata={**common_meta, \"section\": \"abstract\"}\n",
    "        ))\n",
    "\n",
    "    # Claims\n",
    "    for claim in json_data.get(\"claims\", []):\n",
    "        documents.append(Document(\n",
    "            page_content=claim[\"text\"],\n",
    "            metadata={**common_meta, \"section\": \"claim\", \"claim_number\": claim.get(\"claim_number\")}\n",
    "        ))\n",
    "\n",
    "    # Main sections\n",
    "    for section in json_data.get(\"main_sections\", []):\n",
    "        section_name = section.get(\"heading_text\", \"UNKNOWN_SECTION\")\n",
    "        for p in section.get(\"paragraphs\", []):\n",
    "            documents.append(Document(\n",
    "                page_content=f\"{section_name}\\n{p['text']}\",\n",
    "                metadata={**common_meta, \"section\": section_name, \"p_id\": p.get(\"p_id\")}\n",
    "            ))\n",
    "\n",
    "    return documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df7f0996-5e36-4059-b57e-f6d559a8bc3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully imported from data_config.py\n",
      "üîç Testing JSON File Loading Functions\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Test the new JSON loading functionality\n",
    "sys.path.append(str(src_dir / \"data_pipline\" / \"json_loader\"))\n",
    "\n",
    "# Import the JSON loader functions\n",
    "from json_loader_epo import get_epo_json_file_paths, get_all_json_file_paths, load_json_documents\n",
    "\n",
    "print(\"üîç Testing JSON File Loading Functions\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ed2e254-9f1e-442b-8555-f090f5edcc7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Found 1286 EPO JSON files\n",
      "Total chunks:  2160\n",
      "Chunks:  page_content='AUDIO SIGNAL ENCODER' metadata={'doc_id': 'EP13899497B9W1', 'language': 'en', 'country': 'EP', 'doc_number': '3084761', 'application_number': '13899497.5', 'publication_date': '20250611', 'ipc_classes': ['G10L  19/038       20130101AFI20170426BHEP', 'G10L  19/07        20130101ALI20170426BHEP'], 'section': 'title', 'start_index': 0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "os.environ[\"TRANSFORMERS_HTTP_TIMEOUT\"] = \"60\"\n",
    "# load the .json documents\n",
    "file_list = get_epo_json_file_paths()[:50]\n",
    "# file_list = glob.glob(json_files_path)\n",
    "\n",
    "all_documents = []\n",
    "\n",
    "# Preprocessing documents\n",
    "for file_path in file_list:\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "        docs = extract_documents(data)\n",
    "        all_documents.extend(docs)\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=50,\n",
    "    add_start_index=True\n",
    "    )\n",
    "all_chunks = splitter.split_documents(all_documents)\n",
    "print(\"Total chunks: \", len(all_chunks))\n",
    "print(\"Chunks: \", all_chunks[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3c90d553-59ac-470e-ad5b-d462c74dce6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 13:58:28,105 - INFO - Use pytorch device_name: cpu\n",
      "2025-06-18 13:58:28,107 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "vectors = []\n",
    "\n",
    "for chunk in all_chunks:\n",
    "    vector = embeddings.embed_query(chunk.page_content)\n",
    "    vectors.append(vector)\n",
    "\n",
    "# for i, vector in enumerate(vectors[:5]):\n",
    "#     print(f\"Vector {i}: {vector}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bd4f2b97-a068-4979-b64f-369868b9b37d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Summary\n",
      "There is provided according to a first aspect an apparatus according to claim 1.' metadata={'section': 'Summary', 'country': 'EP', 'start_index': 0, 'application_number': '17876867.7', 'language': 'en', 'publication_date': '20250611', 'doc_id': 'EP17876867B1', 'p_id': 'p0009', 'doc_number': '3549354'}\n",
      "Score: 0.893546998500824\n",
      "\n",
      "page_content='Definition of the invention\n",
      "The composition is preferably fluid.' metadata={'publication_date': '20250611', 'p_id': 'p0046', 'country': 'EP', 'doc_number': '4440377', 'doc_id': 'EP22818205B1', 'application_number': '22818205.1', 'section': 'Definition of the invention', 'start_index': 0, 'language': 'en'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_community.vectorstores.utils import filter_complex_metadata\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"example_collection\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not necessary\n",
    ")\n",
    "\n",
    "vectorstore = Chroma.from_documents(filter_complex_metadata(all_chunks), embeddings)\n",
    "\n",
    "\n",
    "ids = vector_store.add_documents(documents=all_chunks)\n",
    "\n",
    "results = vector_store.similarity_search(\"How many paper has written about APPARATUS?\")\n",
    "print(results[0])\n",
    "\n",
    "results = vector_store.similarity_search_with_score(\"Which paper writes this: The invention relates to a substance determining apparatus and substance determining method for determining a substance within a fluid. The invention relates further to a binding device and an analyzing device for cooperating with each other for determining a substance within a fluid, an analyzing method for determining a substance within a fluid and an analyzing computer program for determining a substance within a fluid.\")\n",
    "doc, score = results[0]\n",
    "print(f\"Score: {score}\\n\")\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5fb8e6-9daf-429c-b567-65608e800b6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
