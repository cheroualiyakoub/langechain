{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a1ddad5-ac63-4327-a9c3-27a27b662745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Testing JSON File Loading Functions\n",
      "==================================================\n",
      "ðŸ“ Current directory: /app/notebooks\n",
      "ðŸ“ Project root: /app\n",
      "ðŸ“ Source directory: /app/src\n",
      "âœ… Python paths configured\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "import json\n",
    "import math\n",
    "\n",
    "# Add source directories to Python path\n",
    "current_dir = Path.cwd()\n",
    "project_root = current_dir.parent  # Go up one level from notebooks to project root\n",
    "src_dir = project_root / \"src\"\n",
    "\n",
    "# Add paths - FIXED to include embeddings directory\n",
    "sys.path.append(str(src_dir / \"data_pipline\"))\n",
    "sys.path.append(str(src_dir / \"EU_XML_data_loader\"))\n",
    "sys.path.append(str(src_dir / \"embeddings\"))  # Add this line to include embeddings\n",
    "\n",
    "# Other imports\n",
    "from data_pipline import DataPipeline\n",
    "import get_raw_data_paths_EPO  \n",
    "from xml_loader_EPO import process_xml_files_list\n",
    "\n",
    "# Import the embeddings module\n",
    "from embeddings import DocumentEmbedder, batch_process_json_files, extract_documents_epo\n",
    "\n",
    "# Import the JSON loader functions\n",
    "sys.path.append(str(src_dir / \"data_pipline\" / \"json_loader\"))\n",
    "from json_loader_epo import get_epo_json_file_paths, get_all_json_file_paths, load_json_documents\n",
    "\n",
    "print(\"ðŸ” Testing JSON File Loading Functions\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"ðŸ“ Current directory: {current_dir}\")\n",
    "print(f\"ðŸ“ Project root: {project_root}\")\n",
    "print(f\"ðŸ“ Source directory: {src_dir}\")\n",
    "print(f\"âœ… Python paths configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc7fce6e-67c5-4779-b85f-d54dfd2b0ef8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac1b61a8-94ba-4c57-a7f0-0556712491e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ Found 1286 EPO JSON files\n"
     ]
    }
   ],
   "source": [
    "file_list = get_epo_json_file_paths()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c97eb600-a962-4094-bdc9-b5b29c72fc5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 15:08:03,762 - INFO - Use pytorch device_name: cpu\n",
      "2025-06-18 15:08:03,763 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (691 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1/1\n",
      "Loaded 180 documents from /app/data/parsed/EPO/EPRTBJV2025000024001001/EPW1B9/EP13899497W1B9/EP13899497W1B9.json\n",
      "Loaded 16 documents from /app/data/parsed/EPO/EPRTBJV2025000024001001/EPW1B9/EP22169662W1B9/EP22169662W1B9.json\n",
      "Loaded 2 documents from /app/data/parsed/EPO/EPRTBJV2025000024001001/EPW1A8/EP23166881W1A8/EP23166881W1A8.json\n",
      "Loaded 2 documents from /app/data/parsed/EPO/EPRTBJV2025000024001001/EPW1A8/EP23205649W1A8/EP23205649W1A8.json\n",
      "Loaded 16 documents from /app/data/parsed/EPO/EPRTBJV2025000024001001/EPW1A9/EP23204356W1A9/EP23204356W1A9.json\n",
      "Loaded 111 documents from /app/data/parsed/EPO/EPRTBJV2025000024001001/EPW1A9/EP24185369W1A9/EP24185369W1A9.json\n",
      "Loaded 1 documents from /app/data/parsed/EPO/EPRTBJV2025000024001001/EPW1B8/EP18784075W1B8/EP18784075W1B8.json\n",
      "Loaded 1 documents from /app/data/parsed/EPO/EPRTBJV2025000024001001/EPW1B8/EP22732225W1B8/EP22732225W1B8.json\n",
      "Loaded 1 documents from /app/data/parsed/EPO/EPRTBJV2025000024001001/EPW1B8/EP21207842W1B8/EP21207842W1B8.json\n",
      "Loaded 1 documents from /app/data/parsed/EPO/EPRTBJV2025000024001001/EPW1B8/EP21728115W1B8/EP21728115W1B8.json\n",
      "Added 331 documents. Total documents to process: 331\n",
      "Starting tokenization...\n",
      "Tokenized text length: 5 tokens\n",
      "Processing chunk at position 0/5\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 691 tokens\n",
      "Processing chunk at position 0/691\n",
      "Processing chunk at position 314/691\n",
      "Processing chunk at position 628/691\n",
      "Created 3 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 38 tokens\n",
      "Processing chunk at position 0/38\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 59 tokens\n",
      "Processing chunk at position 0/59\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 82 tokens\n",
      "Processing chunk at position 0/82\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 791 tokens\n",
      "Processing chunk at position 0/791\n",
      "Processing chunk at position 232/791\n",
      "Processing chunk at position 464/791\n",
      "Processing chunk at position 696/791\n",
      "Created 4 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 129 tokens\n",
      "Processing chunk at position 0/129\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 66 tokens\n",
      "Processing chunk at position 0/66\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 82 tokens\n",
      "Processing chunk at position 0/82\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 33 tokens\n",
      "Processing chunk at position 0/33\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 33 tokens\n",
      "Processing chunk at position 0/33\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 116 tokens\n",
      "Processing chunk at position 0/116\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 37 tokens\n",
      "Processing chunk at position 0/37\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 88 tokens\n",
      "Processing chunk at position 0/88\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 30 tokens\n",
      "Processing chunk at position 0/30\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 30 tokens\n",
      "Processing chunk at position 0/30\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 74 tokens\n",
      "Processing chunk at position 0/74\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 25 tokens\n",
      "Processing chunk at position 0/25\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 40 tokens\n",
      "Processing chunk at position 0/40\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 53 tokens\n",
      "Processing chunk at position 0/53\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 47 tokens\n",
      "Processing chunk at position 0/47\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 18 tokens\n",
      "Processing chunk at position 0/18\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 96 tokens\n",
      "Processing chunk at position 0/96\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 109 tokens\n",
      "Processing chunk at position 0/109\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 106 tokens\n",
      "Processing chunk at position 0/106\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 35 tokens\n",
      "Processing chunk at position 0/35\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 112 tokens\n",
      "Processing chunk at position 0/112\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 102 tokens\n",
      "Processing chunk at position 0/102\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 32 tokens\n",
      "Processing chunk at position 0/32\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 231 tokens\n",
      "Processing chunk at position 0/231\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 81 tokens\n",
      "Processing chunk at position 0/81\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 38 tokens\n",
      "Processing chunk at position 0/38\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 77 tokens\n",
      "Processing chunk at position 0/77\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 162 tokens\n",
      "Processing chunk at position 0/162\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 64 tokens\n",
      "Processing chunk at position 0/64\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 22 tokens\n",
      "Processing chunk at position 0/22\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 27 tokens\n",
      "Processing chunk at position 0/27\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 76 tokens\n",
      "Processing chunk at position 0/76\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 121 tokens\n",
      "Processing chunk at position 0/121\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 50 tokens\n",
      "Processing chunk at position 0/50\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 155 tokens\n",
      "Processing chunk at position 0/155\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 19 tokens\n",
      "Processing chunk at position 0/19\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 29 tokens\n",
      "Processing chunk at position 0/29\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 84 tokens\n",
      "Processing chunk at position 0/84\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 84 tokens\n",
      "Processing chunk at position 0/84\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 91 tokens\n",
      "Processing chunk at position 0/91\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 83 tokens\n",
      "Processing chunk at position 0/83\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 21 tokens\n",
      "Processing chunk at position 0/21\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 35 tokens\n",
      "Processing chunk at position 0/35\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 144 tokens\n",
      "Processing chunk at position 0/144\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 52 tokens\n",
      "Processing chunk at position 0/52\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 24 tokens\n",
      "Processing chunk at position 0/24\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 50 tokens\n",
      "Processing chunk at position 0/50\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 22 tokens\n",
      "Processing chunk at position 0/22\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 32 tokens\n",
      "Processing chunk at position 0/32\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 80 tokens\n",
      "Processing chunk at position 0/80\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 22 tokens\n",
      "Processing chunk at position 0/22\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 17 tokens\n",
      "Processing chunk at position 0/17\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 21 tokens\n",
      "Processing chunk at position 0/21\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 13 tokens\n",
      "Processing chunk at position 0/13\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 88 tokens\n",
      "Processing chunk at position 0/88\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 170 tokens\n",
      "Processing chunk at position 0/170\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 30 tokens\n",
      "Processing chunk at position 0/30\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 123 tokens\n",
      "Processing chunk at position 0/123\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 71 tokens\n",
      "Processing chunk at position 0/71\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 95 tokens\n",
      "Processing chunk at position 0/95\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 80 tokens\n",
      "Processing chunk at position 0/80\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 380 tokens\n",
      "Processing chunk at position 0/380\n",
      "Processing chunk at position 158/380\n",
      "Processing chunk at position 316/380\n",
      "Created 3 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 100 tokens\n",
      "Processing chunk at position 0/100\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 111 tokens\n",
      "Processing chunk at position 0/111\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 119 tokens\n",
      "Processing chunk at position 0/119\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 64 tokens\n",
      "Processing chunk at position 0/64\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 142 tokens\n",
      "Processing chunk at position 0/142\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 14 tokens\n",
      "Processing chunk at position 0/14\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 85 tokens\n",
      "Processing chunk at position 0/85\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 23 tokens\n",
      "Processing chunk at position 0/23\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 18 tokens\n",
      "Processing chunk at position 0/18\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 19 tokens\n",
      "Processing chunk at position 0/19\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 21 tokens\n",
      "Processing chunk at position 0/21\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 16 tokens\n",
      "Processing chunk at position 0/16\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 36 tokens\n",
      "Processing chunk at position 0/36\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 33 tokens\n",
      "Processing chunk at position 0/33\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 47 tokens\n",
      "Processing chunk at position 0/47\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 41 tokens\n",
      "Processing chunk at position 0/41\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 25 tokens\n",
      "Processing chunk at position 0/25\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 33 tokens\n",
      "Processing chunk at position 0/33\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 53 tokens\n",
      "Processing chunk at position 0/53\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 123 tokens\n",
      "Processing chunk at position 0/123\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 15 tokens\n",
      "Processing chunk at position 0/15\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 28 tokens\n",
      "Processing chunk at position 0/28\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 42 tokens\n",
      "Processing chunk at position 0/42\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 35 tokens\n",
      "Processing chunk at position 0/35\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 24 tokens\n",
      "Processing chunk at position 0/24\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 79 tokens\n",
      "Processing chunk at position 0/79\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 51 tokens\n",
      "Processing chunk at position 0/51\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 147 tokens\n",
      "Processing chunk at position 0/147\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 82 tokens\n",
      "Processing chunk at position 0/82\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 76 tokens\n",
      "Processing chunk at position 0/76\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 63 tokens\n",
      "Processing chunk at position 0/63\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 34 tokens\n",
      "Processing chunk at position 0/34\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 64 tokens\n",
      "Processing chunk at position 0/64\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 23 tokens\n",
      "Processing chunk at position 0/23\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 13 tokens\n",
      "Processing chunk at position 0/13\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 26 tokens\n",
      "Processing chunk at position 0/26\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 37 tokens\n",
      "Processing chunk at position 0/37\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 30 tokens\n",
      "Processing chunk at position 0/30\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 46 tokens\n",
      "Processing chunk at position 0/46\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 45 tokens\n",
      "Processing chunk at position 0/45\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 60 tokens\n",
      "Processing chunk at position 0/60\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 38 tokens\n",
      "Processing chunk at position 0/38\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 30 tokens\n",
      "Processing chunk at position 0/30\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 50 tokens\n",
      "Processing chunk at position 0/50\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 27 tokens\n",
      "Processing chunk at position 0/27\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 18 tokens\n",
      "Processing chunk at position 0/18\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 53 tokens\n",
      "Processing chunk at position 0/53\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 128 tokens\n",
      "Processing chunk at position 0/128\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 72 tokens\n",
      "Processing chunk at position 0/72\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 63 tokens\n",
      "Processing chunk at position 0/63\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 59 tokens\n",
      "Processing chunk at position 0/59\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 114 tokens\n",
      "Processing chunk at position 0/114\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 90 tokens\n",
      "Processing chunk at position 0/90\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 28 tokens\n",
      "Processing chunk at position 0/28\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 150 tokens\n",
      "Processing chunk at position 0/150\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 36 tokens\n",
      "Processing chunk at position 0/36\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 40 tokens\n",
      "Processing chunk at position 0/40\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 34 tokens\n",
      "Processing chunk at position 0/34\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 88 tokens\n",
      "Processing chunk at position 0/88\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 81 tokens\n",
      "Processing chunk at position 0/81\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 61 tokens\n",
      "Processing chunk at position 0/61\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 76 tokens\n",
      "Processing chunk at position 0/76\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 65 tokens\n",
      "Processing chunk at position 0/65\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 53 tokens\n",
      "Processing chunk at position 0/53\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 20 tokens\n",
      "Processing chunk at position 0/20\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 67 tokens\n",
      "Processing chunk at position 0/67\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 18 tokens\n",
      "Processing chunk at position 0/18\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 19 tokens\n",
      "Processing chunk at position 0/19\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 34 tokens\n",
      "Processing chunk at position 0/34\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 65 tokens\n",
      "Processing chunk at position 0/65\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 32 tokens\n",
      "Processing chunk at position 0/32\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 30 tokens\n",
      "Processing chunk at position 0/30\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 90 tokens\n",
      "Processing chunk at position 0/90\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 83 tokens\n",
      "Processing chunk at position 0/83\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 21 tokens\n",
      "Processing chunk at position 0/21\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 40 tokens\n",
      "Processing chunk at position 0/40\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 40 tokens\n",
      "Processing chunk at position 0/40\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 20 tokens\n",
      "Processing chunk at position 0/20\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 26 tokens\n",
      "Processing chunk at position 0/26\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 38 tokens\n",
      "Processing chunk at position 0/38\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 20 tokens\n",
      "Processing chunk at position 0/20\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 35 tokens\n",
      "Processing chunk at position 0/35\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 28 tokens\n",
      "Processing chunk at position 0/28\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 27 tokens\n",
      "Processing chunk at position 0/27\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 27 tokens\n",
      "Processing chunk at position 0/27\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 46 tokens\n",
      "Processing chunk at position 0/46\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 23 tokens\n",
      "Processing chunk at position 0/23\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 26 tokens\n",
      "Processing chunk at position 0/26\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 38 tokens\n",
      "Processing chunk at position 0/38\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 19 tokens\n",
      "Processing chunk at position 0/19\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 58 tokens\n",
      "Processing chunk at position 0/58\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 22 tokens\n",
      "Processing chunk at position 0/22\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 38 tokens\n",
      "Processing chunk at position 0/38\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 50 tokens\n",
      "Processing chunk at position 0/50\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 27 tokens\n",
      "Processing chunk at position 0/27\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 33 tokens\n",
      "Processing chunk at position 0/33\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 67 tokens\n",
      "Processing chunk at position 0/67\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 47 tokens\n",
      "Processing chunk at position 0/47\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 80 tokens\n",
      "Processing chunk at position 0/80\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 146 tokens\n",
      "Processing chunk at position 0/146\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 104 tokens\n",
      "Processing chunk at position 0/104\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 33 tokens\n",
      "Processing chunk at position 0/33\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 47 tokens\n",
      "Processing chunk at position 0/47\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 33 tokens\n",
      "Processing chunk at position 0/33\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 170 tokens\n",
      "Processing chunk at position 0/170\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 104 tokens\n",
      "Processing chunk at position 0/104\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 133 tokens\n",
      "Processing chunk at position 0/133\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 72 tokens\n",
      "Processing chunk at position 0/72\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 116 tokens\n",
      "Processing chunk at position 0/116\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 29 tokens\n",
      "Processing chunk at position 0/29\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 134 tokens\n",
      "Processing chunk at position 0/134\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 79 tokens\n",
      "Processing chunk at position 0/79\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 5 tokens\n",
      "Processing chunk at position 0/5\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 479 tokens\n",
      "Processing chunk at position 0/479\n",
      "Processing chunk at position 208/479\n",
      "Processing chunk at position 416/479\n",
      "Created 3 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 149 tokens\n",
      "Processing chunk at position 0/149\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 98 tokens\n",
      "Processing chunk at position 0/98\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 142 tokens\n",
      "Processing chunk at position 0/142\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 148 tokens\n",
      "Processing chunk at position 0/148\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 83 tokens\n",
      "Processing chunk at position 0/83\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 103 tokens\n",
      "Processing chunk at position 0/103\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 92 tokens\n",
      "Processing chunk at position 0/92\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 94 tokens\n",
      "Processing chunk at position 0/94\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 60 tokens\n",
      "Processing chunk at position 0/60\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 62 tokens\n",
      "Processing chunk at position 0/62\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 69 tokens\n",
      "Processing chunk at position 0/69\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 43 tokens\n",
      "Processing chunk at position 0/43\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 51 tokens\n",
      "Processing chunk at position 0/51\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 84 tokens\n",
      "Processing chunk at position 0/84\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 15 tokens\n",
      "Processing chunk at position 0/15\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 72 tokens\n",
      "Processing chunk at position 0/72\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 12 tokens\n",
      "Processing chunk at position 0/12\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 255 tokens\n",
      "Processing chunk at position 0/255\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 13 tokens\n",
      "Processing chunk at position 0/13\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 219 tokens\n",
      "Processing chunk at position 0/219\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 172 tokens\n",
      "Processing chunk at position 0/172\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 18 tokens\n",
      "Processing chunk at position 0/18\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 19 tokens\n",
      "Processing chunk at position 0/19\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 194 tokens\n",
      "Processing chunk at position 0/194\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 12 tokens\n",
      "Processing chunk at position 0/12\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 12 tokens\n",
      "Processing chunk at position 0/12\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 12 tokens\n",
      "Processing chunk at position 0/12\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 7 tokens\n",
      "Processing chunk at position 0/7\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 12 tokens\n",
      "Processing chunk at position 0/12\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 452 tokens\n",
      "Processing chunk at position 0/452\n",
      "Processing chunk at position 194/452\n",
      "Processing chunk at position 388/452\n",
      "Created 3 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 65 tokens\n",
      "Processing chunk at position 0/65\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 76 tokens\n",
      "Processing chunk at position 0/76\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 63 tokens\n",
      "Processing chunk at position 0/63\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 32 tokens\n",
      "Processing chunk at position 0/32\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 20 tokens\n",
      "Processing chunk at position 0/20\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 56 tokens\n",
      "Processing chunk at position 0/56\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 54 tokens\n",
      "Processing chunk at position 0/54\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 39 tokens\n",
      "Processing chunk at position 0/39\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 44 tokens\n",
      "Processing chunk at position 0/44\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 47 tokens\n",
      "Processing chunk at position 0/47\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 142 tokens\n",
      "Processing chunk at position 0/142\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 38 tokens\n",
      "Processing chunk at position 0/38\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 548 tokens\n",
      "Processing chunk at position 0/548\n",
      "Processing chunk at position 242/548\n",
      "Processing chunk at position 484/548\n",
      "Created 3 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 1773 tokens\n",
      "Processing chunk at position 0/1773\n",
      "Processing chunk at position 264/1773\n",
      "Processing chunk at position 528/1773\n",
      "Processing chunk at position 792/1773\n",
      "Processing chunk at position 1056/1773\n",
      "Processing chunk at position 1320/1773\n",
      "Processing chunk at position 1584/1773\n",
      "Created 7 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 27 tokens\n",
      "Processing chunk at position 0/27\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 23 tokens\n",
      "Processing chunk at position 0/23\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 42 tokens\n",
      "Processing chunk at position 0/42\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 40 tokens\n",
      "Processing chunk at position 0/40\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 58 tokens\n",
      "Processing chunk at position 0/58\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 50 tokens\n",
      "Processing chunk at position 0/50\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 35 tokens\n",
      "Processing chunk at position 0/35\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 70 tokens\n",
      "Processing chunk at position 0/70\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 103 tokens\n",
      "Processing chunk at position 0/103\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 134 tokens\n",
      "Processing chunk at position 0/134\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 105 tokens\n",
      "Processing chunk at position 0/105\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 110 tokens\n",
      "Processing chunk at position 0/110\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 66 tokens\n",
      "Processing chunk at position 0/66\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 56 tokens\n",
      "Processing chunk at position 0/56\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 51 tokens\n",
      "Processing chunk at position 0/51\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 47 tokens\n",
      "Processing chunk at position 0/47\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 79 tokens\n",
      "Processing chunk at position 0/79\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 118 tokens\n",
      "Processing chunk at position 0/118\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 73 tokens\n",
      "Processing chunk at position 0/73\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 48 tokens\n",
      "Processing chunk at position 0/48\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 85 tokens\n",
      "Processing chunk at position 0/85\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 62 tokens\n",
      "Processing chunk at position 0/62\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 110 tokens\n",
      "Processing chunk at position 0/110\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 28 tokens\n",
      "Processing chunk at position 0/28\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 58 tokens\n",
      "Processing chunk at position 0/58\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 1840 tokens\n",
      "Processing chunk at position 0/1840\n",
      "Processing chunk at position 275/1840\n",
      "Processing chunk at position 550/1840\n",
      "Processing chunk at position 825/1840\n",
      "Processing chunk at position 1100/1840\n",
      "Processing chunk at position 1375/1840\n",
      "Processing chunk at position 1650/1840\n",
      "Created 7 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 553 tokens\n",
      "Processing chunk at position 0/553\n",
      "Processing chunk at position 245/553\n",
      "Processing chunk at position 490/553\n",
      "Created 3 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 108 tokens\n",
      "Processing chunk at position 0/108\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 129 tokens\n",
      "Processing chunk at position 0/129\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 127 tokens\n",
      "Processing chunk at position 0/127\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 56 tokens\n",
      "Processing chunk at position 0/56\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 56 tokens\n",
      "Processing chunk at position 0/56\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 176 tokens\n",
      "Processing chunk at position 0/176\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 36 tokens\n",
      "Processing chunk at position 0/36\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 14 tokens\n",
      "Processing chunk at position 0/14\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 159 tokens\n",
      "Processing chunk at position 0/159\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 23 tokens\n",
      "Processing chunk at position 0/23\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 64 tokens\n",
      "Processing chunk at position 0/64\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 90 tokens\n",
      "Processing chunk at position 0/90\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 165 tokens\n",
      "Processing chunk at position 0/165\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 72 tokens\n",
      "Processing chunk at position 0/72\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 21 tokens\n",
      "Processing chunk at position 0/21\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 179 tokens\n",
      "Processing chunk at position 0/179\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 61 tokens\n",
      "Processing chunk at position 0/61\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 72 tokens\n",
      "Processing chunk at position 0/72\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 30 tokens\n",
      "Processing chunk at position 0/30\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 180 tokens\n",
      "Processing chunk at position 0/180\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 82 tokens\n",
      "Processing chunk at position 0/82\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 49 tokens\n",
      "Processing chunk at position 0/49\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 260 tokens\n",
      "Processing chunk at position 0/260\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 228 tokens\n",
      "Processing chunk at position 0/228\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 142 tokens\n",
      "Processing chunk at position 0/142\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 78 tokens\n",
      "Processing chunk at position 0/78\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 67 tokens\n",
      "Processing chunk at position 0/67\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 107 tokens\n",
      "Processing chunk at position 0/107\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 87 tokens\n",
      "Processing chunk at position 0/87\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 181 tokens\n",
      "Processing chunk at position 0/181\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 72 tokens\n",
      "Processing chunk at position 0/72\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 125 tokens\n",
      "Processing chunk at position 0/125\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 55 tokens\n",
      "Processing chunk at position 0/55\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 99 tokens\n",
      "Processing chunk at position 0/99\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 125 tokens\n",
      "Processing chunk at position 0/125\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 102 tokens\n",
      "Processing chunk at position 0/102\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 194 tokens\n",
      "Processing chunk at position 0/194\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 135 tokens\n",
      "Processing chunk at position 0/135\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 135 tokens\n",
      "Processing chunk at position 0/135\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 131 tokens\n",
      "Processing chunk at position 0/131\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 31 tokens\n",
      "Processing chunk at position 0/31\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 47 tokens\n",
      "Processing chunk at position 0/47\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 181 tokens\n",
      "Processing chunk at position 0/181\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 24 tokens\n",
      "Processing chunk at position 0/24\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 83 tokens\n",
      "Processing chunk at position 0/83\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 31 tokens\n",
      "Processing chunk at position 0/31\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 46 tokens\n",
      "Processing chunk at position 0/46\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 134 tokens\n",
      "Processing chunk at position 0/134\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 141 tokens\n",
      "Processing chunk at position 0/141\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 97 tokens\n",
      "Processing chunk at position 0/97\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 95 tokens\n",
      "Processing chunk at position 0/95\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 98 tokens\n",
      "Processing chunk at position 0/98\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 125 tokens\n",
      "Processing chunk at position 0/125\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 151 tokens\n",
      "Processing chunk at position 0/151\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 69 tokens\n",
      "Processing chunk at position 0/69\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 65 tokens\n",
      "Processing chunk at position 0/65\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 65 tokens\n",
      "Processing chunk at position 0/65\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 208 tokens\n",
      "Processing chunk at position 0/208\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 8 tokens\n",
      "Processing chunk at position 0/8\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 57 tokens\n",
      "Processing chunk at position 0/57\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 104 tokens\n",
      "Processing chunk at position 0/104\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 70 tokens\n",
      "Processing chunk at position 0/70\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 144 tokens\n",
      "Processing chunk at position 0/144\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 184 tokens\n",
      "Processing chunk at position 0/184\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 121 tokens\n",
      "Processing chunk at position 0/121\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 74 tokens\n",
      "Processing chunk at position 0/74\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 90 tokens\n",
      "Processing chunk at position 0/90\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 43 tokens\n",
      "Processing chunk at position 0/43\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 48 tokens\n",
      "Processing chunk at position 0/48\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 86 tokens\n",
      "Processing chunk at position 0/86\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 68 tokens\n",
      "Processing chunk at position 0/68\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 41 tokens\n",
      "Processing chunk at position 0/41\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 27 tokens\n",
      "Processing chunk at position 0/27\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 96 tokens\n",
      "Processing chunk at position 0/96\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 7 tokens\n",
      "Processing chunk at position 0/7\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 2 tokens\n",
      "Processing chunk at position 0/2\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 5 tokens\n",
      "Processing chunk at position 0/5\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "Starting tokenization...\n",
      "Tokenized text length: 12 tokens\n",
      "Processing chunk at position 0/12\n",
      "Created 1 chunks\n",
      "  - Processing batch 1/1\n",
      "âœ“ Embedding completed\n",
      "âœ… Processed 358 document chunks from 331 documents\n"
     ]
    }
   ],
   "source": [
    "Documents = batch_process_json_files(file_list, extract_documents_epo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d248908e-e3b7-4aa0-8820-5c19d8d5f0df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
