{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afcd255c",
   "metadata": {},
   "source": [
    "# Document Processing with LangChain\n",
    "\n",
    "This notebook defines and uses a class to process files, chunk them, generate embeddings, and store them in a structured format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ff3dae",
   "metadata": {},
   "source": [
    "## Define the Document Processor Class\n",
    "\n",
    "We'll create a class that handles:\n",
    "1. Loading documents from file paths\n",
    "2. Chunking documents into smaller pieces\n",
    "3. Creating embeddings for chunks\n",
    "4. Storing processed chunks and their embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c583db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Dict, Any, Optional, Union\n",
    "import pandas as pd\n",
    "\n",
    "from langchain_community.document_loaders import (\n",
    "    TextLoader, \n",
    "    PyPDFLoader, \n",
    "    CSVLoader,\n",
    "    UnstructuredMarkdownLoader\n",
    ")\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "class DocumentProcessor:\n",
    "    \"\"\"\n",
    "    A class to process documents by loading, chunking, embedding, and storing them.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, file_paths: List[str] = None, embedding_model: str = \"openai\"):\n",
    "        \"\"\"\n",
    "        Initialize the DocumentProcessor with file paths and embedding model choice.\n",
    "        \n",
    "        Args:\n",
    "            file_paths: List of paths to documents that should be processed\n",
    "            embedding_model: Type of embedding model to use ('openai' or 'huggingface')\n",
    "        \"\"\"\n",
    "        self.file_paths = file_paths or []\n",
    "        self.documents = []\n",
    "        self.chunks = []\n",
    "        self.embedding_model_name = embedding_model\n",
    "        self.embeddings = None\n",
    "        self._setup_embeddings()\n",
    "        \n",
    "    def _setup_embeddings(self):\n",
    "        \"\"\"Set up the embedding model based on the configuration.\"\"\"\n",
    "        if self.embedding_model_name == \"openai\":\n",
    "            self.embeddings = OpenAIEmbeddings()\n",
    "        elif self.embedding_model_name == \"huggingface\":\n",
    "            self.embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported embedding model: {self.embedding_model_name}\")\n",
    "    \n",
    "    def add_file_path(self, file_path: str):\n",
    "        \"\"\"Add a file path to the processor.\"\"\"\n",
    "        if os.path.exists(file_path):\n",
    "            self.file_paths.append(file_path)\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "            \n",
    "    def load_documents(self):\n",
    "        \"\"\"Load documents from file paths.\"\"\"\n",
    "        documents = []\n",
    "        \n",
    "        for file_path in self.file_paths:\n",
    "            try:\n",
    "                if file_path.endswith('.txt'):\n",
    "                    loader = TextLoader(file_path)\n",
    "                elif file_path.endswith('.pdf'):\n",
    "                    loader = PyPDFLoader(file_path)\n",
    "                elif file_path.endswith('.csv'):\n",
    "                    loader = CSVLoader(file_path)\n",
    "                elif file_path.endswith('.md'):\n",
    "                    loader = UnstructuredMarkdownLoader(file_path)\n",
    "                else:\n",
    "                    print(f\"Unsupported file type: {file_path}\")\n",
    "                    continue\n",
    "                    \n",
    "                file_documents = loader.load()\n",
    "                for doc in file_documents:\n",
    "                    doc.metadata['source_file'] = os.path.basename(file_path)\n",
    "                documents.extend(file_documents)\n",
    "                print(f\"Loaded {len(file_documents)} documents from {file_path}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {file_path}: {e}\")\n",
    "                \n",
    "        self.documents = documents\n",
    "        return self.documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a105c7a",
   "metadata": {},
   "source": [
    "## Chunk Files\n",
    "\n",
    "Next, we'll implement the method to divide documents into smaller chunks for better processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282648da",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def chunk_documents(self, chunk_size: int = 1000, chunk_overlap: int = 200):\n",
    "        \"\"\"\n",
    "        Split documents into smaller chunks.\n",
    "        \n",
    "        Args:\n",
    "            chunk_size: The target size of each chunk\n",
    "            chunk_overlap: The overlap between chunks\n",
    "        \n",
    "        Returns:\n",
    "            List of document chunks\n",
    "        \"\"\"\n",
    "        if not self.documents:\n",
    "            self.load_documents()\n",
    "            \n",
    "        if not self.documents:\n",
    "            print(\"No documents to chunk!\")\n",
    "            return []\n",
    "            \n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    "        )\n",
    "        \n",
    "        self.chunks = text_splitter.split_documents(self.documents)\n",
    "        print(f\"Created {len(self.chunks)} chunks from {len(self.documents)} documents\")\n",
    "        \n",
    "        return self.chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657d0217",
   "metadata": {},
   "source": [
    "## Embed Files\n",
    "\n",
    "Now we'll create a method to generate embeddings for the file chunks using the configured embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e22423",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def embed_chunks(self):\n",
    "        \"\"\"\n",
    "        Generate embeddings for each chunk and store them with the chunk.\n",
    "        \n",
    "        Returns:\n",
    "            List of dictionaries containing chunks and their embeddings\n",
    "        \"\"\"\n",
    "        if not self.chunks:\n",
    "            self.chunk_documents()\n",
    "            \n",
    "        if not self.chunks:\n",
    "            print(\"No chunks to embed!\")\n",
    "            return []\n",
    "            \n",
    "        embedded_documents = []\n",
    "        \n",
    "        # Process in batches of 100\n",
    "        batch_size = 100\n",
    "        for i in range(0, len(self.chunks), batch_size):\n",
    "            batch = self.chunks[i:i + batch_size]\n",
    "            texts = [doc.page_content for doc in batch]\n",
    "            \n",
    "            try:\n",
    "                # Generate embeddings\n",
    "                embeddings = self.embeddings.embed_documents(texts)\n",
    "                \n",
    "                # Store chunks with their embeddings\n",
    "                for j, doc in enumerate(batch):\n",
    "                    embedded_doc = {\n",
    "                        'content': doc.page_content,\n",
    "                        'metadata': doc.metadata,\n",
    "                        'embedding': embeddings[j]\n",
    "                    }\n",
    "                    embedded_documents.append(embedded_doc)\n",
    "                    \n",
    "                print(f\"Embedded chunks {i} to {i + len(batch) - 1}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error generating embeddings for batch {i}-{i + len(batch) - 1}: {e}\")\n",
    "                \n",
    "        self.documents = embedded_documents\n",
    "        return self.documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8176c68",
   "metadata": {},
   "source": [
    "## Store in Documents\n",
    "\n",
    "We'll add methods to process and store the embedded chunks in a format that can be easily queried or exported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64de8b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def process_files(self, chunk_size: int = 1000, chunk_overlap: int = 200):\n",
    "        \"\"\"\n",
    "        Process all files: load, chunk, and embed them.\n",
    "        \n",
    "        Args:\n",
    "            chunk_size: The target size of each chunk\n",
    "            chunk_overlap: The overlap between chunks\n",
    "            \n",
    "        Returns:\n",
    "            Processed documents with embeddings\n",
    "        \"\"\"\n",
    "        self.load_documents()\n",
    "        self.chunk_documents(chunk_size, chunk_overlap)\n",
    "        return self.embed_chunks()\n",
    "    \n",
    "    def get_documents_df(self):\n",
    "        \"\"\"\n",
    "        Convert documents to a pandas DataFrame for easy analysis.\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with document content, metadata, and embedding information\n",
    "        \"\"\"\n",
    "        if not self.documents:\n",
    "            print(\"No processed documents available!\")\n",
    "            return pd.DataFrame()\n",
    "            \n",
    "        # Create a DataFrame without the embeddings (they can make display messy)\n",
    "        docs_for_df = [\n",
    "            {\n",
    "                'content': doc['content'],\n",
    "                'source_file': doc['metadata'].get('source_file', ''),\n",
    "                'page': doc['metadata'].get('page', 0),\n",
    "                'embedding_length': len(doc['embedding'])\n",
    "            }\n",
    "            for doc in self.documents\n",
    "        ]\n",
    "        \n",
    "        return pd.DataFrame(docs_for_df)\n",
    "    \n",
    "    def save_to_json(self, output_path: str):\n",
    "        \"\"\"\n",
    "        Save the processed documents to a JSON file.\n",
    "        \n",
    "        Args:\n",
    "            output_path: Path to save the JSON file\n",
    "            \n",
    "        Returns:\n",
    "            Path to the saved file\n",
    "        \"\"\"\n",
    "        if not self.documents:\n",
    "            print(\"No documents to save!\")\n",
    "            return None\n",
    "            \n",
    "        # Convert embeddings to lists for JSON serialization\n",
    "        json_docs = []\n",
    "        for doc in self.documents:\n",
    "            doc_copy = doc.copy()\n",
    "            doc_copy['embedding'] = doc_copy['embedding'].tolist() if hasattr(doc_copy['embedding'], 'tolist') else doc_copy['embedding']\n",
    "            json_docs.append(doc_copy)\n",
    "            \n",
    "        import json\n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(json_docs, f)\n",
    "            \n",
    "        print(f\"Saved {len(json_docs)} documents to {output_path}\")\n",
    "        return output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc16dd1",
   "metadata": {},
   "source": [
    "## Use the Document Processor Class\n",
    "\n",
    "Let's test our class with some sample files and see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033c485e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's create some sample files to process\n",
    "import tempfile\n",
    "\n",
    "# Create a temporary directory\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "print(f\"Created temporary directory: {temp_dir}\")\n",
    "\n",
    "# Create a sample text file\n",
    "text_path = os.path.join(temp_dir, \"sample.txt\")\n",
    "with open(text_path, \"w\") as f:\n",
    "    f.write(\"\"\"# Sample Document\n",
    "    \n",
    "This is a sample document that we'll use to test our DocumentProcessor class.\n",
    "\n",
    "It contains multiple paragraphs with different content to demonstrate chunking.\n",
    "\n",
    "## Section 1\n",
    "This section talks about machine learning and its applications.\n",
    "Machine learning is a subset of artificial intelligence that provides systems \n",
    "the ability to automatically learn and improve from experience without being \n",
    "explicitly programmed.\n",
    "\n",
    "## Section 2\n",
    "This section covers natural language processing (NLP).\n",
    "NLP is a field of AI that gives computers the ability to understand text and \n",
    "spoken words in much the same way human beings can.\n",
    "\n",
    "## Section 3\n",
    "This section discusses embeddings in machine learning.\n",
    "Embeddings are a type of representation that captures the meaning of objects \n",
    "such as words, sentences, or documents in a numerical form that computers can process.\n",
    "\"\"\")\n",
    "\n",
    "# Create a sample Markdown file\n",
    "md_path = os.path.join(temp_dir, \"notes.md\")\n",
    "with open(md_path, \"w\") as f:\n",
    "    f.write(\"\"\"# Research Notes\n",
    "\n",
    "## Important Concepts\n",
    "\n",
    "- Vector databases store embeddings efficiently\n",
    "- Chunking strategies affect retrieval quality\n",
    "- Embedding models have different strengths\n",
    "\n",
    "## Implementation Ideas\n",
    "\n",
    "1. Use recursive chunking for nested documents\n",
    "2. Compare different embedding models\n",
    "3. Implement similarity search for retrieval\n",
    "4. Add metadata filtering capabilities\n",
    "\n",
    "## Future Work\n",
    "\n",
    "Consider integrating with other systems and adding visualization tools.\n",
    "\"\"\")\n",
    "\n",
    "print(f\"Created sample files at:\\n- {text_path}\\n- {md_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1eb7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our DocumentProcessor\n",
    "# Note: For OpenAI embeddings you would need to have set your API key in the environment\n",
    "# For demonstration, we'll use HuggingFace embeddings which don't require API keys\n",
    "\n",
    "processor = DocumentProcessor(embedding_model=\"huggingface\")\n",
    "\n",
    "# Add our sample files\n",
    "processor.add_file_path(text_path)\n",
    "processor.add_file_path(md_path)\n",
    "\n",
    "# Process the files\n",
    "processor.process_files(chunk_size=200, chunk_overlap=50)\n",
    "\n",
    "# Display the resulting documents as a DataFrame\n",
    "df = processor.get_documents_df()\n",
    "print(f\"Processed {len(df)} document chunks\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0909e992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine the length of our chunks\n",
    "df['content_length'] = df['content'].apply(len)\n",
    "df[['content_length', 'source_file']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40803ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a few chunks to see how the chunking worked\n",
    "for i, row in df.head(3).iterrows():\n",
    "    print(f\"--- Chunk {i + 1} from {row['source_file']} ---\")\n",
    "    print(row['content'])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b6ab8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save our processed documents for later use\n",
    "output_path = os.path.join(temp_dir, \"processed_documents.json\")\n",
    "processor.save_to_json(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff845636",
   "metadata": {},
   "source": [
    "## Semantic Search with Embeddings\n",
    "\n",
    "Now let's implement a simple semantic search function to demonstrate the power of embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93be73e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def semantic_search(query, processor, top_k=3):\n",
    "    \"\"\"\n",
    "    Search for documents similar to the query using cosine similarity.\n",
    "    \n",
    "    Args:\n",
    "        query: The search query\n",
    "        processor: DocumentProcessor instance with embedded documents\n",
    "        top_k: Number of results to return\n",
    "        \n",
    "    Returns:\n",
    "        List of the top_k most similar documents\n",
    "    \"\"\"\n",
    "    if not processor.documents:\n",
    "        print(\"No documents to search!\")\n",
    "        return []\n",
    "        \n",
    "    # Get query embedding\n",
    "    query_embedding = processor.embeddings.embed_query(query)\n",
    "    \n",
    "    # Calculate similarities\n",
    "    similarities = []\n",
    "    for doc in processor.documents:\n",
    "        doc_embedding = np.array(doc['embedding'])\n",
    "        similarity = cosine_similarity([query_embedding], [doc_embedding])[0][0]\n",
    "        similarities.append({\n",
    "            'content': doc['content'],\n",
    "            'metadata': doc['metadata'],\n",
    "            'similarity': similarity\n",
    "        })\n",
    "    \n",
    "    # Sort by similarity (highest first)\n",
    "    sorted_docs = sorted(similarities, key=lambda x: x['similarity'], reverse=True)\n",
    "    \n",
    "    # Return top_k results\n",
    "    return sorted_docs[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84d8da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try a few search queries\n",
    "search_queries = [\n",
    "    \"What is machine learning?\",\n",
    "    \"How do embeddings work?\",\n",
    "    \"Tell me about chunking documents\"\n",
    "]\n",
    "\n",
    "for query in search_queries:\n",
    "    print(f\"\\n--- Search results for: '{query}' ---\")\n",
    "    results = semantic_search(query, processor)\n",
    "    \n",
    "    for i, result in enumerate(results):\n",
    "        print(f\"\\nResult {i+1} (similarity: {result['similarity']:.4f}):\")\n",
    "        print(f\"Source: {result['metadata'].get('source_file', 'unknown')}\")\n",
    "        print(f\"Content: {result['content'][:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae2c151",
   "metadata": {},
   "source": [
    "## Cleanup and Conclusion\n",
    "\n",
    "Our DocumentProcessor class successfully:\n",
    "1. Loads documents from different file formats\n",
    "2. Chunks them into manageable pieces\n",
    "3. Creates embeddings for semantic understanding\n",
    "4. Stores the processed documents with their metadata\n",
    "5. Provides utilities for analysis and retrieval\n",
    "\n",
    "This foundation can be extended for various applications like question answering systems, document search engines, or knowledge bases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7469696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up temporary files\n",
    "import shutil\n",
    "shutil.rmtree(temp_dir)\n",
    "print(f\"Removed temporary directory: {temp_dir}\")\n",
    "\n",
    "print(\"\\nDocumentProcessor demonstration complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
