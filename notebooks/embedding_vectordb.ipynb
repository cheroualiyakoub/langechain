{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c1f4172",
   "metadata": {},
   "source": [
    "1. Extract semantic chunks from your JSON (e.g., abstract, claim, paragraph).\n",
    "\n",
    "2. Use HuggingFaceEmbeddings or OpenAIEmbeddings.\n",
    "\n",
    "3. Store structured info as metadata.\n",
    "\n",
    "4. Use metadata filtering + vector similarity for retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db964030-6765-4dff-8c26-035939f449b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f62f4251-df00-493c-b405-361abc6ada42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Current directory: /app/notebooks\n",
      "📁 Project root: /app\n",
      "📁 Source directory: /app/src\n",
      "✅ Python paths configured\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "import json\n",
    "import math\n",
    "\n",
    "# Add source directories to Python path\n",
    "current_dir = Path.cwd()\n",
    "project_root = current_dir.parent  # Go up one level from notebooks to project root\n",
    "src_dir = project_root / \"src\"\n",
    "\n",
    "# Add paths\n",
    "sys.path.append(str(src_dir / \"data_pipline\"))\n",
    "sys.path.append(str(src_dir / \"EU_XML_data_loader\"))\n",
    "\n",
    "\n",
    "from data_pipline import DataPipeline\n",
    "import get_raw_data_paths_EPO  \n",
    "from xml_loader_EPO import process_xml_files_list\n",
    "\n",
    "\n",
    "print(f\"📁 Current directory: {current_dir}\")\n",
    "print(f\"📁 Project root: {project_root}\")\n",
    "print(f\"📁 Source directory: {src_dir}\")\n",
    "print(f\"✅ Python paths configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d61903a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import os\n",
    "from langchain.docstore.document import Document\n",
    "import sys\n",
    "# Prepare .json file for embedding\n",
    "def extract_documents(json_data):\n",
    "    bibliographic = json_data.get(\"bibliographic_data\", {})\n",
    "    doc_id = bibliographic.get(\"doc_id\", \"UNKNOWN\")\n",
    "    documents = []\n",
    "\n",
    "    # Common metadata to propagate\n",
    "    common_meta = {\n",
    "        \"doc_id\": doc_id,\n",
    "        \"language\": bibliographic.get(\"language\"),\n",
    "        \"country\": bibliographic.get(\"country\"),\n",
    "        \"doc_number\": bibliographic.get(\"doc_number\"),\n",
    "        \"application_number\": bibliographic.get(\"application_number\"),\n",
    "        \"publication_date\": bibliographic.get(\"publication_date\"),\n",
    "        \"ipc_classes\": bibliographic.get(\"ipc_classes\", []),\n",
    "        \"file\":bibliographic.get(\"file\")\n",
    "    }\n",
    "\n",
    "    # Title (en preferred)\n",
    "    title_dict = bibliographic.get(\"title\", {})\n",
    "    title = title_dict.get(\"en\") or next(iter(title_dict.values()), \"\")\n",
    "    if title:\n",
    "        documents.append(Document(\n",
    "            page_content=title,\n",
    "            metadata={**common_meta, \"section\": \"title\"}\n",
    "        ))\n",
    "\n",
    "    # Abstract\n",
    "    abstract = bibliographic.get(\"abstract\")\n",
    "    if abstract:\n",
    "        documents.append(Document(\n",
    "            page_content=abstract,\n",
    "            metadata={**common_meta, \"section\": \"abstract\"}\n",
    "        ))\n",
    "\n",
    "    # Claims\n",
    "    for claim in json_data.get(\"claims\", []):\n",
    "        documents.append(Document(\n",
    "            page_content=claim[\"text\"],\n",
    "            metadata={**common_meta, \"section\": \"claim\", \"claim_number\": claim.get(\"claim_number\")}\n",
    "        ))\n",
    "\n",
    "    # Main sections\n",
    "    for section in json_data.get(\"main_sections\", []):\n",
    "        section_name = section.get(\"heading_text\", \"UNKNOWN_SECTION\")\n",
    "        for p in section.get(\"paragraphs\", []):\n",
    "            documents.append(Document(\n",
    "                page_content=f\"{section_name}\\n{p['text']}\",\n",
    "                metadata={**common_meta, \"section\": section_name, \"p_id\": p.get(\"p_id\")}\n",
    "            ))\n",
    "\n",
    "    return documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df7f0996-5e36-4059-b57e-f6d559a8bc3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully imported from data_config.py\n",
      "🔍 Testing JSON File Loading Functions\n",
      "==================================================\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2ed2e254-9f1e-442b-8555-f090f5edcc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def get_chunk_size(text: str, total_tokens: int, base_chunk_size: int = 350, min_chunk_size: int = 180) -> int:\n",
    "#     # total_tokens = count_tokens(text)\n",
    "\n",
    "#     # If it's small enough, return as one chunk\n",
    "#     if total_tokens <= base_chunk_size:\n",
    "#         return total_tokens\n",
    "\n",
    "#     # Try to divide the tokens into balanced parts\n",
    "#     num_splits = total_tokens // base_chunk_size\n",
    "#     if total_tokens % base_chunk_size != 0:\n",
    "#         num_splits += 1\n",
    "\n",
    "#     # Compute new balanced chunk size\n",
    "#     balanced_chunk_size = total_tokens // num_splits\n",
    "\n",
    "#     # Make sure it doesn't go below a minimum\n",
    "#     balanced_chunk_size = int(max(min_chunk_size, balanced_chunk_size))\n",
    "\n",
    "\n",
    "#     return balanced_chunk_size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "501fa319",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain.schema.document import Document  # Required if your docs are raw strings\n",
    "\n",
    "def chunk_documents(documents):\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "    # Define a helper to count tokens\n",
    "    def count_tokens(text):\n",
    "        return len(tokenizer.encode(text, truncation=False, add_special_tokens=False))\n",
    "\n",
    "    processed_docs = []\n",
    "    for doc in documents:\n",
    "        # If doc is a raw string, wrap it in a Document\n",
    "        if isinstance(doc, str):\n",
    "            doc = Document(page_content=doc)\n",
    "\n",
    "        token_count = count_tokens(doc.page_content)\n",
    "        balanced_chunk_size = get_chunk_size(doc, token_count)  # You must define this function\n",
    "        print(\"Token Count:\", token_count)\n",
    "        print(\"Balanced Chunk Size:\", balanced_chunk_size)\n",
    "\n",
    "        splitter = TokenTextSplitter(\n",
    "            chunk_size=balanced_chunk_size,\n",
    "            chunk_overlap=3,\n",
    "                tokenizer=tokenizer  # ✅ important\n",
    "        )\n",
    "\n",
    "        chunks = splitter.split_documents([doc])\n",
    "        print(\"Before:\", len(processed_docs))\n",
    "        processed_docs.extend(chunks)\n",
    "        print(\"After:\", len(processed_docs))\n",
    "\n",
    "    return processed_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9a0166",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.environ[\"TRANSFORMERS_HTTP_TIMEOUT\"] = \"60\"\n",
    "# load the .json documents\n",
    "file_list = get_epo_json_file_paths()[:100]\n",
    "# file_list = glob.glob(json_files_path)\n",
    "\n",
    "all_documents = []\n",
    "\n",
    "for file_path in file_list:\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "        docs = extract_documents(data)\n",
    "        chunked_docs = chunk_documents(docs)\n",
    "        all_documents.extend(chunked_docs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a3b881-6e4d-460d-8f99-ffb5a9008c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, doc in enumerate(all_documents, start=1):\n",
    "    if i > 2:\n",
    "        break\n",
    "    print(f\"Document {i}\")\n",
    "    print(\"Page Content:\", doc.page_content)\n",
    "    print(\"Metadata: \", doc.metadata)\n",
    "print(\"Wtf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "42c6566e-998b-4a22-bd74-9fbf5def3761",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 11:47:19,914 - INFO - Use pytorch device_name: cpu\n",
      "2025-06-18 11:47:19,915 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Load tokenizer and embedding model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def count_tokens(text):\n",
    "    return len(tokenizer.encode(text, add_special_tokens=False))\n",
    "\n",
    "def get_chunk_size(text: str, total_tokens: int, base_chunk_size: int = 350, min_chunk_size: int = 180) -> int:\n",
    "    if total_tokens <= base_chunk_size:\n",
    "        return total_tokens\n",
    "\n",
    "    num_splits = total_tokens // base_chunk_size\n",
    "    if total_tokens % base_chunk_size != 0:\n",
    "        num_splits += 1\n",
    "\n",
    "\n",
    "    balanced_chunk_size = math.ceil(total_tokens / num_splits)\n",
    "\n",
    "    balanced_chunk_size = int(max(min_chunk_size, balanced_chunk_size))\n",
    "    return balanced_chunk_size\n",
    "\n",
    "def chunk_text_by_tokens(text: str, chunk_size: int, overlap: int = 30):\n",
    "    print(\"Starting tokenization...\")\n",
    "    input_ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "    print(f\"Tokenized text length: {len(input_ids)} tokens\")\n",
    "    \n",
    "    # Safety check to prevent infinite loops or extremely slow processing\n",
    "    if chunk_size <= overlap:\n",
    "        overlap = 0\n",
    "    \n",
    "    chunks = []\n",
    "    start = 0\n",
    "    \n",
    "    # Process in batches for large texts\n",
    "    while start < len(input_ids):\n",
    "        print(f\"Processing chunk at position {start}/{len(input_ids)}\")\n",
    "        end = min(start + chunk_size, len(input_ids))\n",
    "        chunk_ids = input_ids[start:end]\n",
    "        \n",
    "        try:\n",
    "            chunk_text = tokenizer.decode(chunk_ids)\n",
    "            chunks.append(chunk_text)\n",
    "        except Exception as e:\n",
    "            print(f\"Error decoding chunk: {e}\")\n",
    "            # Skip this chunk or use a fallback approach\n",
    "        \n",
    "        # Advance position\n",
    "        start += chunk_size - overlap\n",
    "        \n",
    "        # Safety check to prevent memory issues with too many chunks\n",
    "        if len(chunks) > 1000:  # Arbitrary limit\n",
    "            print(\"⚠️ Warning: Reached maximum number of chunks\")\n",
    "            break\n",
    "    \n",
    "    print(f\"Created {len(chunks)} chunks\")\n",
    "    return chunks\n",
    "\n",
    "def embed_document(text: str):\n",
    "    total_tokens = count_tokens(text)\n",
    "    chunk_size = get_chunk_size(text, total_tokens)\n",
    "\n",
    "    chunks = chunk_text_by_tokens(text, chunk_size)\n",
    "    embeddings = embedding_model.encode(chunks)\n",
    "\n",
    "    return list(zip(chunks, embeddings))  # list of (chunk, embedding_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a4ccdf4c-b73b-4948-84d9-502c145eea2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Found 1286 EPO JSON files\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'doc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m docs \u001b[38;5;241m=\u001b[39m extract_documents(data)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# If doc is a raw string, wrap it in a Document\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[43mdoc\u001b[49m, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     12\u001b[0m     doc \u001b[38;5;241m=\u001b[39m Document(page_content\u001b[38;5;241m=\u001b[39mdoc)\n\u001b[1;32m     13\u001b[0m chunked_docs \u001b[38;5;241m=\u001b[39m embed_document(doc\u001b[38;5;241m.\u001b[39mpage_content)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'doc' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "file_list = get_epo_json_file_paths()[:100]\n",
    "# file_list = glob.glob(json_files_path)\n",
    "\n",
    "all_documents = []\n",
    "\n",
    "for file_path in file_list:\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "        docs = extract_documents(data)\n",
    "        # If doc is a raw string, wrap it in a Document\n",
    "        if isinstance(doc, str):\n",
    "            doc = Document(page_content=doc)\n",
    "        chunked_docs = embed_document(doc.page_content)\n",
    "        all_documents.extend(chunked_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0aee99aa-37fd-4b25-9c89-195e9dd23da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_chunk, first_embedding = all_documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "48b1b364-351f-4d4e-affb-ce2853fd78f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[ document ( metadata = { ' doc _ id ' : ' ep13899497b9w1 ', ' language ' : ' en ', ' country ' : ' ep ', ' doc _ number ' : ' 3084761 ', ' application _ number ' : ' 13899497. 5 ', ' publication _ date ' : ' 20250611 ', ' ipc _ classes ' : [ ' g10l 19 / 038 20130101afi20170426bhep ', ' g10l 19 / 07 20130101ali20170426bhep ' ], ' file ' : ' ep13899497w1b9. xml ', ' section ' : ' title ' }, page _ content = ' audio signal encoder ' ), document ( metadata = { ' doc _ id ' : ' ep13899497b9w1 ', ' language ' : ' en ', ' country ' : ' ep ', ' doc _ number ' : ' 3084761 ', ' application _ number ' : ' 13899497. 5 ', ' publication _ date ' : ' 20250611 ', ' ipc _ classes ' : [ ' g10l 19 / 038 20130101afi20170426bhep ', ' g10l 19 / 07 20130101ali20170426bhep ' ], ' file ' : ' ep13899497w1b9. xml ', ' section ' : ' claim ', ' claim _ number ' : ' 0001 '\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6dbd10ea-2885-4c9c-8bbd-87d1ba6addb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import List, Dict, Any\n",
    "import logging\n",
    "import numpy as np\n",
    "from langchain.schema import Document\n",
    "\n",
    "def embed_documents_with_metadata(documents, debug=True):\n",
    "    \"\"\"Process a list of Document objects, preserving metadata\"\"\"\n",
    "    if debug:\n",
    "        print(f\"📋 Starting document embedding process for {len(documents)} documents\")\n",
    "        start_time = time.time()\n",
    "    \n",
    "    processed_docs = []\n",
    "    \n",
    "    # Track statistics for debugging\n",
    "    total_tokens = 0\n",
    "    total_chunks = 0\n",
    "    errors = 0\n",
    "    \n",
    "    for doc_idx, doc in enumerate(documents):\n",
    "        if debug and doc_idx % 10 == 0:  # Progress update every 10 docs\n",
    "            print(f\"⏳ Processing document {doc_idx+1}/{len(documents)} ({(doc_idx+1)/len(documents)*100:.1f}%)\")\n",
    "        \n",
    "        try:\n",
    "            # Get text content and metadata\n",
    "            text = doc.page_content\n",
    "            metadata = doc.metadata\n",
    "            \n",
    "            if debug and doc_idx < 3:  # Show sample of first few documents\n",
    "                print(f\"\\n📄 Document {doc_idx+1} Preview:\")\n",
    "                print(f\"  - Content: {text[:100]}...\" if len(text) > 100 else f\"  - Content: {text}\")\n",
    "                print(f\"  - Metadata: {metadata}\")\n",
    "\n",
    "            \n",
    "            # Calculate chunk size\n",
    "            doc_tokens = count_tokens(text)\n",
    "            total_tokens += doc_tokens\n",
    "            chunk_size = get_chunk_size(text, doc_tokens)\n",
    "            \n",
    "            if debug and doc_idx < 3:\n",
    "                print(f\"  - Token count: {doc_tokens}\")\n",
    "                print(f\"  - Calculated chunk size: {chunk_size}\")\n",
    "            \n",
    "            # Create chunks\n",
    "            chunk_start_time = time.time()\n",
    "            print(\"x\")\n",
    "            chunks = chunk_text_by_tokens(text, chunk_size)\n",
    "            print(\"y\")\n",
    "            total_chunks += len(chunks)\n",
    "            \n",
    "            if debug and doc_idx < 3:\n",
    "                chunk_time = time.time() - chunk_start_time\n",
    "                print(f\"  - Chunks created: {len(chunks)} (in {chunk_time:.2f}s)\")\n",
    "                print(f\"  - First chunk: {chunks[0][:50]}...\")\n",
    "            \n",
    "            # Generate embeddings for all chunks\n",
    "            embedding_start_time = time.time()\n",
    "            try:\n",
    "                \n",
    "                batch_size = 8 \n",
    "                all_embeddings = []\n",
    "                \n",
    "                for batch_idx in range(0, len(chunks), batch_size):\n",
    "                    batch = chunks[batch_idx:batch_idx+batch_size]\n",
    "                    print(f\"  - Processing batch {batch_idx//batch_size + 1}/{(len(chunks)+batch_size-1)//batch_size}\")\n",
    "                    batch_embeddings = embedding_model.encode(batch, show_progress_bar=False)\n",
    "                    all_embeddings.extend(batch_embeddings)\n",
    "                \n",
    "                embeddings = all_embeddings\n",
    "                print(\"✓ Embedding completed\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error during embedding: {str(e)}\")\n",
    "                raise\n",
    "            print(\"x\")\n",
    "            if debug and doc_idx < 3:\n",
    "                embedding_time = time.time() - embedding_start_time\n",
    "                print(f\"  - Embeddings generated: {len(embeddings)} vectors of shape {embeddings[0].shape}\")\n",
    "                print(f\"  - Embedding time: {embedding_time:.2f}s ({embedding_time/len(chunks):.4f}s per chunk)\")\n",
    "            \n",
    "            # Create new documents with chunks and metadata\n",
    "            for i, (chunk_text, embedding) in enumerate(zip(chunks, embeddings)):\n",
    "                # Create a copy of metadata and add chunk information\n",
    "                chunk_metadata = metadata.copy()\n",
    "                chunk_metadata[\"chunk_index\"] = i\n",
    "                chunk_metadata[\"total_chunks\"] = len(chunks)\n",
    "                chunk_metadata[\"source_doc_idx\"] = doc_idx\n",
    "                \n",
    "                # Check for NaN values in embedding\n",
    "                if np.isnan(embedding).any():\n",
    "                    if debug:\n",
    "                        print(f\"⚠️ Warning: NaN values detected in embedding for document {doc_idx}, chunk {i}\")\n",
    "                    # Replace NaN with zeros\n",
    "                    embedding = np.nan_to_num(embedding)\n",
    "                \n",
    "                # Create a new Document object with chunk text and original metadata\n",
    "                processed_docs.append({\n",
    "                    \"text\": chunk_text,\n",
    "                    \"embedding\": embedding,\n",
    "                    \"metadata\": chunk_metadata\n",
    "                })\n",
    "                \n",
    "        except Exception as e:\n",
    "            errors += 1\n",
    "            if debug:\n",
    "                print(f\"❌ Error processing document {doc_idx}: {str(e)}\")\n",
    "    \n",
    "    if debug:\n",
    "        end_time = time.time()\n",
    "        total_time = end_time - start_time\n",
    "        print(f\"\\n✅ Embedding completed in {total_time:.2f}s\")\n",
    "        print(f\"📊 Statistics:\")\n",
    "        print(f\"  - Documents processed: {len(documents)}\")\n",
    "        print(f\"  - Total tokens: {total_tokens}\")\n",
    "        print(f\"  - Total chunks created: {total_chunks}\")\n",
    "        print(f\"  - Average chunks per document: {total_chunks/len(documents):.2f}\")\n",
    "        print(f\"  - Processing speed: {total_tokens/total_time:.2f} tokens/second\")\n",
    "        print(f\"  - Documents with errors: {errors}\")\n",
    "        print(f\"  - Success rate: {(len(documents)-errors)/len(documents)*100:.2f}%\")\n",
    "        \n",
    "        # Memory usage of embeddings\n",
    "        embedding_size = sum(emb[\"embedding\"].nbytes for emb in processed_docs)\n",
    "        print(f\"  - Embedding memory usage: {embedding_size/1024/1024:.2f} MB\")\n",
    "    \n",
    "    return processed_docs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "07a069dc-f84a-4737-a957-bfde18c9e53d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 Starting document embedding process for 2 documents\n",
      "⏳ Processing document 1/2 (50.0%)\n",
      "\n",
      "📄 Document 1 Preview:\n",
      "  - Content: This is a test document with some content that will be embedded.\n",
      "  - Metadata: {'source': 'test', 'id': 1}\n",
      "  - Token count: 13\n",
      "  - Calculated chunk size: 13\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 13 tokens\n",
      "Processing chunk at position 0/13\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Chunks created: 1 (in 0.00s)\n",
      "  - First chunk: this is a test document with some content that wil...\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "  - Embeddings generated: 1 vectors of shape (384,)\n",
      "  - Embedding time: 0.05s (0.0461s per chunk)\n",
      "\n",
      "📄 Document 2 Preview:\n",
      "  - Content: This is another document with different content for embedding.\n",
      "  - Metadata: {'source': 'test', 'id': 2}\n",
      "  - Token count: 12\n",
      "  - Calculated chunk size: 12\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 12 tokens\n",
      "Processing chunk at position 0/12\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Chunks created: 1 (in 0.00s)\n",
      "  - First chunk: this is another document with different content fo...\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "  - Embeddings generated: 1 vectors of shape (384,)\n",
      "  - Embedding time: 0.01s (0.0139s per chunk)\n",
      "\n",
      "✅ Embedding completed in 0.06s\n",
      "📊 Statistics:\n",
      "  - Documents processed: 2\n",
      "  - Total tokens: 25\n",
      "  - Total chunks created: 2\n",
      "  - Average chunks per document: 1.00\n",
      "  - Processing speed: 394.41 tokens/second\n",
      "  - Documents with errors: 0\n",
      "  - Success rate: 100.00%\n",
      "  - Embedding memory usage: 0.00 MB\n",
      "\n",
      "🔍 First Result Sample:\n",
      "Text: this is a test document with some content that will be embedded.\n",
      "Embedding shape: (384,)\n",
      "Metadata: {'source': 'test', 'id': 1, 'chunk_index': 0, 'total_chunks': 1, 'source_doc_idx': 0}\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Create some test documents\n",
    "    test_docs = [\n",
    "        Document(page_content=\"This is a test document with some content that will be embedded.\",\n",
    "                metadata={\"source\": \"test\", \"id\": 1}),\n",
    "        Document(page_content=\"This is another document with different content for embedding.\",\n",
    "                metadata={\"source\": \"test\", \"id\": 2}),\n",
    "    ]\n",
    "    \n",
    "    # Process with debug output\n",
    "    result = embed_documents_with_metadata(test_docs, debug=True)\n",
    "    \n",
    "    # Show first result\n",
    "    print(\"\\n🔍 First Result Sample:\")\n",
    "    print(f\"Text: {result[0]['text']}\")\n",
    "    print(f\"Embedding shape: {result[0]['embedding'].shape}\")\n",
    "    print(f\"Metadata: {result[0]['metadata']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "eb2a9df1-69f5-4ce5-9bd3-eea48134674b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Found 1286 EPO JSON files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Process all files\n",
    "file_list = get_epo_json_file_paths()[:10]\n",
    "all_documents = []\n",
    "\n",
    "len(file_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d7634cbd-ccd4-4e0c-bacb-93e91500b5c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (691 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/app/data/parsed/EPO/EPRTBJV2025000024001001/EPW1B9/EP13899497W1B9/EP13899497W1B9.json\n",
      "📋 Starting document embedding process for 180 documents\n",
      "⏳ Processing document 1/180 (0.6%)\n",
      "\n",
      "📄 Document 1 Preview:\n",
      "  - Content: AUDIO SIGNAL ENCODER\n",
      "  - Metadata: {'doc_id': 'EP13899497B9W1', 'language': 'en', 'country': 'EP', 'doc_number': '3084761', 'application_number': '13899497.5', 'publication_date': '20250611', 'ipc_classes': ['G10L  19/038       20130101AFI20170426BHEP', 'G10L  19/07        20130101ALI20170426BHEP'], 'file': 'EP13899497W1B9.xml', 'section': 'title'}\n",
      "  - Token count: 5\n",
      "  - Calculated chunk size: 5\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 5 tokens\n",
      "Processing chunk at position 0/5\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Chunks created: 1 (in 0.00s)\n",
      "  - First chunk: audio signal encoder...\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "  - Embeddings generated: 1 vectors of shape (384,)\n",
      "  - Embedding time: 0.03s (0.0272s per chunk)\n",
      "\n",
      "📄 Document 2 Preview:\n",
      "  - Content: A processor-implemented method for encoding at least one audio signal, wherein the method comprises:...\n",
      "  - Metadata: {'doc_id': 'EP13899497B9W1', 'language': 'en', 'country': 'EP', 'doc_number': '3084761', 'application_number': '13899497.5', 'publication_date': '20250611', 'ipc_classes': ['G10L  19/038       20130101AFI20170426BHEP', 'G10L  19/07        20130101ALI20170426BHEP'], 'file': 'EP13899497W1B9.xml', 'section': 'claim', 'claim_number': '0001'}\n",
      "  - Token count: 691\n",
      "  - Calculated chunk size: 346\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 691 tokens\n",
      "Processing chunk at position 0/691\n",
      "Processing chunk at position 316/691\n",
      "Processing chunk at position 632/691\n",
      "Created 3 chunks\n",
      "y\n",
      "  - Chunks created: 3 (in 0.00s)\n",
      "  - First chunk: a processor - implemented method for encoding at l...\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "  - Embeddings generated: 3 vectors of shape (384,)\n",
      "  - Embedding time: 0.07s (0.0229s per chunk)\n",
      "\n",
      "📄 Document 3 Preview:\n",
      "  - Content: The method as claimed in claim 1, further comprising: selecting the scale factor from a plurality of...\n",
      "  - Metadata: {'doc_id': 'EP13899497B9W1', 'language': 'en', 'country': 'EP', 'doc_number': '3084761', 'application_number': '13899497.5', 'publication_date': '20250611', 'ipc_classes': ['G10L  19/038       20130101AFI20170426BHEP', 'G10L  19/07        20130101ALI20170426BHEP'], 'file': 'EP13899497W1B9.xml', 'section': 'claim', 'claim_number': '0002'}\n",
      "  - Token count: 38\n",
      "  - Calculated chunk size: 38\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 38 tokens\n",
      "Processing chunk at position 0/38\n",
      "Processing chunk at position 8/38\n",
      "Processing chunk at position 16/38\n",
      "Processing chunk at position 24/38\n",
      "Processing chunk at position 32/38\n",
      "Created 5 chunks\n",
      "y\n",
      "  - Chunks created: 5 (in 0.00s)\n",
      "  - First chunk: the method as claimed in claim 1, further comprisi...\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "  - Embeddings generated: 5 vectors of shape (384,)\n",
      "  - Embedding time: 0.02s (0.0049s per chunk)\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 59 tokens\n",
      "Processing chunk at position 0/59\n",
      "Processing chunk at position 29/59\n",
      "Processing chunk at position 58/59\n",
      "Created 3 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 82 tokens\n",
      "Processing chunk at position 0/82\n",
      "Processing chunk at position 52/82\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 791 tokens\n",
      "Processing chunk at position 0/791\n",
      "Processing chunk at position 234/791\n",
      "Processing chunk at position 468/791\n",
      "Processing chunk at position 702/791\n",
      "Created 4 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 129 tokens\n",
      "Processing chunk at position 0/129\n",
      "Processing chunk at position 99/129\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 66 tokens\n",
      "Processing chunk at position 0/66\n",
      "Processing chunk at position 36/66\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 82 tokens\n",
      "Processing chunk at position 0/82\n",
      "Processing chunk at position 52/82\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 33 tokens\n",
      "Processing chunk at position 0/33\n",
      "Processing chunk at position 3/33\n",
      "Processing chunk at position 6/33\n",
      "Processing chunk at position 9/33\n",
      "Processing chunk at position 12/33\n",
      "Processing chunk at position 15/33\n",
      "Processing chunk at position 18/33\n",
      "Processing chunk at position 21/33\n",
      "Processing chunk at position 24/33\n",
      "Processing chunk at position 27/33\n",
      "Processing chunk at position 30/33\n",
      "Created 11 chunks\n",
      "y\n",
      "  - Processing batch 1/2\n",
      "  - Processing batch 2/2\n",
      "✓ Embedding completed\n",
      "x\n",
      "⏳ Processing document 11/180 (6.1%)\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 33 tokens\n",
      "Processing chunk at position 0/33\n",
      "Processing chunk at position 3/33\n",
      "Processing chunk at position 6/33\n",
      "Processing chunk at position 9/33\n",
      "Processing chunk at position 12/33\n",
      "Processing chunk at position 15/33\n",
      "Processing chunk at position 18/33\n",
      "Processing chunk at position 21/33\n",
      "Processing chunk at position 24/33\n",
      "Processing chunk at position 27/33\n",
      "Processing chunk at position 30/33\n",
      "Created 11 chunks\n",
      "y\n",
      "  - Processing batch 1/2\n",
      "  - Processing batch 2/2\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 116 tokens\n",
      "Processing chunk at position 0/116\n",
      "Processing chunk at position 86/116\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 37 tokens\n",
      "Processing chunk at position 0/37\n",
      "Processing chunk at position 7/37\n",
      "Processing chunk at position 14/37\n",
      "Processing chunk at position 21/37\n",
      "Processing chunk at position 28/37\n",
      "Processing chunk at position 35/37\n",
      "Created 6 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 88 tokens\n",
      "Processing chunk at position 0/88\n",
      "Processing chunk at position 58/88\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 30 tokens\n",
      "Processing chunk at position 0/30\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 30 tokens\n",
      "Processing chunk at position 0/30\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 74 tokens\n",
      "Processing chunk at position 0/74\n",
      "Processing chunk at position 44/74\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 25 tokens\n",
      "Processing chunk at position 0/25\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 40 tokens\n",
      "Processing chunk at position 0/40\n",
      "Processing chunk at position 10/40\n",
      "Processing chunk at position 20/40\n",
      "Processing chunk at position 30/40\n",
      "Created 4 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 53 tokens\n",
      "Processing chunk at position 0/53\n",
      "Processing chunk at position 23/53\n",
      "Processing chunk at position 46/53\n",
      "Created 3 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "⏳ Processing document 21/180 (11.7%)\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 47 tokens\n",
      "Processing chunk at position 0/47\n",
      "Processing chunk at position 17/47\n",
      "Processing chunk at position 34/47\n",
      "Created 3 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 18 tokens\n",
      "Processing chunk at position 0/18\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 96 tokens\n",
      "Processing chunk at position 0/96\n",
      "Processing chunk at position 66/96\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 109 tokens\n",
      "Processing chunk at position 0/109\n",
      "Processing chunk at position 79/109\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 106 tokens\n",
      "Processing chunk at position 0/106\n",
      "Processing chunk at position 76/106\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 35 tokens\n",
      "Processing chunk at position 0/35\n",
      "Processing chunk at position 5/35\n",
      "Processing chunk at position 10/35\n",
      "Processing chunk at position 15/35\n",
      "Processing chunk at position 20/35\n",
      "Processing chunk at position 25/35\n",
      "Processing chunk at position 30/35\n",
      "Created 7 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 112 tokens\n",
      "Processing chunk at position 0/112\n",
      "Processing chunk at position 82/112\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 102 tokens\n",
      "Processing chunk at position 0/102\n",
      "Processing chunk at position 72/102\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 32 tokens\n",
      "Processing chunk at position 0/32\n",
      "Processing chunk at position 2/32\n",
      "Processing chunk at position 4/32\n",
      "Processing chunk at position 6/32\n",
      "Processing chunk at position 8/32\n",
      "Processing chunk at position 10/32\n",
      "Processing chunk at position 12/32\n",
      "Processing chunk at position 14/32\n",
      "Processing chunk at position 16/32\n",
      "Processing chunk at position 18/32\n",
      "Processing chunk at position 20/32\n",
      "Processing chunk at position 22/32\n",
      "Processing chunk at position 24/32\n",
      "Processing chunk at position 26/32\n",
      "Processing chunk at position 28/32\n",
      "Processing chunk at position 30/32\n",
      "Created 16 chunks\n",
      "y\n",
      "  - Processing batch 1/2\n",
      "  - Processing batch 2/2\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 231 tokens\n",
      "Processing chunk at position 0/231\n",
      "Processing chunk at position 201/231\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "⏳ Processing document 31/180 (17.2%)\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 81 tokens\n",
      "Processing chunk at position 0/81\n",
      "Processing chunk at position 51/81\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 38 tokens\n",
      "Processing chunk at position 0/38\n",
      "Processing chunk at position 8/38\n",
      "Processing chunk at position 16/38\n",
      "Processing chunk at position 24/38\n",
      "Processing chunk at position 32/38\n",
      "Created 5 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 77 tokens\n",
      "Processing chunk at position 0/77\n",
      "Processing chunk at position 47/77\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 162 tokens\n",
      "Processing chunk at position 0/162\n",
      "Processing chunk at position 132/162\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 64 tokens\n",
      "Processing chunk at position 0/64\n",
      "Processing chunk at position 34/64\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 22 tokens\n",
      "Processing chunk at position 0/22\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 27 tokens\n",
      "Processing chunk at position 0/27\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 76 tokens\n",
      "Processing chunk at position 0/76\n",
      "Processing chunk at position 46/76\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 121 tokens\n",
      "Processing chunk at position 0/121\n",
      "Processing chunk at position 91/121\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 50 tokens\n",
      "Processing chunk at position 0/50\n",
      "Processing chunk at position 20/50\n",
      "Processing chunk at position 40/50\n",
      "Created 3 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "⏳ Processing document 41/180 (22.8%)\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 155 tokens\n",
      "Processing chunk at position 0/155\n",
      "Processing chunk at position 125/155\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 19 tokens\n",
      "Processing chunk at position 0/19\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 29 tokens\n",
      "Processing chunk at position 0/29\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 84 tokens\n",
      "Processing chunk at position 0/84\n",
      "Processing chunk at position 54/84\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 84 tokens\n",
      "Processing chunk at position 0/84\n",
      "Processing chunk at position 54/84\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 91 tokens\n",
      "Processing chunk at position 0/91\n",
      "Processing chunk at position 61/91\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 83 tokens\n",
      "Processing chunk at position 0/83\n",
      "Processing chunk at position 53/83\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 21 tokens\n",
      "Processing chunk at position 0/21\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 35 tokens\n",
      "Processing chunk at position 0/35\n",
      "Processing chunk at position 5/35\n",
      "Processing chunk at position 10/35\n",
      "Processing chunk at position 15/35\n",
      "Processing chunk at position 20/35\n",
      "Processing chunk at position 25/35\n",
      "Processing chunk at position 30/35\n",
      "Created 7 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 144 tokens\n",
      "Processing chunk at position 0/144\n",
      "Processing chunk at position 114/144\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "⏳ Processing document 51/180 (28.3%)\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 52 tokens\n",
      "Processing chunk at position 0/52\n",
      "Processing chunk at position 22/52\n",
      "Processing chunk at position 44/52\n",
      "Created 3 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 24 tokens\n",
      "Processing chunk at position 0/24\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 50 tokens\n",
      "Processing chunk at position 0/50\n",
      "Processing chunk at position 20/50\n",
      "Processing chunk at position 40/50\n",
      "Created 3 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 22 tokens\n",
      "Processing chunk at position 0/22\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 32 tokens\n",
      "Processing chunk at position 0/32\n",
      "Processing chunk at position 2/32\n",
      "Processing chunk at position 4/32\n",
      "Processing chunk at position 6/32\n",
      "Processing chunk at position 8/32\n",
      "Processing chunk at position 10/32\n",
      "Processing chunk at position 12/32\n",
      "Processing chunk at position 14/32\n",
      "Processing chunk at position 16/32\n",
      "Processing chunk at position 18/32\n",
      "Processing chunk at position 20/32\n",
      "Processing chunk at position 22/32\n",
      "Processing chunk at position 24/32\n",
      "Processing chunk at position 26/32\n",
      "Processing chunk at position 28/32\n",
      "Processing chunk at position 30/32\n",
      "Created 16 chunks\n",
      "y\n",
      "  - Processing batch 1/2\n",
      "  - Processing batch 2/2\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 80 tokens\n",
      "Processing chunk at position 0/80\n",
      "Processing chunk at position 50/80\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 22 tokens\n",
      "Processing chunk at position 0/22\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 17 tokens\n",
      "Processing chunk at position 0/17\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 21 tokens\n",
      "Processing chunk at position 0/21\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 13 tokens\n",
      "Processing chunk at position 0/13\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "⏳ Processing document 61/180 (33.9%)\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 88 tokens\n",
      "Processing chunk at position 0/88\n",
      "Processing chunk at position 58/88\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 170 tokens\n",
      "Processing chunk at position 0/170\n",
      "Processing chunk at position 140/170\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 30 tokens\n",
      "Processing chunk at position 0/30\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 123 tokens\n",
      "Processing chunk at position 0/123\n",
      "Processing chunk at position 93/123\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 71 tokens\n",
      "Processing chunk at position 0/71\n",
      "Processing chunk at position 41/71\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 95 tokens\n",
      "Processing chunk at position 0/95\n",
      "Processing chunk at position 65/95\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 80 tokens\n",
      "Processing chunk at position 0/80\n",
      "Processing chunk at position 50/80\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 380 tokens\n",
      "Processing chunk at position 0/380\n",
      "Processing chunk at position 160/380\n",
      "Processing chunk at position 320/380\n",
      "Created 3 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 100 tokens\n",
      "Processing chunk at position 0/100\n",
      "Processing chunk at position 70/100\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 111 tokens\n",
      "Processing chunk at position 0/111\n",
      "Processing chunk at position 81/111\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "⏳ Processing document 71/180 (39.4%)\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 119 tokens\n",
      "Processing chunk at position 0/119\n",
      "Processing chunk at position 89/119\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 64 tokens\n",
      "Processing chunk at position 0/64\n",
      "Processing chunk at position 34/64\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 142 tokens\n",
      "Processing chunk at position 0/142\n",
      "Processing chunk at position 112/142\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 14 tokens\n",
      "Processing chunk at position 0/14\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 85 tokens\n",
      "Processing chunk at position 0/85\n",
      "Processing chunk at position 55/85\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 23 tokens\n",
      "Processing chunk at position 0/23\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 18 tokens\n",
      "Processing chunk at position 0/18\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 19 tokens\n",
      "Processing chunk at position 0/19\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 21 tokens\n",
      "Processing chunk at position 0/21\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 16 tokens\n",
      "Processing chunk at position 0/16\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "⏳ Processing document 81/180 (45.0%)\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 36 tokens\n",
      "Processing chunk at position 0/36\n",
      "Processing chunk at position 6/36\n",
      "Processing chunk at position 12/36\n",
      "Processing chunk at position 18/36\n",
      "Processing chunk at position 24/36\n",
      "Processing chunk at position 30/36\n",
      "Created 6 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 33 tokens\n",
      "Processing chunk at position 0/33\n",
      "Processing chunk at position 3/33\n",
      "Processing chunk at position 6/33\n",
      "Processing chunk at position 9/33\n",
      "Processing chunk at position 12/33\n",
      "Processing chunk at position 15/33\n",
      "Processing chunk at position 18/33\n",
      "Processing chunk at position 21/33\n",
      "Processing chunk at position 24/33\n",
      "Processing chunk at position 27/33\n",
      "Processing chunk at position 30/33\n",
      "Created 11 chunks\n",
      "y\n",
      "  - Processing batch 1/2\n",
      "  - Processing batch 2/2\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 47 tokens\n",
      "Processing chunk at position 0/47\n",
      "Processing chunk at position 17/47\n",
      "Processing chunk at position 34/47\n",
      "Created 3 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 41 tokens\n",
      "Processing chunk at position 0/41\n",
      "Processing chunk at position 11/41\n",
      "Processing chunk at position 22/41\n",
      "Processing chunk at position 33/41\n",
      "Created 4 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 25 tokens\n",
      "Processing chunk at position 0/25\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 33 tokens\n",
      "Processing chunk at position 0/33\n",
      "Processing chunk at position 3/33\n",
      "Processing chunk at position 6/33\n",
      "Processing chunk at position 9/33\n",
      "Processing chunk at position 12/33\n",
      "Processing chunk at position 15/33\n",
      "Processing chunk at position 18/33\n",
      "Processing chunk at position 21/33\n",
      "Processing chunk at position 24/33\n",
      "Processing chunk at position 27/33\n",
      "Processing chunk at position 30/33\n",
      "Created 11 chunks\n",
      "y\n",
      "  - Processing batch 1/2\n",
      "  - Processing batch 2/2\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 53 tokens\n",
      "Processing chunk at position 0/53\n",
      "Processing chunk at position 23/53\n",
      "Processing chunk at position 46/53\n",
      "Created 3 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 123 tokens\n",
      "Processing chunk at position 0/123\n",
      "Processing chunk at position 93/123\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 15 tokens\n",
      "Processing chunk at position 0/15\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 28 tokens\n",
      "Processing chunk at position 0/28\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "⏳ Processing document 91/180 (50.6%)\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 42 tokens\n",
      "Processing chunk at position 0/42\n",
      "Processing chunk at position 12/42\n",
      "Processing chunk at position 24/42\n",
      "Processing chunk at position 36/42\n",
      "Created 4 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 35 tokens\n",
      "Processing chunk at position 0/35\n",
      "Processing chunk at position 5/35\n",
      "Processing chunk at position 10/35\n",
      "Processing chunk at position 15/35\n",
      "Processing chunk at position 20/35\n",
      "Processing chunk at position 25/35\n",
      "Processing chunk at position 30/35\n",
      "Created 7 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 24 tokens\n",
      "Processing chunk at position 0/24\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 79 tokens\n",
      "Processing chunk at position 0/79\n",
      "Processing chunk at position 49/79\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 51 tokens\n",
      "Processing chunk at position 0/51\n",
      "Processing chunk at position 21/51\n",
      "Processing chunk at position 42/51\n",
      "Created 3 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 147 tokens\n",
      "Processing chunk at position 0/147\n",
      "Processing chunk at position 117/147\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 82 tokens\n",
      "Processing chunk at position 0/82\n",
      "Processing chunk at position 52/82\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 76 tokens\n",
      "Processing chunk at position 0/76\n",
      "Processing chunk at position 46/76\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 63 tokens\n",
      "Processing chunk at position 0/63\n",
      "Processing chunk at position 33/63\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 34 tokens\n",
      "Processing chunk at position 0/34\n",
      "Processing chunk at position 4/34\n",
      "Processing chunk at position 8/34\n",
      "Processing chunk at position 12/34\n",
      "Processing chunk at position 16/34\n",
      "Processing chunk at position 20/34\n",
      "Processing chunk at position 24/34\n",
      "Processing chunk at position 28/34\n",
      "Processing chunk at position 32/34\n",
      "Created 9 chunks\n",
      "y\n",
      "  - Processing batch 1/2\n",
      "  - Processing batch 2/2\n",
      "✓ Embedding completed\n",
      "x\n",
      "⏳ Processing document 101/180 (56.1%)\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 64 tokens\n",
      "Processing chunk at position 0/64\n",
      "Processing chunk at position 34/64\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 23 tokens\n",
      "Processing chunk at position 0/23\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 13 tokens\n",
      "Processing chunk at position 0/13\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 26 tokens\n",
      "Processing chunk at position 0/26\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 37 tokens\n",
      "Processing chunk at position 0/37\n",
      "Processing chunk at position 7/37\n",
      "Processing chunk at position 14/37\n",
      "Processing chunk at position 21/37\n",
      "Processing chunk at position 28/37\n",
      "Processing chunk at position 35/37\n",
      "Created 6 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 30 tokens\n",
      "Processing chunk at position 0/30\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 46 tokens\n",
      "Processing chunk at position 0/46\n",
      "Processing chunk at position 16/46\n",
      "Processing chunk at position 32/46\n",
      "Created 3 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 45 tokens\n",
      "Processing chunk at position 0/45\n",
      "Processing chunk at position 15/45\n",
      "Processing chunk at position 30/45\n",
      "Created 3 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 60 tokens\n",
      "Processing chunk at position 0/60\n",
      "Processing chunk at position 30/60\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 38 tokens\n",
      "Processing chunk at position 0/38\n",
      "Processing chunk at position 8/38\n",
      "Processing chunk at position 16/38\n",
      "Processing chunk at position 24/38\n",
      "Processing chunk at position 32/38\n",
      "Created 5 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "⏳ Processing document 111/180 (61.7%)\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 30 tokens\n",
      "Processing chunk at position 0/30\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 50 tokens\n",
      "Processing chunk at position 0/50\n",
      "Processing chunk at position 20/50\n",
      "Processing chunk at position 40/50\n",
      "Created 3 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 27 tokens\n",
      "Processing chunk at position 0/27\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 18 tokens\n",
      "Processing chunk at position 0/18\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 53 tokens\n",
      "Processing chunk at position 0/53\n",
      "Processing chunk at position 23/53\n",
      "Processing chunk at position 46/53\n",
      "Created 3 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 128 tokens\n",
      "Processing chunk at position 0/128\n",
      "Processing chunk at position 98/128\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 72 tokens\n",
      "Processing chunk at position 0/72\n",
      "Processing chunk at position 42/72\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 63 tokens\n",
      "Processing chunk at position 0/63\n",
      "Processing chunk at position 33/63\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 59 tokens\n",
      "Processing chunk at position 0/59\n",
      "Processing chunk at position 29/59\n",
      "Processing chunk at position 58/59\n",
      "Created 3 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 114 tokens\n",
      "Processing chunk at position 0/114\n",
      "Processing chunk at position 84/114\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "⏳ Processing document 121/180 (67.2%)\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 90 tokens\n",
      "Processing chunk at position 0/90\n",
      "Processing chunk at position 60/90\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 28 tokens\n",
      "Processing chunk at position 0/28\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 150 tokens\n",
      "Processing chunk at position 0/150\n",
      "Processing chunk at position 120/150\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 36 tokens\n",
      "Processing chunk at position 0/36\n",
      "Processing chunk at position 6/36\n",
      "Processing chunk at position 12/36\n",
      "Processing chunk at position 18/36\n",
      "Processing chunk at position 24/36\n",
      "Processing chunk at position 30/36\n",
      "Created 6 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 40 tokens\n",
      "Processing chunk at position 0/40\n",
      "Processing chunk at position 10/40\n",
      "Processing chunk at position 20/40\n",
      "Processing chunk at position 30/40\n",
      "Created 4 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 34 tokens\n",
      "Processing chunk at position 0/34\n",
      "Processing chunk at position 4/34\n",
      "Processing chunk at position 8/34\n",
      "Processing chunk at position 12/34\n",
      "Processing chunk at position 16/34\n",
      "Processing chunk at position 20/34\n",
      "Processing chunk at position 24/34\n",
      "Processing chunk at position 28/34\n",
      "Processing chunk at position 32/34\n",
      "Created 9 chunks\n",
      "y\n",
      "  - Processing batch 1/2\n",
      "  - Processing batch 2/2\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 88 tokens\n",
      "Processing chunk at position 0/88\n",
      "Processing chunk at position 58/88\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 81 tokens\n",
      "Processing chunk at position 0/81\n",
      "Processing chunk at position 51/81\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 61 tokens\n",
      "Processing chunk at position 0/61\n",
      "Processing chunk at position 31/61\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 76 tokens\n",
      "Processing chunk at position 0/76\n",
      "Processing chunk at position 46/76\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "⏳ Processing document 131/180 (72.8%)\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 65 tokens\n",
      "Processing chunk at position 0/65\n",
      "Processing chunk at position 35/65\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 53 tokens\n",
      "Processing chunk at position 0/53\n",
      "Processing chunk at position 23/53\n",
      "Processing chunk at position 46/53\n",
      "Created 3 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 20 tokens\n",
      "Processing chunk at position 0/20\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 67 tokens\n",
      "Processing chunk at position 0/67\n",
      "Processing chunk at position 37/67\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 18 tokens\n",
      "Processing chunk at position 0/18\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 19 tokens\n",
      "Processing chunk at position 0/19\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 34 tokens\n",
      "Processing chunk at position 0/34\n",
      "Processing chunk at position 4/34\n",
      "Processing chunk at position 8/34\n",
      "Processing chunk at position 12/34\n",
      "Processing chunk at position 16/34\n",
      "Processing chunk at position 20/34\n",
      "Processing chunk at position 24/34\n",
      "Processing chunk at position 28/34\n",
      "Processing chunk at position 32/34\n",
      "Created 9 chunks\n",
      "y\n",
      "  - Processing batch 1/2\n",
      "  - Processing batch 2/2\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 65 tokens\n",
      "Processing chunk at position 0/65\n",
      "Processing chunk at position 35/65\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 32 tokens\n",
      "Processing chunk at position 0/32\n",
      "Processing chunk at position 2/32\n",
      "Processing chunk at position 4/32\n",
      "Processing chunk at position 6/32\n",
      "Processing chunk at position 8/32\n",
      "Processing chunk at position 10/32\n",
      "Processing chunk at position 12/32\n",
      "Processing chunk at position 14/32\n",
      "Processing chunk at position 16/32\n",
      "Processing chunk at position 18/32\n",
      "Processing chunk at position 20/32\n",
      "Processing chunk at position 22/32\n",
      "Processing chunk at position 24/32\n",
      "Processing chunk at position 26/32\n",
      "Processing chunk at position 28/32\n",
      "Processing chunk at position 30/32\n",
      "Created 16 chunks\n",
      "y\n",
      "  - Processing batch 1/2\n",
      "  - Processing batch 2/2\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 30 tokens\n",
      "Processing chunk at position 0/30\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "⏳ Processing document 141/180 (78.3%)\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 90 tokens\n",
      "Processing chunk at position 0/90\n",
      "Processing chunk at position 60/90\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 83 tokens\n",
      "Processing chunk at position 0/83\n",
      "Processing chunk at position 53/83\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 21 tokens\n",
      "Processing chunk at position 0/21\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 40 tokens\n",
      "Processing chunk at position 0/40\n",
      "Processing chunk at position 10/40\n",
      "Processing chunk at position 20/40\n",
      "Processing chunk at position 30/40\n",
      "Created 4 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 40 tokens\n",
      "Processing chunk at position 0/40\n",
      "Processing chunk at position 10/40\n",
      "Processing chunk at position 20/40\n",
      "Processing chunk at position 30/40\n",
      "Created 4 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 20 tokens\n",
      "Processing chunk at position 0/20\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 26 tokens\n",
      "Processing chunk at position 0/26\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 38 tokens\n",
      "Processing chunk at position 0/38\n",
      "Processing chunk at position 8/38\n",
      "Processing chunk at position 16/38\n",
      "Processing chunk at position 24/38\n",
      "Processing chunk at position 32/38\n",
      "Created 5 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 20 tokens\n",
      "Processing chunk at position 0/20\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 35 tokens\n",
      "Processing chunk at position 0/35\n",
      "Processing chunk at position 5/35\n",
      "Processing chunk at position 10/35\n",
      "Processing chunk at position 15/35\n",
      "Processing chunk at position 20/35\n",
      "Processing chunk at position 25/35\n",
      "Processing chunk at position 30/35\n",
      "Created 7 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "⏳ Processing document 151/180 (83.9%)\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 28 tokens\n",
      "Processing chunk at position 0/28\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 27 tokens\n",
      "Processing chunk at position 0/27\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 27 tokens\n",
      "Processing chunk at position 0/27\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 46 tokens\n",
      "Processing chunk at position 0/46\n",
      "Processing chunk at position 16/46\n",
      "Processing chunk at position 32/46\n",
      "Created 3 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 23 tokens\n",
      "Processing chunk at position 0/23\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 26 tokens\n",
      "Processing chunk at position 0/26\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 38 tokens\n",
      "Processing chunk at position 0/38\n",
      "Processing chunk at position 8/38\n",
      "Processing chunk at position 16/38\n",
      "Processing chunk at position 24/38\n",
      "Processing chunk at position 32/38\n",
      "Created 5 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 19 tokens\n",
      "Processing chunk at position 0/19\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 58 tokens\n",
      "Processing chunk at position 0/58\n",
      "Processing chunk at position 28/58\n",
      "Processing chunk at position 56/58\n",
      "Created 3 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 22 tokens\n",
      "Processing chunk at position 0/22\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "⏳ Processing document 161/180 (89.4%)\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 38 tokens\n",
      "Processing chunk at position 0/38\n",
      "Processing chunk at position 8/38\n",
      "Processing chunk at position 16/38\n",
      "Processing chunk at position 24/38\n",
      "Processing chunk at position 32/38\n",
      "Created 5 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 50 tokens\n",
      "Processing chunk at position 0/50\n",
      "Processing chunk at position 20/50\n",
      "Processing chunk at position 40/50\n",
      "Created 3 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 27 tokens\n",
      "Processing chunk at position 0/27\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 33 tokens\n",
      "Processing chunk at position 0/33\n",
      "Processing chunk at position 3/33\n",
      "Processing chunk at position 6/33\n",
      "Processing chunk at position 9/33\n",
      "Processing chunk at position 12/33\n",
      "Processing chunk at position 15/33\n",
      "Processing chunk at position 18/33\n",
      "Processing chunk at position 21/33\n",
      "Processing chunk at position 24/33\n",
      "Processing chunk at position 27/33\n",
      "Processing chunk at position 30/33\n",
      "Created 11 chunks\n",
      "y\n",
      "  - Processing batch 1/2\n",
      "  - Processing batch 2/2\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 67 tokens\n",
      "Processing chunk at position 0/67\n",
      "Processing chunk at position 37/67\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 47 tokens\n",
      "Processing chunk at position 0/47\n",
      "Processing chunk at position 17/47\n",
      "Processing chunk at position 34/47\n",
      "Created 3 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 80 tokens\n",
      "Processing chunk at position 0/80\n",
      "Processing chunk at position 50/80\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 146 tokens\n",
      "Processing chunk at position 0/146\n",
      "Processing chunk at position 116/146\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 104 tokens\n",
      "Processing chunk at position 0/104\n",
      "Processing chunk at position 74/104\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 33 tokens\n",
      "Processing chunk at position 0/33\n",
      "Processing chunk at position 3/33\n",
      "Processing chunk at position 6/33\n",
      "Processing chunk at position 9/33\n",
      "Processing chunk at position 12/33\n",
      "Processing chunk at position 15/33\n",
      "Processing chunk at position 18/33\n",
      "Processing chunk at position 21/33\n",
      "Processing chunk at position 24/33\n",
      "Processing chunk at position 27/33\n",
      "Processing chunk at position 30/33\n",
      "Created 11 chunks\n",
      "y\n",
      "  - Processing batch 1/2\n",
      "  - Processing batch 2/2\n",
      "✓ Embedding completed\n",
      "x\n",
      "⏳ Processing document 171/180 (95.0%)\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 47 tokens\n",
      "Processing chunk at position 0/47\n",
      "Processing chunk at position 17/47\n",
      "Processing chunk at position 34/47\n",
      "Created 3 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 33 tokens\n",
      "Processing chunk at position 0/33\n",
      "Processing chunk at position 3/33\n",
      "Processing chunk at position 6/33\n",
      "Processing chunk at position 9/33\n",
      "Processing chunk at position 12/33\n",
      "Processing chunk at position 15/33\n",
      "Processing chunk at position 18/33\n",
      "Processing chunk at position 21/33\n",
      "Processing chunk at position 24/33\n",
      "Processing chunk at position 27/33\n",
      "Processing chunk at position 30/33\n",
      "Created 11 chunks\n",
      "y\n",
      "  - Processing batch 1/2\n",
      "  - Processing batch 2/2\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 170 tokens\n",
      "Processing chunk at position 0/170\n",
      "Processing chunk at position 140/170\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 104 tokens\n",
      "Processing chunk at position 0/104\n",
      "Processing chunk at position 74/104\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 133 tokens\n",
      "Processing chunk at position 0/133\n",
      "Processing chunk at position 103/133\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 72 tokens\n",
      "Processing chunk at position 0/72\n",
      "Processing chunk at position 42/72\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 116 tokens\n",
      "Processing chunk at position 0/116\n",
      "Processing chunk at position 86/116\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 29 tokens\n",
      "Processing chunk at position 0/29\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 134 tokens\n",
      "Processing chunk at position 0/134\n",
      "Processing chunk at position 104/134\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 79 tokens\n",
      "Processing chunk at position 0/79\n",
      "Processing chunk at position 49/79\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "\n",
      "✅ Embedding completed in 4.14s\n",
      "📊 Statistics:\n",
      "  - Documents processed: 180\n",
      "  - Total tokens: 12374\n",
      "  - Total chunks created: 524\n",
      "  - Average chunks per document: 2.91\n",
      "  - Processing speed: 2991.66 tokens/second\n",
      "  - Documents with errors: 0\n",
      "  - Success rate: 100.00%\n",
      "  - Embedding memory usage: 0.77 MB\n",
      "Processed /app/data/parsed/EPO/EPRTBJV2025000024001001/EPW1B9/EP13899497W1B9/EP13899497W1B9.json, total documents: 1456\n",
      "/app/data/parsed/EPO/EPRTBJV2025000024001001/EPW1B9/EP22169662W1B9/EP22169662W1B9.json\n",
      "📋 Starting document embedding process for 16 documents\n",
      "⏳ Processing document 1/16 (6.2%)\n",
      "\n",
      "📄 Document 1 Preview:\n",
      "  - Content: MONITORING OF A COMMERCIAL VEHICLE\n",
      "  - Metadata: {'doc_id': 'EP22169662B9W1', 'language': 'de', 'country': 'EP', 'doc_number': '4270346', 'application_number': '22169662.8', 'publication_date': '20250611', 'ipc_classes': ['G08B  21/02        20060101AFI20240902BHEP', 'G08B  29/18        20060101ALI20240902BHEP', 'G08B  31/00        20060101ALI20240902BHEP', 'G08G   1/00        20060101ALI20240902BHEP'], 'file': 'EP22169662W1B9.xml', 'section': 'title'}\n",
      "  - Token count: 5\n",
      "  - Calculated chunk size: 5\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 5 tokens\n",
      "Processing chunk at position 0/5\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Chunks created: 1 (in 0.00s)\n",
      "  - First chunk: monitoring of a commercial vehicle...\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "  - Embeddings generated: 1 vectors of shape (384,)\n",
      "  - Embedding time: 0.01s (0.0063s per chunk)\n",
      "\n",
      "📄 Document 2 Preview:\n",
      "  - Content: Verfahren ausgeführt durch einen Server (2), wobei das Verfahren umfasst: - Bereithalten (300) einer...\n",
      "  - Metadata: {'doc_id': 'EP22169662B9W1', 'language': 'de', 'country': 'EP', 'doc_number': '4270346', 'application_number': '22169662.8', 'publication_date': '20250611', 'ipc_classes': ['G08B  21/02        20060101AFI20240902BHEP', 'G08B  29/18        20060101ALI20240902BHEP', 'G08B  31/00        20060101ALI20240902BHEP', 'G08G   1/00        20060101ALI20240902BHEP'], 'file': 'EP22169662W1B9.xml', 'section': 'claim', 'claim_number': '0001'}\n",
      "  - Token count: 479\n",
      "  - Calculated chunk size: 240\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 479 tokens\n",
      "Processing chunk at position 0/479\n",
      "Processing chunk at position 210/479\n",
      "Processing chunk at position 420/479\n",
      "Created 3 chunks\n",
      "y\n",
      "  - Chunks created: 3 (in 0.00s)\n",
      "  - First chunk: verfahren ausgefuhrt durch einen server ( 2 ), wob...\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "  - Embeddings generated: 3 vectors of shape (384,)\n",
      "  - Embedding time: 0.07s (0.0228s per chunk)\n",
      "\n",
      "📄 Document 3 Preview:\n",
      "  - Content: Verfahren nach Anspruch 1, wobei Bestimmen, ob die zeitliche Überfälligkeit ein zu erwartendes Ereig...\n",
      "  - Metadata: {'doc_id': 'EP22169662B9W1', 'language': 'de', 'country': 'EP', 'doc_number': '4270346', 'application_number': '22169662.8', 'publication_date': '20250611', 'ipc_classes': ['G08B  21/02        20060101AFI20240902BHEP', 'G08B  29/18        20060101ALI20240902BHEP', 'G08B  31/00        20060101ALI20240902BHEP', 'G08G   1/00        20060101ALI20240902BHEP'], 'file': 'EP22169662W1B9.xml', 'section': 'claim', 'claim_number': '0002'}\n",
      "  - Token count: 149\n",
      "  - Calculated chunk size: 149\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 149 tokens\n",
      "Processing chunk at position 0/149\n",
      "Processing chunk at position 119/149\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Chunks created: 2 (in 0.00s)\n",
      "  - First chunk: verfahren nach anspruch 1, wobei bestimmen, ob die...\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "  - Embeddings generated: 2 vectors of shape (384,)\n",
      "  - Embedding time: 0.04s (0.0182s per chunk)\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 98 tokens\n",
      "Processing chunk at position 0/98\n",
      "Processing chunk at position 68/98\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 142 tokens\n",
      "Processing chunk at position 0/142\n",
      "Processing chunk at position 112/142\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 148 tokens\n",
      "Processing chunk at position 0/148\n",
      "Processing chunk at position 118/148\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 83 tokens\n",
      "Processing chunk at position 0/83\n",
      "Processing chunk at position 53/83\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 103 tokens\n",
      "Processing chunk at position 0/103\n",
      "Processing chunk at position 73/103\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 92 tokens\n",
      "Processing chunk at position 0/92\n",
      "Processing chunk at position 62/92\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 94 tokens\n",
      "Processing chunk at position 0/94\n",
      "Processing chunk at position 64/94\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "⏳ Processing document 11/16 (68.8%)\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 60 tokens\n",
      "Processing chunk at position 0/60\n",
      "Processing chunk at position 30/60\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 62 tokens\n",
      "Processing chunk at position 0/62\n",
      "Processing chunk at position 32/62\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 69 tokens\n",
      "Processing chunk at position 0/69\n",
      "Processing chunk at position 39/69\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 43 tokens\n",
      "Processing chunk at position 0/43\n",
      "Processing chunk at position 13/43\n",
      "Processing chunk at position 26/43\n",
      "Processing chunk at position 39/43\n",
      "Created 4 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 51 tokens\n",
      "Processing chunk at position 0/51\n",
      "Processing chunk at position 21/51\n",
      "Processing chunk at position 42/51\n",
      "Created 3 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 84 tokens\n",
      "Processing chunk at position 0/84\n",
      "Processing chunk at position 54/84\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "\n",
      "✅ Embedding completed in 0.42s\n",
      "📊 Statistics:\n",
      "  - Documents processed: 16\n",
      "  - Total tokens: 1762\n",
      "  - Total chunks created: 35\n",
      "  - Average chunks per document: 2.19\n",
      "  - Processing speed: 4210.94 tokens/second\n",
      "  - Documents with errors: 0\n",
      "  - Success rate: 100.00%\n",
      "  - Embedding memory usage: 0.05 MB\n",
      "Processed /app/data/parsed/EPO/EPRTBJV2025000024001001/EPW1B9/EP22169662W1B9/EP22169662W1B9.json, total documents: 1491\n",
      "/app/data/parsed/EPO/EPRTBJV2025000024001001/EPW1A8/EP23166881W1A8/EP23166881W1A8.json\n",
      "📋 Starting document embedding process for 2 documents\n",
      "⏳ Processing document 1/2 (50.0%)\n",
      "\n",
      "📄 Document 1 Preview:\n",
      "  - Content: SHAMPOO APPLICATOR AND REMOVER FOR CLEANSING HAIR TO NEAR DRY CONDITIONS\n",
      "  - Metadata: {'doc_id': 'EP23166881A8W1', 'language': 'en', 'country': 'EP', 'doc_number': '4223176', 'application_number': '23166881.5', 'publication_date': '20250611', 'ipc_classes': ['A45D  19/00        20060101AFI20230704BHEP', 'A45D  19/02        20060101ALI20230704BHEP', 'A45D  19/16        20060101ALI20230704BHEP', 'A45D  24/22        20060101ALI20230704BHEP', 'A45D  24/30        20060101ALI20230704BHEP', 'A45D  24/32        20060101ALI20230704BHEP'], 'file': 'EP23166881W1A8.xml', 'section': 'title'}\n",
      "  - Token count: 15\n",
      "  - Calculated chunk size: 15\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 15 tokens\n",
      "Processing chunk at position 0/15\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Chunks created: 1 (in 0.00s)\n",
      "  - First chunk: shampoo applicator and remover for cleansing hair ...\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "  - Embeddings generated: 1 vectors of shape (384,)\n",
      "  - Embedding time: 0.01s (0.0051s per chunk)\n",
      "\n",
      "📄 Document 2 Preview:\n",
      "  - Content: A device for application of hair formulation includes a body structure having one or more tines at a...\n",
      "  - Metadata: {'doc_id': 'EP23166881A8W1', 'language': 'en', 'country': 'EP', 'doc_number': '4223176', 'application_number': '23166881.5', 'publication_date': '20250611', 'ipc_classes': ['A45D  19/00        20060101AFI20230704BHEP', 'A45D  19/02        20060101ALI20230704BHEP', 'A45D  19/16        20060101ALI20230704BHEP', 'A45D  24/22        20060101ALI20230704BHEP', 'A45D  24/30        20060101ALI20230704BHEP', 'A45D  24/32        20060101ALI20230704BHEP'], 'file': 'EP23166881W1A8.xml', 'section': 'abstract'}\n",
      "  - Token count: 72\n",
      "  - Calculated chunk size: 72\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 72 tokens\n",
      "Processing chunk at position 0/72\n",
      "Processing chunk at position 42/72\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Chunks created: 2 (in 0.00s)\n",
      "  - First chunk: a device for application of hair formulation inclu...\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "  - Embeddings generated: 2 vectors of shape (384,)\n",
      "  - Embedding time: 0.02s (0.0081s per chunk)\n",
      "\n",
      "✅ Embedding completed in 0.02s\n",
      "📊 Statistics:\n",
      "  - Documents processed: 2\n",
      "  - Total tokens: 87\n",
      "  - Total chunks created: 3\n",
      "  - Average chunks per document: 1.50\n",
      "  - Processing speed: 3975.60 tokens/second\n",
      "  - Documents with errors: 0\n",
      "  - Success rate: 100.00%\n",
      "  - Embedding memory usage: 0.00 MB\n",
      "Processed /app/data/parsed/EPO/EPRTBJV2025000024001001/EPW1A8/EP23166881W1A8/EP23166881W1A8.json, total documents: 1494\n",
      "/app/data/parsed/EPO/EPRTBJV2025000024001001/EPW1A8/EP23205649W1A8/EP23205649W1A8.json\n",
      "📋 Starting document embedding process for 2 documents\n",
      "⏳ Processing document 1/2 (50.0%)\n",
      "\n",
      "📄 Document 1 Preview:\n",
      "  - Content: ENERGY STORE AND PROTECTION AGAINST INVERTED POLARITY FOR SAID ENERGY STORE\n",
      "  - Metadata: {'doc_id': 'EP23205649A8W1', 'language': 'de', 'country': 'EP', 'doc_number': '4546553', 'application_number': '23205649.9', 'publication_date': '20250611', 'ipc_classes': ['H01M  50/588       20210101AFI20240415BHEP', 'H01M  50/597       20210101ALI20240415BHEP', 'H01R  11/28        20060101ALI20240415BHEP'], 'file': 'EP23205649W1A8.xml', 'section': 'title'}\n",
      "  - Token count: 12\n",
      "  - Calculated chunk size: 12\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 12 tokens\n",
      "Processing chunk at position 0/12\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Chunks created: 1 (in 0.00s)\n",
      "  - First chunk: energy store and protection against inverted polar...\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "  - Embeddings generated: 1 vectors of shape (384,)\n",
      "  - Embedding time: 0.00s (0.0045s per chunk)\n",
      "\n",
      "📄 Document 2 Preview:\n",
      "  - Content: Es wird ein Energiespeicher (1) und ein Verpolungsschutz (1) für diesen Energiespeicher (2), insbeso...\n",
      "  - Metadata: {'doc_id': 'EP23205649A8W1', 'language': 'de', 'country': 'EP', 'doc_number': '4546553', 'application_number': '23205649.9', 'publication_date': '20250611', 'ipc_classes': ['H01M  50/588       20210101AFI20240415BHEP', 'H01M  50/597       20210101ALI20240415BHEP', 'H01R  11/28        20060101ALI20240415BHEP'], 'file': 'EP23205649W1A8.xml', 'section': 'abstract'}\n",
      "  - Token count: 255\n",
      "  - Calculated chunk size: 255\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 255 tokens\n",
      "Processing chunk at position 0/255\n",
      "Processing chunk at position 225/255\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Chunks created: 2 (in 0.00s)\n",
      "  - First chunk: es wird ein energiespeicher ( 1 ) und ein verpolun...\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "  - Embeddings generated: 2 vectors of shape (384,)\n",
      "  - Embedding time: 0.05s (0.0241s per chunk)\n",
      "\n",
      "✅ Embedding completed in 0.05s\n",
      "📊 Statistics:\n",
      "  - Documents processed: 2\n",
      "  - Total tokens: 267\n",
      "  - Total chunks created: 3\n",
      "  - Average chunks per document: 1.50\n",
      "  - Processing speed: 4972.38 tokens/second\n",
      "  - Documents with errors: 0\n",
      "  - Success rate: 100.00%\n",
      "  - Embedding memory usage: 0.00 MB\n",
      "Processed /app/data/parsed/EPO/EPRTBJV2025000024001001/EPW1A8/EP23205649W1A8/EP23205649W1A8.json, total documents: 1497\n",
      "/app/data/parsed/EPO/EPRTBJV2025000024001001/EPW1A9/EP23204356W1A9/EP23204356W1A9.json\n",
      "📋 Starting document embedding process for 16 documents\n",
      "⏳ Processing document 1/16 (6.2%)\n",
      "\n",
      "📄 Document 1 Preview:\n",
      "  - Content: SEMI-HOLLOW SELF-PIERCING RIVET AND JOINING METHOD USING SAME\n",
      "  - Metadata: {'doc_id': 'EP23204356A9W1', 'language': 'en', 'country': 'EP', 'doc_number': '4542062', 'application_number': '23204356.2', 'publication_date': '20250611', 'ipc_classes': ['F16B   5/04        20060101AFI20240223BHEP', 'B21J  15/02        20060101ALI20240223BHEP', 'B21J  15/14        20060101ALI20240223BHEP', 'B29C  65/56        20060101ALI20240223BHEP', 'B29C  65/00        20060101ALI20240223BHEP', 'F16B  19/06        20060101ALI20240223BHEP', 'F16B  19/08        20060101ALI20240223BHEP'], 'file': 'EP23204356W1A9.xml', 'section': 'title'}\n",
      "  - Token count: 13\n",
      "  - Calculated chunk size: 13\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 13 tokens\n",
      "Processing chunk at position 0/13\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Chunks created: 1 (in 0.00s)\n",
      "  - First chunk: semi - hollow self - piercing rivet and joining me...\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "  - Embeddings generated: 1 vectors of shape (384,)\n",
      "  - Embedding time: 0.03s (0.0262s per chunk)\n",
      "\n",
      "📄 Document 2 Preview:\n",
      "  - Content: A semi-tubular self-piercing rivet (10) comprising a head (12), and a shaft (14), wherein the shaft ...\n",
      "  - Metadata: {'doc_id': 'EP23204356A9W1', 'language': 'en', 'country': 'EP', 'doc_number': '4542062', 'application_number': '23204356.2', 'publication_date': '20250611', 'ipc_classes': ['F16B   5/04        20060101AFI20240223BHEP', 'B21J  15/02        20060101ALI20240223BHEP', 'B21J  15/14        20060101ALI20240223BHEP', 'B29C  65/56        20060101ALI20240223BHEP', 'B29C  65/00        20060101ALI20240223BHEP', 'F16B  19/06        20060101ALI20240223BHEP', 'F16B  19/08        20060101ALI20240223BHEP'], 'file': 'EP23204356W1A9.xml', 'section': 'abstract'}\n",
      "  - Token count: 219\n",
      "  - Calculated chunk size: 219\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 219 tokens\n",
      "Processing chunk at position 0/219\n",
      "Processing chunk at position 189/219\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Chunks created: 2 (in 0.00s)\n",
      "  - First chunk: a semi - tubular self - piercing rivet ( 10 ) comp...\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "  - Embeddings generated: 2 vectors of shape (384,)\n",
      "  - Embedding time: 0.05s (0.0270s per chunk)\n",
      "\n",
      "📄 Document 3 Preview:\n",
      "  - Content: A semi-tubular self-piercing rivet (10) comprising - a head (12), and - a shaft (14), and wherein th...\n",
      "  - Metadata: {'doc_id': 'EP23204356A9W1', 'language': 'en', 'country': 'EP', 'doc_number': '4542062', 'application_number': '23204356.2', 'publication_date': '20250611', 'ipc_classes': ['F16B   5/04        20060101AFI20240223BHEP', 'B21J  15/02        20060101ALI20240223BHEP', 'B21J  15/14        20060101ALI20240223BHEP', 'B29C  65/56        20060101ALI20240223BHEP', 'B29C  65/00        20060101ALI20240223BHEP', 'F16B  19/06        20060101ALI20240223BHEP', 'F16B  19/08        20060101ALI20240223BHEP'], 'file': 'EP23204356W1A9.xml', 'section': 'claim', 'claim_number': '0001'}\n",
      "  - Token count: 172\n",
      "  - Calculated chunk size: 172\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 172 tokens\n",
      "Processing chunk at position 0/172\n",
      "Processing chunk at position 142/172\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Chunks created: 2 (in 0.00s)\n",
      "  - First chunk: a semi - tubular self - piercing rivet ( 10 ) comp...\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "  - Embeddings generated: 2 vectors of shape (384,)\n",
      "  - Embedding time: 0.04s (0.0197s per chunk)\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 18 tokens\n",
      "Processing chunk at position 0/18\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 19 tokens\n",
      "Processing chunk at position 0/19\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 194 tokens\n",
      "Processing chunk at position 0/194\n",
      "Processing chunk at position 164/194\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 12 tokens\n",
      "Processing chunk at position 0/12\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 12 tokens\n",
      "Processing chunk at position 0/12\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 12 tokens\n",
      "Processing chunk at position 0/12\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 7 tokens\n",
      "Processing chunk at position 0/7\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "⏳ Processing document 11/16 (68.8%)\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 12 tokens\n",
      "Processing chunk at position 0/12\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 452 tokens\n",
      "Processing chunk at position 0/452\n",
      "Processing chunk at position 196/452\n",
      "Processing chunk at position 392/452\n",
      "Created 3 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 65 tokens\n",
      "Processing chunk at position 0/65\n",
      "Processing chunk at position 35/65\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 76 tokens\n",
      "Processing chunk at position 0/76\n",
      "Processing chunk at position 46/76\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 63 tokens\n",
      "Processing chunk at position 0/63\n",
      "Processing chunk at position 33/63\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 32 tokens\n",
      "Processing chunk at position 0/32\n",
      "Processing chunk at position 2/32\n",
      "Processing chunk at position 4/32\n",
      "Processing chunk at position 6/32\n",
      "Processing chunk at position 8/32\n",
      "Processing chunk at position 10/32\n",
      "Processing chunk at position 12/32\n",
      "Processing chunk at position 14/32\n",
      "Processing chunk at position 16/32\n",
      "Processing chunk at position 18/32\n",
      "Processing chunk at position 20/32\n",
      "Processing chunk at position 22/32\n",
      "Processing chunk at position 24/32\n",
      "Processing chunk at position 26/32\n",
      "Processing chunk at position 28/32\n",
      "Processing chunk at position 30/32\n",
      "Created 16 chunks\n",
      "y\n",
      "  - Processing batch 1/2\n",
      "  - Processing batch 2/2\n",
      "✓ Embedding completed\n",
      "x\n",
      "\n",
      "✅ Embedding completed in 0.43s\n",
      "📊 Statistics:\n",
      "  - Documents processed: 16\n",
      "  - Total tokens: 1378\n",
      "  - Total chunks created: 39\n",
      "  - Average chunks per document: 2.44\n",
      "  - Processing speed: 3227.02 tokens/second\n",
      "  - Documents with errors: 0\n",
      "  - Success rate: 100.00%\n",
      "  - Embedding memory usage: 0.06 MB\n",
      "Processed /app/data/parsed/EPO/EPRTBJV2025000024001001/EPW1A9/EP23204356W1A9/EP23204356W1A9.json, total documents: 1536\n",
      "/app/data/parsed/EPO/EPRTBJV2025000024001001/EPW1A9/EP24185369W1A9/EP24185369W1A9.json\n",
      "📋 Starting document embedding process for 111 documents\n",
      "⏳ Processing document 1/111 (0.9%)\n",
      "\n",
      "📄 Document 1 Preview:\n",
      "  - Content: POLYIMIDE VARNISH FOR HIGH-FUNCTIONAL CONDUCTOR COATING AND POLYIMIDE COATING PREPARED THEREFROM\n",
      "  - Metadata: {'doc_id': 'EP24185369A9W1', 'language': 'en', 'country': 'EP', 'doc_number': '4484467', 'application_number': '24185369.6', 'publication_date': '20250611', 'ipc_classes': ['C08G  73/10        20060101AFI20241125BHEP', 'C08K   3/36        20060101ALI20241125BHEP', 'C08K   3/38        20060101ALI20241125BHEP', 'C08K   9/06        20060101ALI20241125BHEP', 'C09D 179/08        20060101ALI20241125BHEP'], 'file': 'EP24185369W1A9.xml', 'section': 'title'}\n",
      "  - Token count: 20\n",
      "  - Calculated chunk size: 20\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 20 tokens\n",
      "Processing chunk at position 0/20\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Chunks created: 1 (in 0.00s)\n",
      "  - First chunk: polyimide varnish for high - functional conductor ...\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "  - Embeddings generated: 1 vectors of shape (384,)\n",
      "  - Embedding time: 0.03s (0.0305s per chunk)\n",
      "\n",
      "📄 Document 2 Preview:\n",
      "  - Content: Provided is polyimide varnish comprising: a polyamic acid solution containing diamine monomer and di...\n",
      "  - Metadata: {'doc_id': 'EP24185369A9W1', 'language': 'en', 'country': 'EP', 'doc_number': '4484467', 'application_number': '24185369.6', 'publication_date': '20250611', 'ipc_classes': ['C08G  73/10        20060101AFI20241125BHEP', 'C08K   3/36        20060101ALI20241125BHEP', 'C08K   3/38        20060101ALI20241125BHEP', 'C08K   9/06        20060101ALI20241125BHEP', 'C09D 179/08        20060101ALI20241125BHEP'], 'file': 'EP24185369W1A9.xml', 'section': 'abstract'}\n",
      "  - Token count: 56\n",
      "  - Calculated chunk size: 56\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 56 tokens\n",
      "Processing chunk at position 0/56\n",
      "Processing chunk at position 26/56\n",
      "Processing chunk at position 52/56\n",
      "Created 3 chunks\n",
      "y\n",
      "  - Chunks created: 3 (in 0.00s)\n",
      "  - First chunk: provided is polyimide varnish comprising : a polya...\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "  - Embeddings generated: 3 vectors of shape (384,)\n",
      "  - Embedding time: 0.02s (0.0083s per chunk)\n",
      "\n",
      "📄 Document 3 Preview:\n",
      "  - Content: Polyimide varnish comprising: a polyamic acid solution containing diamine monomer and dianhydride mo...\n",
      "  - Metadata: {'doc_id': 'EP24185369A9W1', 'language': 'en', 'country': 'EP', 'doc_number': '4484467', 'application_number': '24185369.6', 'publication_date': '20250611', 'ipc_classes': ['C08G  73/10        20060101AFI20241125BHEP', 'C08K   3/36        20060101ALI20241125BHEP', 'C08K   3/38        20060101ALI20241125BHEP', 'C08K   9/06        20060101ALI20241125BHEP', 'C09D 179/08        20060101ALI20241125BHEP'], 'file': 'EP24185369W1A9.xml', 'section': 'claim', 'claim_number': '0001'}\n",
      "  - Token count: 54\n",
      "  - Calculated chunk size: 54\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 54 tokens\n",
      "Processing chunk at position 0/54\n",
      "Processing chunk at position 24/54\n",
      "Processing chunk at position 48/54\n",
      "Created 3 chunks\n",
      "y\n",
      "  - Chunks created: 3 (in 0.00s)\n",
      "  - First chunk: polyimide varnish comprising : a polyamic acid sol...\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "  - Embeddings generated: 3 vectors of shape (384,)\n",
      "  - Embedding time: 0.02s (0.0073s per chunk)\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 39 tokens\n",
      "Processing chunk at position 0/39\n",
      "Processing chunk at position 9/39\n",
      "Processing chunk at position 18/39\n",
      "Processing chunk at position 27/39\n",
      "Processing chunk at position 36/39\n",
      "Created 5 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 44 tokens\n",
      "Processing chunk at position 0/44\n",
      "Processing chunk at position 14/44\n",
      "Processing chunk at position 28/44\n",
      "Processing chunk at position 42/44\n",
      "Created 4 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 47 tokens\n",
      "Processing chunk at position 0/47\n",
      "Processing chunk at position 17/47\n",
      "Processing chunk at position 34/47\n",
      "Created 3 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 142 tokens\n",
      "Processing chunk at position 0/142\n",
      "Processing chunk at position 112/142\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 38 tokens\n",
      "Processing chunk at position 0/38\n",
      "Processing chunk at position 8/38\n",
      "Processing chunk at position 16/38\n",
      "Processing chunk at position 24/38\n",
      "Processing chunk at position 32/38\n",
      "Created 5 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 548 tokens\n",
      "Processing chunk at position 0/548\n",
      "Processing chunk at position 244/548\n",
      "Processing chunk at position 488/548\n",
      "Created 3 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 1773 tokens\n",
      "Processing chunk at position 0/1773\n",
      "Processing chunk at position 266/1773\n",
      "Processing chunk at position 532/1773\n",
      "Processing chunk at position 798/1773\n",
      "Processing chunk at position 1064/1773\n",
      "Processing chunk at position 1330/1773\n",
      "Processing chunk at position 1596/1773\n",
      "Created 7 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "⏳ Processing document 11/111 (9.9%)\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 27 tokens\n",
      "Processing chunk at position 0/27\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 23 tokens\n",
      "Processing chunk at position 0/23\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 42 tokens\n",
      "Processing chunk at position 0/42\n",
      "Processing chunk at position 12/42\n",
      "Processing chunk at position 24/42\n",
      "Processing chunk at position 36/42\n",
      "Created 4 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 40 tokens\n",
      "Processing chunk at position 0/40\n",
      "Processing chunk at position 10/40\n",
      "Processing chunk at position 20/40\n",
      "Processing chunk at position 30/40\n",
      "Created 4 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 58 tokens\n",
      "Processing chunk at position 0/58\n",
      "Processing chunk at position 28/58\n",
      "Processing chunk at position 56/58\n",
      "Created 3 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 50 tokens\n",
      "Processing chunk at position 0/50\n",
      "Processing chunk at position 20/50\n",
      "Processing chunk at position 40/50\n",
      "Created 3 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 35 tokens\n",
      "Processing chunk at position 0/35\n",
      "Processing chunk at position 5/35\n",
      "Processing chunk at position 10/35\n",
      "Processing chunk at position 15/35\n",
      "Processing chunk at position 20/35\n",
      "Processing chunk at position 25/35\n",
      "Processing chunk at position 30/35\n",
      "Created 7 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 70 tokens\n",
      "Processing chunk at position 0/70\n",
      "Processing chunk at position 40/70\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 103 tokens\n",
      "Processing chunk at position 0/103\n",
      "Processing chunk at position 73/103\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 134 tokens\n",
      "Processing chunk at position 0/134\n",
      "Processing chunk at position 104/134\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "⏳ Processing document 21/111 (18.9%)\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 105 tokens\n",
      "Processing chunk at position 0/105\n",
      "Processing chunk at position 75/105\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 110 tokens\n",
      "Processing chunk at position 0/110\n",
      "Processing chunk at position 80/110\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 66 tokens\n",
      "Processing chunk at position 0/66\n",
      "Processing chunk at position 36/66\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 56 tokens\n",
      "Processing chunk at position 0/56\n",
      "Processing chunk at position 26/56\n",
      "Processing chunk at position 52/56\n",
      "Created 3 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 51 tokens\n",
      "Processing chunk at position 0/51\n",
      "Processing chunk at position 21/51\n",
      "Processing chunk at position 42/51\n",
      "Created 3 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 47 tokens\n",
      "Processing chunk at position 0/47\n",
      "Processing chunk at position 17/47\n",
      "Processing chunk at position 34/47\n",
      "Created 3 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 79 tokens\n",
      "Processing chunk at position 0/79\n",
      "Processing chunk at position 49/79\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 118 tokens\n",
      "Processing chunk at position 0/118\n",
      "Processing chunk at position 88/118\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 73 tokens\n",
      "Processing chunk at position 0/73\n",
      "Processing chunk at position 43/73\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 48 tokens\n",
      "Processing chunk at position 0/48\n",
      "Processing chunk at position 18/48\n",
      "Processing chunk at position 36/48\n",
      "Created 3 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "⏳ Processing document 31/111 (27.9%)\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 85 tokens\n",
      "Processing chunk at position 0/85\n",
      "Processing chunk at position 55/85\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 62 tokens\n",
      "Processing chunk at position 0/62\n",
      "Processing chunk at position 32/62\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 110 tokens\n",
      "Processing chunk at position 0/110\n",
      "Processing chunk at position 80/110\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 28 tokens\n",
      "Processing chunk at position 0/28\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 58 tokens\n",
      "Processing chunk at position 0/58\n",
      "Processing chunk at position 28/58\n",
      "Processing chunk at position 56/58\n",
      "Created 3 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 1840 tokens\n",
      "Processing chunk at position 0/1840\n",
      "Processing chunk at position 277/1840\n",
      "Processing chunk at position 554/1840\n",
      "Processing chunk at position 831/1840\n",
      "Processing chunk at position 1108/1840\n",
      "Processing chunk at position 1385/1840\n",
      "Processing chunk at position 1662/1840\n",
      "Created 7 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 553 tokens\n",
      "Processing chunk at position 0/553\n",
      "Processing chunk at position 247/553\n",
      "Processing chunk at position 494/553\n",
      "Created 3 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 108 tokens\n",
      "Processing chunk at position 0/108\n",
      "Processing chunk at position 78/108\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 129 tokens\n",
      "Processing chunk at position 0/129\n",
      "Processing chunk at position 99/129\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 127 tokens\n",
      "Processing chunk at position 0/127\n",
      "Processing chunk at position 97/127\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "⏳ Processing document 41/111 (36.9%)\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 56 tokens\n",
      "Processing chunk at position 0/56\n",
      "Processing chunk at position 26/56\n",
      "Processing chunk at position 52/56\n",
      "Created 3 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 56 tokens\n",
      "Processing chunk at position 0/56\n",
      "Processing chunk at position 26/56\n",
      "Processing chunk at position 52/56\n",
      "Created 3 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 176 tokens\n",
      "Processing chunk at position 0/176\n",
      "Processing chunk at position 146/176\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 36 tokens\n",
      "Processing chunk at position 0/36\n",
      "Processing chunk at position 6/36\n",
      "Processing chunk at position 12/36\n",
      "Processing chunk at position 18/36\n",
      "Processing chunk at position 24/36\n",
      "Processing chunk at position 30/36\n",
      "Created 6 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 14 tokens\n",
      "Processing chunk at position 0/14\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 159 tokens\n",
      "Processing chunk at position 0/159\n",
      "Processing chunk at position 129/159\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 23 tokens\n",
      "Processing chunk at position 0/23\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 64 tokens\n",
      "Processing chunk at position 0/64\n",
      "Processing chunk at position 34/64\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 90 tokens\n",
      "Processing chunk at position 0/90\n",
      "Processing chunk at position 60/90\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 165 tokens\n",
      "Processing chunk at position 0/165\n",
      "Processing chunk at position 135/165\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "⏳ Processing document 51/111 (45.9%)\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 72 tokens\n",
      "Processing chunk at position 0/72\n",
      "Processing chunk at position 42/72\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 21 tokens\n",
      "Processing chunk at position 0/21\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 179 tokens\n",
      "Processing chunk at position 0/179\n",
      "Processing chunk at position 149/179\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 61 tokens\n",
      "Processing chunk at position 0/61\n",
      "Processing chunk at position 31/61\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 72 tokens\n",
      "Processing chunk at position 0/72\n",
      "Processing chunk at position 42/72\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 30 tokens\n",
      "Processing chunk at position 0/30\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 180 tokens\n",
      "Processing chunk at position 0/180\n",
      "Processing chunk at position 150/180\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 82 tokens\n",
      "Processing chunk at position 0/82\n",
      "Processing chunk at position 52/82\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 49 tokens\n",
      "Processing chunk at position 0/49\n",
      "Processing chunk at position 19/49\n",
      "Processing chunk at position 38/49\n",
      "Created 3 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 260 tokens\n",
      "Processing chunk at position 0/260\n",
      "Processing chunk at position 230/260\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "⏳ Processing document 61/111 (55.0%)\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 228 tokens\n",
      "Processing chunk at position 0/228\n",
      "Processing chunk at position 198/228\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 142 tokens\n",
      "Processing chunk at position 0/142\n",
      "Processing chunk at position 112/142\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 78 tokens\n",
      "Processing chunk at position 0/78\n",
      "Processing chunk at position 48/78\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 67 tokens\n",
      "Processing chunk at position 0/67\n",
      "Processing chunk at position 37/67\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 107 tokens\n",
      "Processing chunk at position 0/107\n",
      "Processing chunk at position 77/107\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 87 tokens\n",
      "Processing chunk at position 0/87\n",
      "Processing chunk at position 57/87\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 181 tokens\n",
      "Processing chunk at position 0/181\n",
      "Processing chunk at position 151/181\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 72 tokens\n",
      "Processing chunk at position 0/72\n",
      "Processing chunk at position 42/72\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 125 tokens\n",
      "Processing chunk at position 0/125\n",
      "Processing chunk at position 95/125\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 55 tokens\n",
      "Processing chunk at position 0/55\n",
      "Processing chunk at position 25/55\n",
      "Processing chunk at position 50/55\n",
      "Created 3 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "⏳ Processing document 71/111 (64.0%)\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 99 tokens\n",
      "Processing chunk at position 0/99\n",
      "Processing chunk at position 69/99\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 125 tokens\n",
      "Processing chunk at position 0/125\n",
      "Processing chunk at position 95/125\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 102 tokens\n",
      "Processing chunk at position 0/102\n",
      "Processing chunk at position 72/102\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 194 tokens\n",
      "Processing chunk at position 0/194\n",
      "Processing chunk at position 164/194\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 135 tokens\n",
      "Processing chunk at position 0/135\n",
      "Processing chunk at position 105/135\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 135 tokens\n",
      "Processing chunk at position 0/135\n",
      "Processing chunk at position 105/135\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 131 tokens\n",
      "Processing chunk at position 0/131\n",
      "Processing chunk at position 101/131\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 31 tokens\n",
      "Processing chunk at position 0/31\n",
      "Processing chunk at position 1/31\n",
      "Processing chunk at position 2/31\n",
      "Processing chunk at position 3/31\n",
      "Processing chunk at position 4/31\n",
      "Processing chunk at position 5/31\n",
      "Processing chunk at position 6/31\n",
      "Processing chunk at position 7/31\n",
      "Processing chunk at position 8/31\n",
      "Processing chunk at position 9/31\n",
      "Processing chunk at position 10/31\n",
      "Processing chunk at position 11/31\n",
      "Processing chunk at position 12/31\n",
      "Processing chunk at position 13/31\n",
      "Processing chunk at position 14/31\n",
      "Processing chunk at position 15/31\n",
      "Processing chunk at position 16/31\n",
      "Processing chunk at position 17/31\n",
      "Processing chunk at position 18/31\n",
      "Processing chunk at position 19/31\n",
      "Processing chunk at position 20/31\n",
      "Processing chunk at position 21/31\n",
      "Processing chunk at position 22/31\n",
      "Processing chunk at position 23/31\n",
      "Processing chunk at position 24/31\n",
      "Processing chunk at position 25/31\n",
      "Processing chunk at position 26/31\n",
      "Processing chunk at position 27/31\n",
      "Processing chunk at position 28/31\n",
      "Processing chunk at position 29/31\n",
      "Processing chunk at position 30/31\n",
      "Created 31 chunks\n",
      "y\n",
      "  - Processing batch 1/4\n",
      "  - Processing batch 2/4\n",
      "  - Processing batch 3/4\n",
      "  - Processing batch 4/4\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 47 tokens\n",
      "Processing chunk at position 0/47\n",
      "Processing chunk at position 17/47\n",
      "Processing chunk at position 34/47\n",
      "Created 3 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 181 tokens\n",
      "Processing chunk at position 0/181\n",
      "Processing chunk at position 151/181\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "⏳ Processing document 81/111 (73.0%)\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 24 tokens\n",
      "Processing chunk at position 0/24\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 83 tokens\n",
      "Processing chunk at position 0/83\n",
      "Processing chunk at position 53/83\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 31 tokens\n",
      "Processing chunk at position 0/31\n",
      "Processing chunk at position 1/31\n",
      "Processing chunk at position 2/31\n",
      "Processing chunk at position 3/31\n",
      "Processing chunk at position 4/31\n",
      "Processing chunk at position 5/31\n",
      "Processing chunk at position 6/31\n",
      "Processing chunk at position 7/31\n",
      "Processing chunk at position 8/31\n",
      "Processing chunk at position 9/31\n",
      "Processing chunk at position 10/31\n",
      "Processing chunk at position 11/31\n",
      "Processing chunk at position 12/31\n",
      "Processing chunk at position 13/31\n",
      "Processing chunk at position 14/31\n",
      "Processing chunk at position 15/31\n",
      "Processing chunk at position 16/31\n",
      "Processing chunk at position 17/31\n",
      "Processing chunk at position 18/31\n",
      "Processing chunk at position 19/31\n",
      "Processing chunk at position 20/31\n",
      "Processing chunk at position 21/31\n",
      "Processing chunk at position 22/31\n",
      "Processing chunk at position 23/31\n",
      "Processing chunk at position 24/31\n",
      "Processing chunk at position 25/31\n",
      "Processing chunk at position 26/31\n",
      "Processing chunk at position 27/31\n",
      "Processing chunk at position 28/31\n",
      "Processing chunk at position 29/31\n",
      "Processing chunk at position 30/31\n",
      "Created 31 chunks\n",
      "y\n",
      "  - Processing batch 1/4\n",
      "  - Processing batch 2/4\n",
      "  - Processing batch 3/4\n",
      "  - Processing batch 4/4\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 46 tokens\n",
      "Processing chunk at position 0/46\n",
      "Processing chunk at position 16/46\n",
      "Processing chunk at position 32/46\n",
      "Created 3 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 134 tokens\n",
      "Processing chunk at position 0/134\n",
      "Processing chunk at position 104/134\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 141 tokens\n",
      "Processing chunk at position 0/141\n",
      "Processing chunk at position 111/141\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 97 tokens\n",
      "Processing chunk at position 0/97\n",
      "Processing chunk at position 67/97\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 95 tokens\n",
      "Processing chunk at position 0/95\n",
      "Processing chunk at position 65/95\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 98 tokens\n",
      "Processing chunk at position 0/98\n",
      "Processing chunk at position 68/98\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 125 tokens\n",
      "Processing chunk at position 0/125\n",
      "Processing chunk at position 95/125\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "⏳ Processing document 91/111 (82.0%)\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 151 tokens\n",
      "Processing chunk at position 0/151\n",
      "Processing chunk at position 121/151\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 69 tokens\n",
      "Processing chunk at position 0/69\n",
      "Processing chunk at position 39/69\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 65 tokens\n",
      "Processing chunk at position 0/65\n",
      "Processing chunk at position 35/65\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 65 tokens\n",
      "Processing chunk at position 0/65\n",
      "Processing chunk at position 35/65\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 208 tokens\n",
      "Processing chunk at position 0/208\n",
      "Processing chunk at position 178/208\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 8 tokens\n",
      "Processing chunk at position 0/8\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 57 tokens\n",
      "Processing chunk at position 0/57\n",
      "Processing chunk at position 27/57\n",
      "Processing chunk at position 54/57\n",
      "Created 3 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 104 tokens\n",
      "Processing chunk at position 0/104\n",
      "Processing chunk at position 74/104\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 70 tokens\n",
      "Processing chunk at position 0/70\n",
      "Processing chunk at position 40/70\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 144 tokens\n",
      "Processing chunk at position 0/144\n",
      "Processing chunk at position 114/144\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "⏳ Processing document 101/111 (91.0%)\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 184 tokens\n",
      "Processing chunk at position 0/184\n",
      "Processing chunk at position 154/184\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 121 tokens\n",
      "Processing chunk at position 0/121\n",
      "Processing chunk at position 91/121\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 74 tokens\n",
      "Processing chunk at position 0/74\n",
      "Processing chunk at position 44/74\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 90 tokens\n",
      "Processing chunk at position 0/90\n",
      "Processing chunk at position 60/90\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 43 tokens\n",
      "Processing chunk at position 0/43\n",
      "Processing chunk at position 13/43\n",
      "Processing chunk at position 26/43\n",
      "Processing chunk at position 39/43\n",
      "Created 4 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 48 tokens\n",
      "Processing chunk at position 0/48\n",
      "Processing chunk at position 18/48\n",
      "Processing chunk at position 36/48\n",
      "Created 3 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 86 tokens\n",
      "Processing chunk at position 0/86\n",
      "Processing chunk at position 56/86\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 68 tokens\n",
      "Processing chunk at position 0/68\n",
      "Processing chunk at position 38/68\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 41 tokens\n",
      "Processing chunk at position 0/41\n",
      "Processing chunk at position 11/41\n",
      "Processing chunk at position 22/41\n",
      "Processing chunk at position 33/41\n",
      "Created 4 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 27 tokens\n",
      "Processing chunk at position 0/27\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "⏳ Processing document 111/111 (100.0%)\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 96 tokens\n",
      "Processing chunk at position 0/96\n",
      "Processing chunk at position 66/96\n",
      "Created 2 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "\n",
      "✅ Embedding completed in 3.48s\n",
      "📊 Statistics:\n",
      "  - Documents processed: 111\n",
      "  - Total tokens: 14056\n",
      "  - Total chunks created: 324\n",
      "  - Average chunks per document: 2.92\n",
      "  - Processing speed: 4034.83 tokens/second\n",
      "  - Documents with errors: 0\n",
      "  - Success rate: 100.00%\n",
      "  - Embedding memory usage: 0.47 MB\n",
      "Processed /app/data/parsed/EPO/EPRTBJV2025000024001001/EPW1A9/EP24185369W1A9/EP24185369W1A9.json, total documents: 1860\n",
      "/app/data/parsed/EPO/EPRTBJV2025000024001001/EPW1B8/EP18784075W1B8/EP18784075W1B8.json\n",
      "📋 Starting document embedding process for 1 documents\n",
      "⏳ Processing document 1/1 (100.0%)\n",
      "\n",
      "📄 Document 1 Preview:\n",
      "  - Content: THERMAL MANAGEMENT OF PHOTON-COUNTING DETECTORS\n",
      "  - Metadata: {'doc_id': 'EP18784075B8W1', 'language': 'en', 'country': 'EP', 'doc_number': '3610300', 'application_number': '18784075.6', 'publication_date': '20250611', 'ipc_classes': ['G01T   1/24        20060101AFI20210114BHEP', 'A61B   6/03        20060101ALI20210114BHEP', 'G01N  23/04        20180101ALI20210114BHEP', 'G01N  23/083       20180101ALI20210114BHEP', 'G01T   1/36        20060101ALI20210114BHEP'], 'file': 'EP18784075W1B8.xml', 'section': 'title'}\n",
      "  - Token count: 7\n",
      "  - Calculated chunk size: 7\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 7 tokens\n",
      "Processing chunk at position 0/7\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Chunks created: 1 (in 0.00s)\n",
      "  - First chunk: thermal management of photon - counting detectors...\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "  - Embeddings generated: 1 vectors of shape (384,)\n",
      "  - Embedding time: 0.00s (0.0042s per chunk)\n",
      "\n",
      "✅ Embedding completed in 0.00s\n",
      "📊 Statistics:\n",
      "  - Documents processed: 1\n",
      "  - Total tokens: 7\n",
      "  - Total chunks created: 1\n",
      "  - Average chunks per document: 1.00\n",
      "  - Processing speed: 1604.82 tokens/second\n",
      "  - Documents with errors: 0\n",
      "  - Success rate: 100.00%\n",
      "  - Embedding memory usage: 0.00 MB\n",
      "Processed /app/data/parsed/EPO/EPRTBJV2025000024001001/EPW1B8/EP18784075W1B8/EP18784075W1B8.json, total documents: 1861\n",
      "/app/data/parsed/EPO/EPRTBJV2025000024001001/EPW1B8/EP22732225W1B8/EP22732225W1B8.json\n",
      "📋 Starting document embedding process for 1 documents\n",
      "⏳ Processing document 1/1 (100.0%)\n",
      "\n",
      "📄 Document 1 Preview:\n",
      "  - Content: BRACKET SYSTEM\n",
      "  - Metadata: {'doc_id': 'EP22732225B8W1', 'language': 'en', 'country': 'EP', 'doc_number': '4356033', 'application_number': '22732225.2', 'publication_date': '20250611', 'ipc_classes': ['F16L   3/13        20060101AFI20221223BHEP', 'F16B   2/22        20060101ALI20221223BHEP', 'H02G   3/32        20060101ALI20221223BHEP'], 'file': 'EP22732225W1B8.xml', 'section': 'title'}\n",
      "  - Token count: 2\n",
      "  - Calculated chunk size: 2\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 2 tokens\n",
      "Processing chunk at position 0/2\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Chunks created: 1 (in 0.00s)\n",
      "  - First chunk: bracket system...\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "  - Embeddings generated: 1 vectors of shape (384,)\n",
      "  - Embedding time: 0.00s (0.0036s per chunk)\n",
      "\n",
      "✅ Embedding completed in 0.00s\n",
      "📊 Statistics:\n",
      "  - Documents processed: 1\n",
      "  - Total tokens: 2\n",
      "  - Total chunks created: 1\n",
      "  - Average chunks per document: 1.00\n",
      "  - Processing speed: 541.17 tokens/second\n",
      "  - Documents with errors: 0\n",
      "  - Success rate: 100.00%\n",
      "  - Embedding memory usage: 0.00 MB\n",
      "Processed /app/data/parsed/EPO/EPRTBJV2025000024001001/EPW1B8/EP22732225W1B8/EP22732225W1B8.json, total documents: 1862\n",
      "/app/data/parsed/EPO/EPRTBJV2025000024001001/EPW1B8/EP21207842W1B8/EP21207842W1B8.json\n",
      "📋 Starting document embedding process for 1 documents\n",
      "⏳ Processing document 1/1 (100.0%)\n",
      "\n",
      "📄 Document 1 Preview:\n",
      "  - Content: REFRACTIVE INDEX SENSOR DEVICE\n",
      "  - Metadata: {'doc_id': 'EP21207842B8W1', 'language': 'en', 'country': 'EP', 'doc_number': '4180796', 'application_number': '21207842.2', 'publication_date': '20250611', 'ipc_classes': ['G01N  21/41        20060101AFI20220425BHEP', 'G01N  21/552       20140101ALI20220425BHEP'], 'file': 'EP21207842W1B8.xml', 'section': 'title'}\n",
      "  - Token count: 5\n",
      "  - Calculated chunk size: 5\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 5 tokens\n",
      "Processing chunk at position 0/5\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Chunks created: 1 (in 0.00s)\n",
      "  - First chunk: refractive index sensor device...\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "  - Embeddings generated: 1 vectors of shape (384,)\n",
      "  - Embedding time: 0.00s (0.0041s per chunk)\n",
      "\n",
      "✅ Embedding completed in 0.00s\n",
      "📊 Statistics:\n",
      "  - Documents processed: 1\n",
      "  - Total tokens: 5\n",
      "  - Total chunks created: 1\n",
      "  - Average chunks per document: 1.00\n",
      "  - Processing speed: 1165.93 tokens/second\n",
      "  - Documents with errors: 0\n",
      "  - Success rate: 100.00%\n",
      "  - Embedding memory usage: 0.00 MB\n",
      "Processed /app/data/parsed/EPO/EPRTBJV2025000024001001/EPW1B8/EP21207842W1B8/EP21207842W1B8.json, total documents: 1863\n",
      "/app/data/parsed/EPO/EPRTBJV2025000024001001/EPW1B8/EP21728115W1B8/EP21728115W1B8.json\n",
      "📋 Starting document embedding process for 1 documents\n",
      "⏳ Processing document 1/1 (100.0%)\n",
      "\n",
      "📄 Document 1 Preview:\n",
      "  - Content: SUPERFOOD FOR PROLONGING THE LONGEVITY OF AN INDIVIDUAL\n",
      "  - Metadata: {'doc_id': 'EP21728115B8W1', 'language': 'en', 'country': 'EP', 'doc_number': '4142762', 'application_number': '21728115.3', 'publication_date': '20250611', 'ipc_classes': ['A61K  36/185       20060101AFI20211105BHEP', 'A61K  36/31        20060101ALI20211105BHEP', 'A61K  36/52        20060101ALI20211105BHEP', 'A61K  36/54        20060101ALI20211105BHEP', 'A61K  36/73        20060101ALI20211105BHEP', 'A61K  36/889       20060101ALI20211105BHEP', 'A61P  39/00        20060101ALI20211105BHEP'], 'file': 'EP21728115W1B8.xml', 'section': 'title'}\n",
      "  - Token count: 12\n",
      "  - Calculated chunk size: 12\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 12 tokens\n",
      "Processing chunk at position 0/12\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Chunks created: 1 (in 0.00s)\n",
      "  - First chunk: superfood for prolonging the longevity of an indiv...\n",
      "  - Processing batch 1/1\n",
      "✓ Embedding completed\n",
      "x\n",
      "  - Embeddings generated: 1 vectors of shape (384,)\n",
      "  - Embedding time: 0.01s (0.0075s per chunk)\n",
      "\n",
      "✅ Embedding completed in 0.01s\n",
      "📊 Statistics:\n",
      "  - Documents processed: 1\n",
      "  - Total tokens: 12\n",
      "  - Total chunks created: 1\n",
      "  - Average chunks per document: 1.00\n",
      "  - Processing speed: 1565.48 tokens/second\n",
      "  - Documents with errors: 0\n",
      "  - Success rate: 100.00%\n",
      "  - Embedding memory usage: 0.00 MB\n",
      "Processed /app/data/parsed/EPO/EPRTBJV2025000024001001/EPW1B8/EP21728115W1B8/EP21728115W1B8.json, total documents: 1864\n",
      "Total embedded chunks: 1864\n",
      "\n",
      "Document 1:\n",
      "Text: audio signal encoder...\n",
      "Embedding shape: (384,)\n",
      "Metadata: {'doc_id': 'EP13899497B9W1', 'language': 'en', 'country': 'EP', 'doc_number': '3084761', 'application_number': '13899497.5', 'publication_date': '20250611', 'ipc_classes': ['G10L  19/038       20130101AFI20170426BHEP', 'G10L  19/07        20130101ALI20170426BHEP'], 'file': 'EP13899497W1B9.xml', 'section': 'title', 'chunk_index': 0, 'total_chunks': 1, 'source_doc_idx': 0}\n",
      "\n",
      "Document 2:\n",
      "Text: a processor - implemented method for encoding at least one audio signal, wherein the method comprise...\n",
      "Embedding shape: (384,)\n",
      "Metadata: {'doc_id': 'EP13899497B9W1', 'language': 'en', 'country': 'EP', 'doc_number': '3084761', 'application_number': '13899497.5', 'publication_date': '20250611', 'ipc_classes': ['G10L  19/038       20130101AFI20170426BHEP', 'G10L  19/07        20130101ALI20170426BHEP'], 'file': 'EP13899497W1B9.xml', 'section': 'claim', 'claim_number': '0001', 'chunk_index': 0, 'total_chunks': 3, 'source_doc_idx': 1}\n",
      "\n",
      "Document 3:\n",
      "Text: of non - zero parity and when the number of minus signs of the components of the single vector of pa...\n",
      "Embedding shape: (384,)\n",
      "Metadata: {'doc_id': 'EP13899497B9W1', 'language': 'en', 'country': 'EP', 'doc_number': '3084761', 'application_number': '13899497.5', 'publication_date': '20250611', 'ipc_classes': ['G10L  19/038       20130101AFI20170426BHEP', 'G10L  19/07        20130101ALI20170426BHEP'], 'file': 'EP13899497W1B9.xml', 'section': 'claim', 'claim_number': '0001', 'chunk_index': 1, 'total_chunks': 3, 'source_doc_idx': 1}\n"
     ]
    }
   ],
   "source": [
    "for file_path in file_list:\n",
    "    print(file_path)\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "            # Extract documents (returns list of Document objects)\n",
    "            docs = extract_documents(data)\n",
    "            \n",
    "            # Process each document with metadata preservation\n",
    "            chunked_docs = embed_documents_with_metadata(docs)\n",
    "            all_documents.extend(chunked_docs)\n",
    "            \n",
    "        print(f\"Processed {file_path}, total documents: {len(all_documents)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {str(e)}\")\n",
    "\n",
    "print(f\"Total embedded chunks: {len(all_documents)}\")\n",
    "\n",
    "# Example of examining the first few documents\n",
    "for i, doc in enumerate(all_documents[:3]):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(f\"Text: {doc['text'][:100]}...\")\n",
    "    print(f\"Embedding shape: {doc['embedding'].shape}\")\n",
    "    print(f\"Metadata: {doc['metadata']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0123f31c-148b-4311-a3cd-02db93dc45e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'scale factor squared ; determining the best leader class associated with the single potential code vector which generates the smallest associated distance ; and sorting components of the best leader class by the reverse ordering of the descending order of absolute values of the components of the single vector of parameters to generate an output lattice - quantized vector.',\n",
       " 'embedding': array([-1.39612034e-02,  3.35320830e-02, -6.83307201e-02, -3.19998823e-02,\n",
       "        -2.53480002e-02,  5.13324663e-02, -4.18933146e-02, -6.43100543e-03,\n",
       "        -2.48833969e-02,  4.77418443e-03,  4.21190932e-02,  3.78175043e-02,\n",
       "         2.91607752e-02,  3.00353784e-02, -8.81722793e-02, -1.84398983e-02,\n",
       "         1.23946741e-02,  1.27153769e-01, -8.33532736e-02, -6.15738258e-02,\n",
       "         4.38563526e-02, -1.01426795e-01, -2.92504746e-02, -4.27244231e-03,\n",
       "         7.42027014e-02, -2.93293186e-02,  3.31224091e-02,  3.88909392e-02,\n",
       "         9.69608501e-02, -7.51155764e-02,  2.25826353e-02,  6.06075153e-02,\n",
       "         1.02092765e-01,  2.85138227e-02, -8.71291608e-02,  6.74270093e-02,\n",
       "         5.63727058e-02, -1.80968586e-02, -3.78573611e-02,  6.27591610e-02,\n",
       "         2.21544523e-02,  2.39680018e-02,  4.25035022e-02,  1.58014032e-03,\n",
       "         1.35937380e-02,  6.66468684e-03,  1.94836296e-02, -1.42314658e-02,\n",
       "        -1.29201367e-01, -1.11130811e-01, -3.54685821e-02,  5.24230674e-02,\n",
       "        -8.33429024e-02,  2.53428482e-02, -1.13995932e-02, -5.01368605e-02,\n",
       "         5.06122559e-02, -8.73007178e-02,  3.02836555e-03, -3.44869643e-02,\n",
       "        -5.46303056e-02, -2.14121677e-02, -1.61104649e-02, -1.02826403e-02,\n",
       "         5.97615167e-02,  3.99421435e-03, -2.57106982e-02,  2.67868694e-02,\n",
       "         6.80082943e-03,  6.14483990e-02,  5.34122717e-03, -7.06563368e-02,\n",
       "        -9.70201418e-02,  7.25198016e-02,  7.28850216e-02,  1.42656760e-02,\n",
       "        -9.32700653e-03, -2.97984350e-02, -2.09019314e-02,  4.78138328e-02,\n",
       "        -4.60818596e-02,  1.75988544e-02, -4.70360518e-02,  7.60517418e-02,\n",
       "         6.04512207e-02,  1.41927358e-02,  7.56555935e-03, -9.69030149e-03,\n",
       "         3.91734727e-02,  1.01773692e-02,  2.74762269e-02,  5.78551777e-02,\n",
       "         1.54247526e-02,  6.10805862e-02, -6.32731691e-02, -1.18622333e-02,\n",
       "         9.11729857e-02, -6.17431626e-02,  4.83725369e-02,  2.71058474e-02,\n",
       "         8.06156248e-02, -1.01083554e-02,  6.17974947e-05, -5.25636040e-02,\n",
       "         3.47827449e-02, -3.86400707e-02,  5.90925924e-02,  3.77572142e-02,\n",
       "         4.42871451e-02, -8.15719366e-02, -2.27219146e-02, -3.80250625e-02,\n",
       "        -6.04974367e-02, -8.45254958e-03,  4.59593683e-02, -5.81004024e-02,\n",
       "         5.22814766e-02,  1.14711739e-01,  3.18518840e-02, -8.82441700e-02,\n",
       "         4.18241024e-02, -2.16624839e-03,  1.12976246e-02, -2.65474357e-02,\n",
       "         1.61742978e-03,  8.12469944e-02, -1.38851374e-01,  2.05031653e-33,\n",
       "         2.40773428e-02,  6.49888888e-02,  4.30847853e-02,  4.93389107e-02,\n",
       "         1.09078607e-03, -2.77935751e-02,  1.77413858e-02, -9.41219181e-02,\n",
       "         2.77961399e-02,  6.85188919e-02, -5.08756004e-03,  8.02332386e-02,\n",
       "         3.08016632e-02,  9.13120732e-02,  8.90740305e-02, -7.31026530e-02,\n",
       "        -1.81605835e-02,  4.79238154e-03, -1.12356525e-02, -1.62330847e-02,\n",
       "         9.70842838e-02, -1.18486080e-02,  2.27461215e-02,  3.76512925e-03,\n",
       "         5.20238243e-02, -2.45808922e-02,  2.67005377e-02, -5.01213260e-02,\n",
       "        -5.14098480e-02,  2.59150807e-02, -5.04570492e-02, -1.92704219e-02,\n",
       "        -7.49024153e-02, -8.71056505e-03,  3.41985337e-02, -4.28635487e-03,\n",
       "         1.42859286e-02, -3.52211110e-02,  2.16722954e-02, -1.11081980e-01,\n",
       "        -3.24548073e-02, -1.32360719e-02,  2.77127549e-02, -6.39595836e-02,\n",
       "         4.28526476e-02, -3.26644536e-03,  4.89089042e-02,  6.89657256e-02,\n",
       "         2.65731383e-02, -3.65370214e-02, -2.45490111e-02, -1.67122707e-02,\n",
       "         8.16935673e-02,  2.06241775e-02,  6.59807846e-02,  2.30569188e-02,\n",
       "         5.02689853e-02, -1.33751053e-02,  1.45299844e-02,  9.58683640e-02,\n",
       "        -5.38117066e-02,  1.14491871e-02, -4.71547358e-02, -6.18601101e-04,\n",
       "        -8.06196593e-03, -6.94236010e-02, -5.59771806e-02, -1.19401224e-01,\n",
       "         6.10344857e-02,  7.23152012e-02, -2.09352877e-02,  6.45591393e-02,\n",
       "        -3.32957543e-02, -6.13712333e-02,  3.75458151e-02, -6.30398989e-02,\n",
       "         1.70052908e-02, -5.97513877e-02, -1.06660938e-02, -7.11600184e-02,\n",
       "        -1.45923391e-01,  5.46851084e-02, -2.13100095e-04, -7.63686225e-02,\n",
       "        -2.99444627e-02, -9.97252837e-02, -6.28382945e-03, -5.72722480e-02,\n",
       "        -7.18539432e-02,  1.50845863e-03, -9.63814706e-02, -1.61051136e-02,\n",
       "         7.48058781e-02,  4.07036021e-02, -8.61523002e-02, -1.94687983e-33,\n",
       "        -5.82350157e-02,  5.76221570e-02,  7.77874980e-03,  7.92552903e-02,\n",
       "         4.81682010e-02, -3.13067995e-02, -7.32293911e-03, -1.73240658e-02,\n",
       "         1.03000309e-02, -6.01868029e-04, -3.59150544e-02,  3.80667001e-02,\n",
       "         4.48685512e-02,  1.28184138e-02,  9.78622958e-02,  4.99756075e-02,\n",
       "         3.17831757e-03, -5.92495464e-02, -2.56574936e-02, -3.61756124e-02,\n",
       "         2.24333145e-02,  2.95596253e-02,  3.34127210e-02, -1.54186739e-03,\n",
       "         3.37197110e-02, -4.25734604e-03,  4.09501642e-02,  9.82335210e-03,\n",
       "        -2.17625145e-02, -3.41909342e-02,  3.01613100e-02, -7.22660944e-02,\n",
       "        -7.75557710e-03, -3.34894955e-02, -8.35394114e-02, -3.94324251e-02,\n",
       "         4.60693836e-02, -3.94097343e-02, -4.08543348e-02,  1.03996359e-01,\n",
       "         9.10081726e-04,  2.74805743e-02, -1.28465947e-02,  4.48793499e-03,\n",
       "         3.49246189e-02,  3.82347405e-02,  4.76424471e-02,  5.30857174e-03,\n",
       "        -2.42089275e-02,  1.32271098e-02, -7.98636489e-03,  3.15871201e-02,\n",
       "        -2.25423719e-03,  8.38739127e-02, -4.41163033e-02,  1.30616585e-02,\n",
       "        -2.98083909e-02,  3.68583016e-02,  4.56795543e-02,  2.47424114e-02,\n",
       "         2.70499196e-02, -1.01282850e-01,  6.71634600e-02,  2.03454513e-02,\n",
       "        -6.63068891e-02, -4.17991430e-02, -5.06245159e-03,  7.26002455e-02,\n",
       "        -5.74876890e-02,  6.19961321e-02, -1.42260166e-02, -1.56602811e-03,\n",
       "         2.25348994e-02,  3.89647833e-03, -7.52643570e-02, -7.07502067e-02,\n",
       "        -5.24019375e-02,  5.24573177e-02,  1.05331903e-02, -2.26983316e-02,\n",
       "         4.02482226e-02,  6.33437112e-02,  3.28887664e-02, -1.08580645e-02,\n",
       "         1.57165632e-03,  1.84997469e-02,  1.14007331e-01,  7.90954828e-02,\n",
       "         8.37499648e-02, -9.11620632e-03,  1.50055271e-02, -3.84211796e-03,\n",
       "         6.51217438e-03,  8.04298818e-02,  1.05508659e-02, -3.44900215e-08,\n",
       "        -1.08579539e-01, -2.47791912e-02,  3.19479092e-04, -3.03316526e-02,\n",
       "         3.33894901e-02,  2.37914249e-02, -4.96355351e-03,  2.98645087e-02,\n",
       "         1.22719752e-02,  4.28487398e-02,  5.83380908e-02, -2.61647962e-02,\n",
       "        -1.09949410e-01, -1.10649900e-03,  7.71787465e-02,  4.97401170e-02,\n",
       "        -2.43901536e-02,  2.61664130e-02, -4.13848050e-02, -6.32678643e-02,\n",
       "         1.25261182e-02,  5.02163023e-02, -6.72629476e-02,  8.37859288e-02,\n",
       "        -8.68752152e-02, -1.77456513e-02, -4.29493971e-02, -1.81790981e-02,\n",
       "         3.63538340e-02,  6.97648823e-02, -5.53170871e-03,  3.42211425e-02,\n",
       "         4.45027947e-02, -1.26904063e-02, -4.56938110e-02,  5.48013672e-02,\n",
       "        -1.53830469e-01, -2.74559800e-02,  1.98270450e-03, -2.13839822e-02,\n",
       "        -4.12745774e-02, -1.32250460e-02, -2.44087391e-02,  7.88087491e-03,\n",
       "         9.03771967e-02,  8.09403509e-02, -1.62787903e-02, -1.60135645e-02,\n",
       "        -3.26586445e-03,  3.86477299e-02, -4.88378741e-02,  8.56741238e-03,\n",
       "        -9.08039585e-02, -5.21165244e-02,  1.51137896e-02,  3.38995568e-02,\n",
       "        -7.65595213e-02, -5.13432994e-02,  1.06191356e-02, -1.07402697e-01,\n",
       "         3.95129547e-02,  3.68462615e-02, -2.73130890e-02,  4.58905064e-02],\n",
       "       dtype=float32),\n",
       " 'metadata': {'doc_id': 'EP13899497B9W1',\n",
       "  'language': 'en',\n",
       "  'country': 'EP',\n",
       "  'doc_number': '3084761',\n",
       "  'application_number': '13899497.5',\n",
       "  'publication_date': '20250611',\n",
       "  'ipc_classes': ['G10L  19/038       20130101AFI20170426BHEP',\n",
       "   'G10L  19/07        20130101ALI20170426BHEP'],\n",
       "  'file': 'EP13899497W1B9.xml',\n",
       "  'section': 'claim',\n",
       "  'claim_number': '0001',\n",
       "  'chunk_index': 2,\n",
       "  'total_chunks': 3,\n",
       "  'source_doc_idx': 1}}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_documents[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec434401-30c1-402e-a5c0-1fc7e3845784",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
