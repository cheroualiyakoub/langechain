{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "778fdd7b-d66b-4e02-a036-f48cd3baf506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Testing JSON File Loading Functions\n",
      "==================================================\n",
      "üìÅ Current directory: /app/notebooks\n",
      "üìÅ Project root: /app\n",
      "üìÅ Source directory: /app/src\n",
      "‚úÖ Python paths configured\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "import json\n",
    "import math\n",
    "\n",
    "# Add source directories to Python path\n",
    "current_dir = Path.cwd()\n",
    "project_root = current_dir.parent  # Go up one level from notebooks to project root\n",
    "src_dir = project_root / \"src\"\n",
    "\n",
    "# Add paths\n",
    "sys.path.append(str(src_dir / \"data_pipline\"))\n",
    "sys.path.append(str(src_dir / \"EU_XML_data_loader\"))\n",
    "\n",
    "\n",
    "from data_pipline import DataPipeline\n",
    "import get_raw_data_paths_EPO  \n",
    "from xml_loader_EPO import process_xml_files_list\n",
    "\n",
    "\n",
    "# Test the new JSON loading functionality\n",
    "sys.path.append(str(src_dir / \"data_pipline\" / \"json_loader\"))\n",
    "\n",
    "# Import the JSON loader functions\n",
    "from json_loader_epo import get_epo_json_file_paths, get_all_json_file_paths, load_json_documents\n",
    "\n",
    "print(\"üîç Testing JSON File Loading Functions\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"üìÅ Current directory: {current_dir}\")\n",
    "print(f\"üìÅ Project root: {project_root}\")\n",
    "print(f\"üìÅ Source directory: {src_dir}\")\n",
    "print(f\"‚úÖ Python paths configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d48035a2-2792-452e-b23c-d08ca965396b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import os\n",
    "from langchain.docstore.document import Document\n",
    "import sys\n",
    "# Prepare .json file for embedding\n",
    "def extract_documents(json_data):\n",
    "    bibliographic = json_data.get(\"bibliographic_data\", {})\n",
    "    doc_id = bibliographic.get(\"doc_id\", \"UNKNOWN\")\n",
    "    documents = []\n",
    "\n",
    "    # Common metadata to propagate\n",
    "    common_meta = {\n",
    "        \"doc_id\": doc_id,\n",
    "        \"language\": bibliographic.get(\"language\"),\n",
    "        \"country\": bibliographic.get(\"country\"),\n",
    "        \"doc_number\": bibliographic.get(\"doc_number\"),\n",
    "        \"application_number\": bibliographic.get(\"application_number\"),\n",
    "        \"publication_date\": bibliographic.get(\"publication_date\"),\n",
    "        \"ipc_classes\": bibliographic.get(\"ipc_classes\", []),\n",
    "        \"file\":bibliographic.get(\"file\")\n",
    "    }\n",
    "\n",
    "    # Title (en preferred)\n",
    "    title_dict = bibliographic.get(\"title\", {})\n",
    "    title = title_dict.get(\"en\") or next(iter(title_dict.values()), \"\")\n",
    "    if title:\n",
    "        documents.append(Document(\n",
    "            page_content=title,\n",
    "            metadata={**common_meta, \"section\": \"title\"}\n",
    "        ))\n",
    "\n",
    "    # Abstract\n",
    "    abstract = bibliographic.get(\"abstract\")\n",
    "    if abstract:\n",
    "        documents.append(Document(\n",
    "            page_content=abstract,\n",
    "            metadata={**common_meta, \"section\": \"abstract\"}\n",
    "        ))\n",
    "\n",
    "    # Claims\n",
    "    for claim in json_data.get(\"claims\", []):\n",
    "        documents.append(Document(\n",
    "            page_content=claim[\"text\"],\n",
    "            metadata={**common_meta, \"section\": \"claim\", \"claim_number\": claim.get(\"claim_number\")}\n",
    "        ))\n",
    "\n",
    "    # Main sections\n",
    "    for section in json_data.get(\"main_sections\", []):\n",
    "        section_name = section.get(\"heading_text\", \"UNKNOWN_SECTION\")\n",
    "        for p in section.get(\"paragraphs\", []):\n",
    "            documents.append(Document(\n",
    "                page_content=f\"{section_name}\\n{p['text']}\",\n",
    "                metadata={**common_meta, \"section\": section_name, \"p_id\": p.get(\"p_id\")}\n",
    "            ))\n",
    "\n",
    "    return documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "20792a95-d12e-4006-9779-f59452c101d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 12:44:31,439 - INFO - Use pytorch device_name: cpu\n",
      "2025-06-18 12:44:31,440 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Load tokenizer and embedding model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def count_tokens(text):\n",
    "    return len(tokenizer.encode(text, add_special_tokens=False))\n",
    "\n",
    "def get_chunk_size(text: str, total_tokens: int, base_chunk_size: int = 350, min_chunk_size: int = 180) -> int:\n",
    "    if total_tokens <= base_chunk_size:\n",
    "        return total_tokens\n",
    "\n",
    "    num_splits = total_tokens // base_chunk_size\n",
    "    if total_tokens % base_chunk_size != 0:\n",
    "        num_splits += 1\n",
    "    \n",
    "    balanced_chunk_size = math.ceil(total_tokens / num_splits)\n",
    "\n",
    "    balanced_chunk_size = int(max(min_chunk_size, balanced_chunk_size))\n",
    "    return balanced_chunk_size\n",
    "\n",
    "def chunk_text_by_tokens(text: str, chunk_size: int, overlap : int = 30):\n",
    "    print(\"Starting tokenization...\")\n",
    "    input_ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "    print(f\"Tokenized text length: {len(input_ids)} tokens\")\n",
    "    \n",
    "    # Safety check to prevent infinite loops or extremely slow processing\n",
    "    # if chunk_size <= 350:\n",
    "    #     overlap = 0\n",
    "    \n",
    "    chunks = []\n",
    "    start = 0\n",
    "    \n",
    "    # Process in batches for large texts\n",
    "    while start < len(input_ids):\n",
    "        print(f\"Processing chunk at position {start}/{len(input_ids)}\")\n",
    "        end = min(start + chunk_size, len(input_ids))\n",
    "        chunk_ids = input_ids[start:end]\n",
    "        \n",
    "        try:\n",
    "            chunk_text = tokenizer.decode(chunk_ids)\n",
    "            chunks.append(chunk_text)\n",
    "        except Exception as e:\n",
    "            print(f\"Error decoding chunk: {e}\")\n",
    "            # Skip this chunk or use a fallback approach\n",
    "        print(\"OVERLAP = \", overlap)\n",
    "        # Advance positionŒ©\n",
    "        start += chunk_size - overlap\n",
    "        \n",
    "        # Safety check to prevent memory issues with too many chunks\n",
    "        if len(chunks) > 1000:  # Arbitrary limit\n",
    "            print(\"‚ö†Ô∏è Warning: Reached maximum number of chunks\")\n",
    "            break\n",
    "    \n",
    "    print(f\"Created {len(chunks)} chunks\")\n",
    "    return chunks\n",
    "\n",
    "def embed_document(text: str):\n",
    "    total_tokens = count_tokens(text)\n",
    "    chunk_size = get_chunk_size(text, total_tokens)\n",
    "\n",
    "    chunks = chunk_text_by_tokens(text, chunk_size)\n",
    "    embeddings = embedding_model.encode(chunks)\n",
    "\n",
    "    return list(zip(chunks, embeddings))  # list of (chunk, embedding_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b0d8d3b3-0d54-4cc4-a302-871c8e852689",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import List, Dict, Any\n",
    "import logging\n",
    "import numpy as np\n",
    "from langchain.schema import Document\n",
    "\n",
    "def embed_documents_with_metadata(documents, debug=True):\n",
    "    \"\"\"Process a list of Document objects, preserving metadata\"\"\"\n",
    "    if debug:\n",
    "        print(f\"üìã Starting document embedding process for {len(documents)} documents\")\n",
    "        start_time = time.time()\n",
    "    \n",
    "    processed_docs = []\n",
    "    \n",
    "    # Track statistics for debugging\n",
    "    total_tokens = 0\n",
    "    total_chunks = 0\n",
    "    errors = 0\n",
    "    \n",
    "    for doc_idx, doc in enumerate(documents):\n",
    "        if debug and doc_idx % 10 == 0:  # Progress update every 10 docs\n",
    "            print(f\"‚è≥ Processing document {doc_idx+1}/{len(documents)} ({(doc_idx+1)/len(documents)*100:.1f}%)\")\n",
    "        \n",
    "        try:\n",
    "            # Get text content and metadata\n",
    "            text = doc.page_content\n",
    "            metadata = doc.metadata\n",
    "            \n",
    "            if debug and doc_idx < 3:  # Show sample of first few documents\n",
    "                print(f\"\\nüìÑ Document {doc_idx+1} Preview:\")\n",
    "                print(f\"  - Content: {text[:100]}...\" if len(text) > 100 else f\"  - Content: {text}\")\n",
    "                print(f\"  - Metadata: {metadata}\")\n",
    "\n",
    "            \n",
    "            # Calculate chunk size\n",
    "            doc_tokens = count_tokens(text)\n",
    "            total_tokens += doc_tokens\n",
    "            chunk_size = get_chunk_size(text, doc_tokens)\n",
    "            \n",
    "            if debug and doc_idx < 3:\n",
    "                print(f\"  - Token count: {doc_tokens}\")\n",
    "                print(f\"  - Calculated chunk size: {chunk_size}\")\n",
    "            \n",
    "            # Create chunks\n",
    "            chunk_start_time = time.time()\n",
    "            print(\"x\")\n",
    "            if (doc_tokens < 350) :\n",
    "                overlap = 0\n",
    "            else :\n",
    "                overlap = 32\n",
    "            chunks = chunk_text_by_tokens(text, chunk_size, overlap)\n",
    "            print(\"y\")\n",
    "            total_chunks += len(chunks)\n",
    "            \n",
    "            if debug and doc_idx < 3:\n",
    "                chunk_time = time.time() - chunk_start_time\n",
    "                print(f\"  - Chunks created: {len(chunks)} (in {chunk_time:.2f}s)\")\n",
    "                print(f\"  - First chunk: {chunks[0][:50]}...\")\n",
    "            \n",
    "            # Generate embeddings for all chunks\n",
    "            embedding_start_time = time.time()\n",
    "            try:\n",
    "                \n",
    "                batch_size = 8 \n",
    "                all_embeddings = []\n",
    "                \n",
    "                for batch_idx in range(0, len(chunks), batch_size):\n",
    "                    batch = chunks[batch_idx:batch_idx+batch_size]\n",
    "                    print(f\"  - Processing batch {batch_idx//batch_size + 1}/{(len(chunks)+batch_size-1)//batch_size}\")\n",
    "                    batch_embeddings = embedding_model.encode(batch, show_progress_bar=False)\n",
    "                    all_embeddings.extend(batch_embeddings)\n",
    "                \n",
    "                embeddings = all_embeddings\n",
    "                print(\"‚úì Embedding completed\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error during embedding: {str(e)}\")\n",
    "                raise\n",
    "            print(\"x\")\n",
    "            if debug and doc_idx < 3:\n",
    "                embedding_time = time.time() - embedding_start_time\n",
    "                print(f\"  - Embeddings generated: {len(embeddings)} vectors of shape {embeddings[0].shape}\")\n",
    "                print(f\"  - Embedding time: {embedding_time:.2f}s ({embedding_time/len(chunks):.4f}s per chunk)\")\n",
    "            \n",
    "            # Create new documents with chunks and metadata\n",
    "            for i, (chunk_text, embedding) in enumerate(zip(chunks, embeddings)):\n",
    "                # Create a copy of metadata and add chunk information\n",
    "                chunk_metadata = metadata.copy()\n",
    "                chunk_metadata[\"chunk_index\"] = i\n",
    "                chunk_metadata[\"total_chunks\"] = len(chunks)\n",
    "                chunk_metadata[\"source_doc_idx\"] = doc_idx\n",
    "                \n",
    "                # Check for NaN values in embedding\n",
    "                if np.isnan(embedding).any():\n",
    "                    if debug:\n",
    "                        print(f\"‚ö†Ô∏è Warning: NaN values detected in embedding for document {doc_idx}, chunk {i}\")\n",
    "                    # Replace NaN with zeros\n",
    "                    embedding = np.nan_to_num(embedding)\n",
    "                \n",
    "                # Create a new Document object with chunk text and original metadata\n",
    "                processed_docs.append({\n",
    "                    \"text\": chunk_text,\n",
    "                    \"embedding\": embedding,\n",
    "                    \"metadata\": chunk_metadata\n",
    "                })\n",
    "                \n",
    "        except Exception as e:\n",
    "            errors += 1\n",
    "            if debug:\n",
    "                print(f\"‚ùå Error processing document {doc_idx}: {str(e)}\")\n",
    "    \n",
    "    if debug:\n",
    "        end_time = time.time()\n",
    "        total_time = end_time - start_time\n",
    "        print(f\"\\n‚úÖ Embedding completed in {total_time:.2f}s\")\n",
    "        print(f\"üìä Statistics:\")\n",
    "        print(f\"  - Documents processed: {len(documents)}\")\n",
    "        print(f\"  - Total tokens: {total_tokens}\")\n",
    "        print(f\"  - Total chunks created: {total_chunks}\")\n",
    "        print(f\"  - Average chunks per document: {total_chunks/len(documents):.2f}\")\n",
    "        print(f\"  - Processing speed: {total_tokens/total_time:.2f} tokens/second\")\n",
    "        print(f\"  - Documents with errors: {errors}\")\n",
    "        print(f\"  - Success rate: {(len(documents)-errors)/len(documents)*100:.2f}%\")\n",
    "        \n",
    "        # Memory usage of embeddings\n",
    "        embedding_size = sum(emb[\"embedding\"].nbytes for emb in processed_docs)\n",
    "        print(f\"  - Embedding memory usage: {embedding_size/1024/1024:.2f} MB\")\n",
    "    \n",
    "    return processed_docs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "415359ef-272a-4a20-a0ac-a4de57db11be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Found 1286 EPO JSON files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Process all files\n",
    "file_list = get_epo_json_file_paths()[:10]\n",
    "all_documents = []\n",
    "\n",
    "len(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7ea11ef0-2a6c-4eba-a93c-3cb23a64dcce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/app/data/parsed/EPO/EPRTBJV2025000024001001/EPW1B9/EP13899497W1B9/EP13899497W1B9.json\n",
      "üìã Starting document embedding process for 180 documents\n",
      "‚è≥ Processing document 1/180 (0.6%)\n",
      "\n",
      "üìÑ Document 1 Preview:\n",
      "  - Content: AUDIO SIGNAL ENCODER\n",
      "  - Metadata: {'doc_id': 'EP13899497B9W1', 'language': 'en', 'country': 'EP', 'doc_number': '3084761', 'application_number': '13899497.5', 'publication_date': '20250611', 'ipc_classes': ['G10L  19/038       20130101AFI20170426BHEP', 'G10L  19/07        20130101ALI20170426BHEP'], 'file': 'EP13899497W1B9.xml', 'section': 'title'}\n",
      "  - Token count: 5\n",
      "  - Calculated chunk size: 5\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 5 tokens\n",
      "Processing chunk at position 0/5\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Chunks created: 1 (in 0.00s)\n",
      "  - First chunk: audio signal encoder...\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "  - Embeddings generated: 1 vectors of shape (384,)\n",
      "  - Embedding time: 0.03s (0.0348s per chunk)\n",
      "\n",
      "üìÑ Document 2 Preview:\n",
      "  - Content: A processor-implemented method for encoding at least one audio signal, wherein the method comprises:...\n",
      "  - Metadata: {'doc_id': 'EP13899497B9W1', 'language': 'en', 'country': 'EP', 'doc_number': '3084761', 'application_number': '13899497.5', 'publication_date': '20250611', 'ipc_classes': ['G10L  19/038       20130101AFI20170426BHEP', 'G10L  19/07        20130101ALI20170426BHEP'], 'file': 'EP13899497W1B9.xml', 'section': 'claim', 'claim_number': '0001'}\n",
      "  - Token count: 691\n",
      "  - Calculated chunk size: 346\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 691 tokens\n",
      "Processing chunk at position 0/691\n",
      "OVERLAP =  32\n",
      "Processing chunk at position 314/691\n",
      "OVERLAP =  32\n",
      "Processing chunk at position 628/691\n",
      "OVERLAP =  32\n",
      "Created 3 chunks\n",
      "y\n",
      "  - Chunks created: 3 (in 0.00s)\n",
      "  - First chunk: a processor - implemented method for encoding at l...\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "  - Embeddings generated: 3 vectors of shape (384,)\n",
      "  - Embedding time: 0.07s (0.0233s per chunk)\n",
      "\n",
      "üìÑ Document 3 Preview:\n",
      "  - Content: The method as claimed in claim 1, further comprising: selecting the scale factor from a plurality of...\n",
      "  - Metadata: {'doc_id': 'EP13899497B9W1', 'language': 'en', 'country': 'EP', 'doc_number': '3084761', 'application_number': '13899497.5', 'publication_date': '20250611', 'ipc_classes': ['G10L  19/038       20130101AFI20170426BHEP', 'G10L  19/07        20130101ALI20170426BHEP'], 'file': 'EP13899497W1B9.xml', 'section': 'claim', 'claim_number': '0002'}\n",
      "  - Token count: 38\n",
      "  - Calculated chunk size: 38\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 38 tokens\n",
      "Processing chunk at position 0/38\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Chunks created: 1 (in 0.00s)\n",
      "  - First chunk: the method as claimed in claim 1, further comprisi...\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "  - Embeddings generated: 1 vectors of shape (384,)\n",
      "  - Embedding time: 0.02s (0.0152s per chunk)\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 59 tokens\n",
      "Processing chunk at position 0/59\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 82 tokens\n",
      "Processing chunk at position 0/82\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 791 tokens\n",
      "Processing chunk at position 0/791\n",
      "OVERLAP =  32\n",
      "Processing chunk at position 232/791\n",
      "OVERLAP =  32\n",
      "Processing chunk at position 464/791\n",
      "OVERLAP =  32\n",
      "Processing chunk at position 696/791\n",
      "OVERLAP =  32\n",
      "Created 4 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 129 tokens\n",
      "Processing chunk at position 0/129\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 66 tokens\n",
      "Processing chunk at position 0/66\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 82 tokens\n",
      "Processing chunk at position 0/82\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 33 tokens\n",
      "Processing chunk at position 0/33\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "‚è≥ Processing document 11/180 (6.1%)\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 33 tokens\n",
      "Processing chunk at position 0/33\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 116 tokens\n",
      "Processing chunk at position 0/116\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 37 tokens\n",
      "Processing chunk at position 0/37\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 88 tokens\n",
      "Processing chunk at position 0/88\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 30 tokens\n",
      "Processing chunk at position 0/30\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 30 tokens\n",
      "Processing chunk at position 0/30\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 74 tokens\n",
      "Processing chunk at position 0/74\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 25 tokens\n",
      "Processing chunk at position 0/25\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 40 tokens\n",
      "Processing chunk at position 0/40\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 53 tokens\n",
      "Processing chunk at position 0/53\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "‚è≥ Processing document 21/180 (11.7%)\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 47 tokens\n",
      "Processing chunk at position 0/47\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 18 tokens\n",
      "Processing chunk at position 0/18\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 96 tokens\n",
      "Processing chunk at position 0/96\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 109 tokens\n",
      "Processing chunk at position 0/109\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 106 tokens\n",
      "Processing chunk at position 0/106\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 35 tokens\n",
      "Processing chunk at position 0/35\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 112 tokens\n",
      "Processing chunk at position 0/112\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 102 tokens\n",
      "Processing chunk at position 0/102\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 32 tokens\n",
      "Processing chunk at position 0/32\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 231 tokens\n",
      "Processing chunk at position 0/231\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "‚è≥ Processing document 31/180 (17.2%)\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 81 tokens\n",
      "Processing chunk at position 0/81\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 38 tokens\n",
      "Processing chunk at position 0/38\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 77 tokens\n",
      "Processing chunk at position 0/77\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 162 tokens\n",
      "Processing chunk at position 0/162\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 64 tokens\n",
      "Processing chunk at position 0/64\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 22 tokens\n",
      "Processing chunk at position 0/22\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 27 tokens\n",
      "Processing chunk at position 0/27\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 76 tokens\n",
      "Processing chunk at position 0/76\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 121 tokens\n",
      "Processing chunk at position 0/121\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 50 tokens\n",
      "Processing chunk at position 0/50\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "‚è≥ Processing document 41/180 (22.8%)\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 155 tokens\n",
      "Processing chunk at position 0/155\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 19 tokens\n",
      "Processing chunk at position 0/19\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 29 tokens\n",
      "Processing chunk at position 0/29\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 84 tokens\n",
      "Processing chunk at position 0/84\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 84 tokens\n",
      "Processing chunk at position 0/84\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 91 tokens\n",
      "Processing chunk at position 0/91\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 83 tokens\n",
      "Processing chunk at position 0/83\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 21 tokens\n",
      "Processing chunk at position 0/21\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 35 tokens\n",
      "Processing chunk at position 0/35\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 144 tokens\n",
      "Processing chunk at position 0/144\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "‚è≥ Processing document 51/180 (28.3%)\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 52 tokens\n",
      "Processing chunk at position 0/52\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 24 tokens\n",
      "Processing chunk at position 0/24\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 50 tokens\n",
      "Processing chunk at position 0/50\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 22 tokens\n",
      "Processing chunk at position 0/22\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 32 tokens\n",
      "Processing chunk at position 0/32\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 80 tokens\n",
      "Processing chunk at position 0/80\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 22 tokens\n",
      "Processing chunk at position 0/22\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 17 tokens\n",
      "Processing chunk at position 0/17\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 21 tokens\n",
      "Processing chunk at position 0/21\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 13 tokens\n",
      "Processing chunk at position 0/13\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "‚è≥ Processing document 61/180 (33.9%)\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 88 tokens\n",
      "Processing chunk at position 0/88\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 170 tokens\n",
      "Processing chunk at position 0/170\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 30 tokens\n",
      "Processing chunk at position 0/30\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 123 tokens\n",
      "Processing chunk at position 0/123\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 71 tokens\n",
      "Processing chunk at position 0/71\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 95 tokens\n",
      "Processing chunk at position 0/95\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 80 tokens\n",
      "Processing chunk at position 0/80\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 380 tokens\n",
      "Processing chunk at position 0/380\n",
      "OVERLAP =  32\n",
      "Processing chunk at position 158/380\n",
      "OVERLAP =  32\n",
      "Processing chunk at position 316/380\n",
      "OVERLAP =  32\n",
      "Created 3 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 100 tokens\n",
      "Processing chunk at position 0/100\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 111 tokens\n",
      "Processing chunk at position 0/111\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "‚è≥ Processing document 71/180 (39.4%)\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 119 tokens\n",
      "Processing chunk at position 0/119\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 64 tokens\n",
      "Processing chunk at position 0/64\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 142 tokens\n",
      "Processing chunk at position 0/142\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 14 tokens\n",
      "Processing chunk at position 0/14\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 85 tokens\n",
      "Processing chunk at position 0/85\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 23 tokens\n",
      "Processing chunk at position 0/23\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 18 tokens\n",
      "Processing chunk at position 0/18\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 19 tokens\n",
      "Processing chunk at position 0/19\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 21 tokens\n",
      "Processing chunk at position 0/21\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 16 tokens\n",
      "Processing chunk at position 0/16\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "‚è≥ Processing document 81/180 (45.0%)\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 36 tokens\n",
      "Processing chunk at position 0/36\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 33 tokens\n",
      "Processing chunk at position 0/33\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 47 tokens\n",
      "Processing chunk at position 0/47\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 41 tokens\n",
      "Processing chunk at position 0/41\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 25 tokens\n",
      "Processing chunk at position 0/25\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 33 tokens\n",
      "Processing chunk at position 0/33\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 53 tokens\n",
      "Processing chunk at position 0/53\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 123 tokens\n",
      "Processing chunk at position 0/123\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 15 tokens\n",
      "Processing chunk at position 0/15\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 28 tokens\n",
      "Processing chunk at position 0/28\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "‚è≥ Processing document 91/180 (50.6%)\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 42 tokens\n",
      "Processing chunk at position 0/42\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 35 tokens\n",
      "Processing chunk at position 0/35\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 24 tokens\n",
      "Processing chunk at position 0/24\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 79 tokens\n",
      "Processing chunk at position 0/79\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 51 tokens\n",
      "Processing chunk at position 0/51\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 147 tokens\n",
      "Processing chunk at position 0/147\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 82 tokens\n",
      "Processing chunk at position 0/82\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 76 tokens\n",
      "Processing chunk at position 0/76\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 63 tokens\n",
      "Processing chunk at position 0/63\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 34 tokens\n",
      "Processing chunk at position 0/34\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "‚è≥ Processing document 101/180 (56.1%)\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 64 tokens\n",
      "Processing chunk at position 0/64\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 23 tokens\n",
      "Processing chunk at position 0/23\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 13 tokens\n",
      "Processing chunk at position 0/13\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 26 tokens\n",
      "Processing chunk at position 0/26\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 37 tokens\n",
      "Processing chunk at position 0/37\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 30 tokens\n",
      "Processing chunk at position 0/30\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 46 tokens\n",
      "Processing chunk at position 0/46\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 45 tokens\n",
      "Processing chunk at position 0/45\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 60 tokens\n",
      "Processing chunk at position 0/60\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 38 tokens\n",
      "Processing chunk at position 0/38\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "‚è≥ Processing document 111/180 (61.7%)\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 30 tokens\n",
      "Processing chunk at position 0/30\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 50 tokens\n",
      "Processing chunk at position 0/50\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 27 tokens\n",
      "Processing chunk at position 0/27\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 18 tokens\n",
      "Processing chunk at position 0/18\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 53 tokens\n",
      "Processing chunk at position 0/53\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 128 tokens\n",
      "Processing chunk at position 0/128\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 72 tokens\n",
      "Processing chunk at position 0/72\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 63 tokens\n",
      "Processing chunk at position 0/63\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 59 tokens\n",
      "Processing chunk at position 0/59\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 114 tokens\n",
      "Processing chunk at position 0/114\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "‚è≥ Processing document 121/180 (67.2%)\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 90 tokens\n",
      "Processing chunk at position 0/90\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 28 tokens\n",
      "Processing chunk at position 0/28\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 150 tokens\n",
      "Processing chunk at position 0/150\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 36 tokens\n",
      "Processing chunk at position 0/36\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 40 tokens\n",
      "Processing chunk at position 0/40\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 34 tokens\n",
      "Processing chunk at position 0/34\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 88 tokens\n",
      "Processing chunk at position 0/88\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 81 tokens\n",
      "Processing chunk at position 0/81\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 61 tokens\n",
      "Processing chunk at position 0/61\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 76 tokens\n",
      "Processing chunk at position 0/76\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "‚è≥ Processing document 131/180 (72.8%)\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 65 tokens\n",
      "Processing chunk at position 0/65\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 53 tokens\n",
      "Processing chunk at position 0/53\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 20 tokens\n",
      "Processing chunk at position 0/20\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 67 tokens\n",
      "Processing chunk at position 0/67\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 18 tokens\n",
      "Processing chunk at position 0/18\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 19 tokens\n",
      "Processing chunk at position 0/19\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 34 tokens\n",
      "Processing chunk at position 0/34\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 65 tokens\n",
      "Processing chunk at position 0/65\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 32 tokens\n",
      "Processing chunk at position 0/32\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 30 tokens\n",
      "Processing chunk at position 0/30\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "‚è≥ Processing document 141/180 (78.3%)\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 90 tokens\n",
      "Processing chunk at position 0/90\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 83 tokens\n",
      "Processing chunk at position 0/83\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 21 tokens\n",
      "Processing chunk at position 0/21\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 40 tokens\n",
      "Processing chunk at position 0/40\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 40 tokens\n",
      "Processing chunk at position 0/40\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 20 tokens\n",
      "Processing chunk at position 0/20\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 26 tokens\n",
      "Processing chunk at position 0/26\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 38 tokens\n",
      "Processing chunk at position 0/38\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 20 tokens\n",
      "Processing chunk at position 0/20\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 35 tokens\n",
      "Processing chunk at position 0/35\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "‚è≥ Processing document 151/180 (83.9%)\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 28 tokens\n",
      "Processing chunk at position 0/28\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 27 tokens\n",
      "Processing chunk at position 0/27\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 27 tokens\n",
      "Processing chunk at position 0/27\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 46 tokens\n",
      "Processing chunk at position 0/46\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 23 tokens\n",
      "Processing chunk at position 0/23\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 26 tokens\n",
      "Processing chunk at position 0/26\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 38 tokens\n",
      "Processing chunk at position 0/38\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 19 tokens\n",
      "Processing chunk at position 0/19\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 58 tokens\n",
      "Processing chunk at position 0/58\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 22 tokens\n",
      "Processing chunk at position 0/22\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "‚è≥ Processing document 161/180 (89.4%)\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 38 tokens\n",
      "Processing chunk at position 0/38\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 50 tokens\n",
      "Processing chunk at position 0/50\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 27 tokens\n",
      "Processing chunk at position 0/27\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 33 tokens\n",
      "Processing chunk at position 0/33\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 67 tokens\n",
      "Processing chunk at position 0/67\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 47 tokens\n",
      "Processing chunk at position 0/47\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 80 tokens\n",
      "Processing chunk at position 0/80\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 146 tokens\n",
      "Processing chunk at position 0/146\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 104 tokens\n",
      "Processing chunk at position 0/104\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 33 tokens\n",
      "Processing chunk at position 0/33\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "‚è≥ Processing document 171/180 (95.0%)\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 47 tokens\n",
      "Processing chunk at position 0/47\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 33 tokens\n",
      "Processing chunk at position 0/33\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 170 tokens\n",
      "Processing chunk at position 0/170\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 104 tokens\n",
      "Processing chunk at position 0/104\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 133 tokens\n",
      "Processing chunk at position 0/133\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 72 tokens\n",
      "Processing chunk at position 0/72\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 116 tokens\n",
      "Processing chunk at position 0/116\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 29 tokens\n",
      "Processing chunk at position 0/29\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 134 tokens\n",
      "Processing chunk at position 0/134\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 79 tokens\n",
      "Processing chunk at position 0/79\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "\n",
      "‚úÖ Embedding completed in 2.61s\n",
      "üìä Statistics:\n",
      "  - Documents processed: 180\n",
      "  - Total tokens: 12374\n",
      "  - Total chunks created: 187\n",
      "  - Average chunks per document: 1.04\n",
      "  - Processing speed: 4744.85 tokens/second\n",
      "  - Documents with errors: 0\n",
      "  - Success rate: 100.00%\n",
      "  - Embedding memory usage: 0.27 MB\n",
      "Processed /app/data/parsed/EPO/EPRTBJV2025000024001001/EPW1B9/EP13899497W1B9/EP13899497W1B9.json, total documents: 187\n",
      "/app/data/parsed/EPO/EPRTBJV2025000024001001/EPW1B9/EP22169662W1B9/EP22169662W1B9.json\n",
      "üìã Starting document embedding process for 16 documents\n",
      "‚è≥ Processing document 1/16 (6.2%)\n",
      "\n",
      "üìÑ Document 1 Preview:\n",
      "  - Content: MONITORING OF A COMMERCIAL VEHICLE\n",
      "  - Metadata: {'doc_id': 'EP22169662B9W1', 'language': 'de', 'country': 'EP', 'doc_number': '4270346', 'application_number': '22169662.8', 'publication_date': '20250611', 'ipc_classes': ['G08B  21/02        20060101AFI20240902BHEP', 'G08B  29/18        20060101ALI20240902BHEP', 'G08B  31/00        20060101ALI20240902BHEP', 'G08G   1/00        20060101ALI20240902BHEP'], 'file': 'EP22169662W1B9.xml', 'section': 'title'}\n",
      "  - Token count: 5\n",
      "  - Calculated chunk size: 5\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 5 tokens\n",
      "Processing chunk at position 0/5\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Chunks created: 1 (in 0.00s)\n",
      "  - First chunk: monitoring of a commercial vehicle...\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "  - Embeddings generated: 1 vectors of shape (384,)\n",
      "  - Embedding time: 0.00s (0.0048s per chunk)\n",
      "\n",
      "üìÑ Document 2 Preview:\n",
      "  - Content: Verfahren ausgef√ºhrt durch einen Server (2), wobei das Verfahren umfasst: - Bereithalten (300) einer...\n",
      "  - Metadata: {'doc_id': 'EP22169662B9W1', 'language': 'de', 'country': 'EP', 'doc_number': '4270346', 'application_number': '22169662.8', 'publication_date': '20250611', 'ipc_classes': ['G08B  21/02        20060101AFI20240902BHEP', 'G08B  29/18        20060101ALI20240902BHEP', 'G08B  31/00        20060101ALI20240902BHEP', 'G08G   1/00        20060101ALI20240902BHEP'], 'file': 'EP22169662W1B9.xml', 'section': 'claim', 'claim_number': '0001'}\n",
      "  - Token count: 479\n",
      "  - Calculated chunk size: 240\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 479 tokens\n",
      "Processing chunk at position 0/479\n",
      "OVERLAP =  32\n",
      "Processing chunk at position 208/479\n",
      "OVERLAP =  32\n",
      "Processing chunk at position 416/479\n",
      "OVERLAP =  32\n",
      "Created 3 chunks\n",
      "y\n",
      "  - Chunks created: 3 (in 0.00s)\n",
      "  - First chunk: verfahren ausgefuhrt durch einen server ( 2 ), wob...\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "  - Embeddings generated: 3 vectors of shape (384,)\n",
      "  - Embedding time: 0.06s (0.0202s per chunk)\n",
      "\n",
      "üìÑ Document 3 Preview:\n",
      "  - Content: Verfahren nach Anspruch 1, wobei Bestimmen, ob die zeitliche √úberf√§lligkeit ein zu erwartendes Ereig...\n",
      "  - Metadata: {'doc_id': 'EP22169662B9W1', 'language': 'de', 'country': 'EP', 'doc_number': '4270346', 'application_number': '22169662.8', 'publication_date': '20250611', 'ipc_classes': ['G08B  21/02        20060101AFI20240902BHEP', 'G08B  29/18        20060101ALI20240902BHEP', 'G08B  31/00        20060101ALI20240902BHEP', 'G08G   1/00        20060101ALI20240902BHEP'], 'file': 'EP22169662W1B9.xml', 'section': 'claim', 'claim_number': '0002'}\n",
      "  - Token count: 149\n",
      "  - Calculated chunk size: 149\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 149 tokens\n",
      "Processing chunk at position 0/149\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Chunks created: 1 (in 0.00s)\n",
      "  - First chunk: verfahren nach anspruch 1, wobei bestimmen, ob die...\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "  - Embeddings generated: 1 vectors of shape (384,)\n",
      "  - Embedding time: 0.03s (0.0251s per chunk)\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 98 tokens\n",
      "Processing chunk at position 0/98\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 142 tokens\n",
      "Processing chunk at position 0/142\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 148 tokens\n",
      "Processing chunk at position 0/148\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 83 tokens\n",
      "Processing chunk at position 0/83\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 103 tokens\n",
      "Processing chunk at position 0/103\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 92 tokens\n",
      "Processing chunk at position 0/92\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 94 tokens\n",
      "Processing chunk at position 0/94\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "‚è≥ Processing document 11/16 (68.8%)\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 60 tokens\n",
      "Processing chunk at position 0/60\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 62 tokens\n",
      "Processing chunk at position 0/62\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 69 tokens\n",
      "Processing chunk at position 0/69\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 43 tokens\n",
      "Processing chunk at position 0/43\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 51 tokens\n",
      "Processing chunk at position 0/51\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 84 tokens\n",
      "Processing chunk at position 0/84\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "\n",
      "‚úÖ Embedding completed in 0.30s\n",
      "üìä Statistics:\n",
      "  - Documents processed: 16\n",
      "  - Total tokens: 1762\n",
      "  - Total chunks created: 18\n",
      "  - Average chunks per document: 1.12\n",
      "  - Processing speed: 5966.62 tokens/second\n",
      "  - Documents with errors: 0\n",
      "  - Success rate: 100.00%\n",
      "  - Embedding memory usage: 0.03 MB\n",
      "Processed /app/data/parsed/EPO/EPRTBJV2025000024001001/EPW1B9/EP22169662W1B9/EP22169662W1B9.json, total documents: 205\n",
      "/app/data/parsed/EPO/EPRTBJV2025000024001001/EPW1A8/EP23166881W1A8/EP23166881W1A8.json\n",
      "üìã Starting document embedding process for 2 documents\n",
      "‚è≥ Processing document 1/2 (50.0%)\n",
      "\n",
      "üìÑ Document 1 Preview:\n",
      "  - Content: SHAMPOO APPLICATOR AND REMOVER FOR CLEANSING HAIR TO NEAR DRY CONDITIONS\n",
      "  - Metadata: {'doc_id': 'EP23166881A8W1', 'language': 'en', 'country': 'EP', 'doc_number': '4223176', 'application_number': '23166881.5', 'publication_date': '20250611', 'ipc_classes': ['A45D  19/00        20060101AFI20230704BHEP', 'A45D  19/02        20060101ALI20230704BHEP', 'A45D  19/16        20060101ALI20230704BHEP', 'A45D  24/22        20060101ALI20230704BHEP', 'A45D  24/30        20060101ALI20230704BHEP', 'A45D  24/32        20060101ALI20230704BHEP'], 'file': 'EP23166881W1A8.xml', 'section': 'title'}\n",
      "  - Token count: 15\n",
      "  - Calculated chunk size: 15\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 15 tokens\n",
      "Processing chunk at position 0/15\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Chunks created: 1 (in 0.00s)\n",
      "  - First chunk: shampoo applicator and remover for cleansing hair ...\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "  - Embeddings generated: 1 vectors of shape (384,)\n",
      "  - Embedding time: 0.01s (0.0056s per chunk)\n",
      "\n",
      "üìÑ Document 2 Preview:\n",
      "  - Content: A device for application of hair formulation includes a body structure having one or more tines at a...\n",
      "  - Metadata: {'doc_id': 'EP23166881A8W1', 'language': 'en', 'country': 'EP', 'doc_number': '4223176', 'application_number': '23166881.5', 'publication_date': '20250611', 'ipc_classes': ['A45D  19/00        20060101AFI20230704BHEP', 'A45D  19/02        20060101ALI20230704BHEP', 'A45D  19/16        20060101ALI20230704BHEP', 'A45D  24/22        20060101ALI20230704BHEP', 'A45D  24/30        20060101ALI20230704BHEP', 'A45D  24/32        20060101ALI20230704BHEP'], 'file': 'EP23166881W1A8.xml', 'section': 'abstract'}\n",
      "  - Token count: 72\n",
      "  - Calculated chunk size: 72\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 72 tokens\n",
      "Processing chunk at position 0/72\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Chunks created: 1 (in 0.00s)\n",
      "  - First chunk: a device for application of hair formulation inclu...\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "  - Embeddings generated: 1 vectors of shape (384,)\n",
      "  - Embedding time: 0.01s (0.0119s per chunk)\n",
      "\n",
      "‚úÖ Embedding completed in 0.02s\n",
      "üìä Statistics:\n",
      "  - Documents processed: 2\n",
      "  - Total tokens: 87\n",
      "  - Total chunks created: 2\n",
      "  - Average chunks per document: 1.00\n",
      "  - Processing speed: 4732.63 tokens/second\n",
      "  - Documents with errors: 0\n",
      "  - Success rate: 100.00%\n",
      "  - Embedding memory usage: 0.00 MB\n",
      "Processed /app/data/parsed/EPO/EPRTBJV2025000024001001/EPW1A8/EP23166881W1A8/EP23166881W1A8.json, total documents: 207\n",
      "/app/data/parsed/EPO/EPRTBJV2025000024001001/EPW1A8/EP23205649W1A8/EP23205649W1A8.json\n",
      "üìã Starting document embedding process for 2 documents\n",
      "‚è≥ Processing document 1/2 (50.0%)\n",
      "\n",
      "üìÑ Document 1 Preview:\n",
      "  - Content: ENERGY STORE AND PROTECTION AGAINST INVERTED POLARITY FOR SAID ENERGY STORE\n",
      "  - Metadata: {'doc_id': 'EP23205649A8W1', 'language': 'de', 'country': 'EP', 'doc_number': '4546553', 'application_number': '23205649.9', 'publication_date': '20250611', 'ipc_classes': ['H01M  50/588       20210101AFI20240415BHEP', 'H01M  50/597       20210101ALI20240415BHEP', 'H01R  11/28        20060101ALI20240415BHEP'], 'file': 'EP23205649W1A8.xml', 'section': 'title'}\n",
      "  - Token count: 12\n",
      "  - Calculated chunk size: 12\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 12 tokens\n",
      "Processing chunk at position 0/12\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Chunks created: 1 (in 0.00s)\n",
      "  - First chunk: energy store and protection against inverted polar...\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "  - Embeddings generated: 1 vectors of shape (384,)\n",
      "  - Embedding time: 0.00s (0.0049s per chunk)\n",
      "\n",
      "üìÑ Document 2 Preview:\n",
      "  - Content: Es wird ein Energiespeicher (1) und ein Verpolungsschutz (1) f√ºr diesen Energiespeicher (2), insbeso...\n",
      "  - Metadata: {'doc_id': 'EP23205649A8W1', 'language': 'de', 'country': 'EP', 'doc_number': '4546553', 'application_number': '23205649.9', 'publication_date': '20250611', 'ipc_classes': ['H01M  50/588       20210101AFI20240415BHEP', 'H01M  50/597       20210101ALI20240415BHEP', 'H01R  11/28        20060101ALI20240415BHEP'], 'file': 'EP23205649W1A8.xml', 'section': 'abstract'}\n",
      "  - Token count: 255\n",
      "  - Calculated chunk size: 255\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 255 tokens\n",
      "Processing chunk at position 0/255\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Chunks created: 1 (in 0.00s)\n",
      "  - First chunk: es wird ein energiespeicher ( 1 ) und ein verpolun...\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "  - Embeddings generated: 1 vectors of shape (384,)\n",
      "  - Embedding time: 0.03s (0.0315s per chunk)\n",
      "\n",
      "‚úÖ Embedding completed in 0.04s\n",
      "üìä Statistics:\n",
      "  - Documents processed: 2\n",
      "  - Total tokens: 267\n",
      "  - Total chunks created: 2\n",
      "  - Average chunks per document: 1.00\n",
      "  - Processing speed: 7084.84 tokens/second\n",
      "  - Documents with errors: 0\n",
      "  - Success rate: 100.00%\n",
      "  - Embedding memory usage: 0.00 MB\n",
      "Processed /app/data/parsed/EPO/EPRTBJV2025000024001001/EPW1A8/EP23205649W1A8/EP23205649W1A8.json, total documents: 209\n",
      "/app/data/parsed/EPO/EPRTBJV2025000024001001/EPW1A9/EP23204356W1A9/EP23204356W1A9.json\n",
      "üìã Starting document embedding process for 16 documents\n",
      "‚è≥ Processing document 1/16 (6.2%)\n",
      "\n",
      "üìÑ Document 1 Preview:\n",
      "  - Content: SEMI-HOLLOW SELF-PIERCING RIVET AND JOINING METHOD USING SAME\n",
      "  - Metadata: {'doc_id': 'EP23204356A9W1', 'language': 'en', 'country': 'EP', 'doc_number': '4542062', 'application_number': '23204356.2', 'publication_date': '20250611', 'ipc_classes': ['F16B   5/04        20060101AFI20240223BHEP', 'B21J  15/02        20060101ALI20240223BHEP', 'B21J  15/14        20060101ALI20240223BHEP', 'B29C  65/56        20060101ALI20240223BHEP', 'B29C  65/00        20060101ALI20240223BHEP', 'F16B  19/06        20060101ALI20240223BHEP', 'F16B  19/08        20060101ALI20240223BHEP'], 'file': 'EP23204356W1A9.xml', 'section': 'title'}\n",
      "  - Token count: 13\n",
      "  - Calculated chunk size: 13\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 13 tokens\n",
      "Processing chunk at position 0/13\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Chunks created: 1 (in 0.00s)\n",
      "  - First chunk: semi - hollow self - piercing rivet and joining me...\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "  - Embeddings generated: 1 vectors of shape (384,)\n",
      "  - Embedding time: 0.01s (0.0055s per chunk)\n",
      "\n",
      "üìÑ Document 2 Preview:\n",
      "  - Content: A semi-tubular self-piercing rivet (10) comprising a head (12), and a shaft (14), wherein the shaft ...\n",
      "  - Metadata: {'doc_id': 'EP23204356A9W1', 'language': 'en', 'country': 'EP', 'doc_number': '4542062', 'application_number': '23204356.2', 'publication_date': '20250611', 'ipc_classes': ['F16B   5/04        20060101AFI20240223BHEP', 'B21J  15/02        20060101ALI20240223BHEP', 'B21J  15/14        20060101ALI20240223BHEP', 'B29C  65/56        20060101ALI20240223BHEP', 'B29C  65/00        20060101ALI20240223BHEP', 'F16B  19/06        20060101ALI20240223BHEP', 'F16B  19/08        20060101ALI20240223BHEP'], 'file': 'EP23204356W1A9.xml', 'section': 'abstract'}\n",
      "  - Token count: 219\n",
      "  - Calculated chunk size: 219\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 219 tokens\n",
      "Processing chunk at position 0/219\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Chunks created: 1 (in 0.00s)\n",
      "  - First chunk: a semi - tubular self - piercing rivet ( 10 ) comp...\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "  - Embeddings generated: 1 vectors of shape (384,)\n",
      "  - Embedding time: 0.03s (0.0263s per chunk)\n",
      "\n",
      "üìÑ Document 3 Preview:\n",
      "  - Content: A semi-tubular self-piercing rivet (10) comprising - a head (12), and - a shaft (14), and wherein th...\n",
      "  - Metadata: {'doc_id': 'EP23204356A9W1', 'language': 'en', 'country': 'EP', 'doc_number': '4542062', 'application_number': '23204356.2', 'publication_date': '20250611', 'ipc_classes': ['F16B   5/04        20060101AFI20240223BHEP', 'B21J  15/02        20060101ALI20240223BHEP', 'B21J  15/14        20060101ALI20240223BHEP', 'B29C  65/56        20060101ALI20240223BHEP', 'B29C  65/00        20060101ALI20240223BHEP', 'F16B  19/06        20060101ALI20240223BHEP', 'F16B  19/08        20060101ALI20240223BHEP'], 'file': 'EP23204356W1A9.xml', 'section': 'claim', 'claim_number': '0001'}\n",
      "  - Token count: 172\n",
      "  - Calculated chunk size: 172\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 172 tokens\n",
      "Processing chunk at position 0/172\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Chunks created: 1 (in 0.00s)\n",
      "  - First chunk: a semi - tubular self - piercing rivet ( 10 ) comp...\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "  - Embeddings generated: 1 vectors of shape (384,)\n",
      "  - Embedding time: 0.03s (0.0301s per chunk)\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 18 tokens\n",
      "Processing chunk at position 0/18\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 19 tokens\n",
      "Processing chunk at position 0/19\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 194 tokens\n",
      "Processing chunk at position 0/194\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 12 tokens\n",
      "Processing chunk at position 0/12\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 12 tokens\n",
      "Processing chunk at position 0/12\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 12 tokens\n",
      "Processing chunk at position 0/12\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 7 tokens\n",
      "Processing chunk at position 0/7\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "‚è≥ Processing document 11/16 (68.8%)\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 12 tokens\n",
      "Processing chunk at position 0/12\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 452 tokens\n",
      "Processing chunk at position 0/452\n",
      "OVERLAP =  32\n",
      "Processing chunk at position 194/452\n",
      "OVERLAP =  32\n",
      "Processing chunk at position 388/452\n",
      "OVERLAP =  32\n",
      "Created 3 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 65 tokens\n",
      "Processing chunk at position 0/65\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 76 tokens\n",
      "Processing chunk at position 0/76\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 63 tokens\n",
      "Processing chunk at position 0/63\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 32 tokens\n",
      "Processing chunk at position 0/32\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "\n",
      "‚úÖ Embedding completed in 0.23s\n",
      "üìä Statistics:\n",
      "  - Documents processed: 16\n",
      "  - Total tokens: 1378\n",
      "  - Total chunks created: 18\n",
      "  - Average chunks per document: 1.12\n",
      "  - Processing speed: 5942.53 tokens/second\n",
      "  - Documents with errors: 0\n",
      "  - Success rate: 100.00%\n",
      "  - Embedding memory usage: 0.03 MB\n",
      "Processed /app/data/parsed/EPO/EPRTBJV2025000024001001/EPW1A9/EP23204356W1A9/EP23204356W1A9.json, total documents: 227\n",
      "/app/data/parsed/EPO/EPRTBJV2025000024001001/EPW1A9/EP24185369W1A9/EP24185369W1A9.json\n",
      "üìã Starting document embedding process for 111 documents\n",
      "‚è≥ Processing document 1/111 (0.9%)\n",
      "\n",
      "üìÑ Document 1 Preview:\n",
      "  - Content: POLYIMIDE VARNISH FOR HIGH-FUNCTIONAL CONDUCTOR COATING AND POLYIMIDE COATING PREPARED THEREFROM\n",
      "  - Metadata: {'doc_id': 'EP24185369A9W1', 'language': 'en', 'country': 'EP', 'doc_number': '4484467', 'application_number': '24185369.6', 'publication_date': '20250611', 'ipc_classes': ['C08G  73/10        20060101AFI20241125BHEP', 'C08K   3/36        20060101ALI20241125BHEP', 'C08K   3/38        20060101ALI20241125BHEP', 'C08K   9/06        20060101ALI20241125BHEP', 'C09D 179/08        20060101ALI20241125BHEP'], 'file': 'EP24185369W1A9.xml', 'section': 'title'}\n",
      "  - Token count: 20\n",
      "  - Calculated chunk size: 20\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 20 tokens\n",
      "Processing chunk at position 0/20\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Chunks created: 1 (in 0.00s)\n",
      "  - First chunk: polyimide varnish for high - functional conductor ...\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "  - Embeddings generated: 1 vectors of shape (384,)\n",
      "  - Embedding time: 0.01s (0.0094s per chunk)\n",
      "\n",
      "üìÑ Document 2 Preview:\n",
      "  - Content: Provided is polyimide varnish comprising: a polyamic acid solution containing diamine monomer and di...\n",
      "  - Metadata: {'doc_id': 'EP24185369A9W1', 'language': 'en', 'country': 'EP', 'doc_number': '4484467', 'application_number': '24185369.6', 'publication_date': '20250611', 'ipc_classes': ['C08G  73/10        20060101AFI20241125BHEP', 'C08K   3/36        20060101ALI20241125BHEP', 'C08K   3/38        20060101ALI20241125BHEP', 'C08K   9/06        20060101ALI20241125BHEP', 'C09D 179/08        20060101ALI20241125BHEP'], 'file': 'EP24185369W1A9.xml', 'section': 'abstract'}\n",
      "  - Token count: 56\n",
      "  - Calculated chunk size: 56\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 56 tokens\n",
      "Processing chunk at position 0/56\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Chunks created: 1 (in 0.00s)\n",
      "  - First chunk: provided is polyimide varnish comprising : a polya...\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "  - Embeddings generated: 1 vectors of shape (384,)\n",
      "  - Embedding time: 0.01s (0.0105s per chunk)\n",
      "\n",
      "üìÑ Document 3 Preview:\n",
      "  - Content: Polyimide varnish comprising: a polyamic acid solution containing diamine monomer and dianhydride mo...\n",
      "  - Metadata: {'doc_id': 'EP24185369A9W1', 'language': 'en', 'country': 'EP', 'doc_number': '4484467', 'application_number': '24185369.6', 'publication_date': '20250611', 'ipc_classes': ['C08G  73/10        20060101AFI20241125BHEP', 'C08K   3/36        20060101ALI20241125BHEP', 'C08K   3/38        20060101ALI20241125BHEP', 'C08K   9/06        20060101ALI20241125BHEP', 'C09D 179/08        20060101ALI20241125BHEP'], 'file': 'EP24185369W1A9.xml', 'section': 'claim', 'claim_number': '0001'}\n",
      "  - Token count: 54\n",
      "  - Calculated chunk size: 54\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 54 tokens\n",
      "Processing chunk at position 0/54\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Chunks created: 1 (in 0.00s)\n",
      "  - First chunk: polyimide varnish comprising : a polyamic acid sol...\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "  - Embeddings generated: 1 vectors of shape (384,)\n",
      "  - Embedding time: 0.01s (0.0104s per chunk)\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 39 tokens\n",
      "Processing chunk at position 0/39\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 44 tokens\n",
      "Processing chunk at position 0/44\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 47 tokens\n",
      "Processing chunk at position 0/47\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 142 tokens\n",
      "Processing chunk at position 0/142\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 38 tokens\n",
      "Processing chunk at position 0/38\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 548 tokens\n",
      "Processing chunk at position 0/548\n",
      "OVERLAP =  32\n",
      "Processing chunk at position 242/548\n",
      "OVERLAP =  32\n",
      "Processing chunk at position 484/548\n",
      "OVERLAP =  32\n",
      "Created 3 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 1773 tokens\n",
      "Processing chunk at position 0/1773\n",
      "OVERLAP =  32\n",
      "Processing chunk at position 264/1773\n",
      "OVERLAP =  32\n",
      "Processing chunk at position 528/1773\n",
      "OVERLAP =  32\n",
      "Processing chunk at position 792/1773\n",
      "OVERLAP =  32\n",
      "Processing chunk at position 1056/1773\n",
      "OVERLAP =  32\n",
      "Processing chunk at position 1320/1773\n",
      "OVERLAP =  32\n",
      "Processing chunk at position 1584/1773\n",
      "OVERLAP =  32\n",
      "Created 7 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "‚è≥ Processing document 11/111 (9.9%)\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 27 tokens\n",
      "Processing chunk at position 0/27\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 23 tokens\n",
      "Processing chunk at position 0/23\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 42 tokens\n",
      "Processing chunk at position 0/42\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 40 tokens\n",
      "Processing chunk at position 0/40\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 58 tokens\n",
      "Processing chunk at position 0/58\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 50 tokens\n",
      "Processing chunk at position 0/50\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 35 tokens\n",
      "Processing chunk at position 0/35\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 70 tokens\n",
      "Processing chunk at position 0/70\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 103 tokens\n",
      "Processing chunk at position 0/103\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 134 tokens\n",
      "Processing chunk at position 0/134\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "‚è≥ Processing document 21/111 (18.9%)\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 105 tokens\n",
      "Processing chunk at position 0/105\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 110 tokens\n",
      "Processing chunk at position 0/110\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 66 tokens\n",
      "Processing chunk at position 0/66\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 56 tokens\n",
      "Processing chunk at position 0/56\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 51 tokens\n",
      "Processing chunk at position 0/51\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 47 tokens\n",
      "Processing chunk at position 0/47\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 79 tokens\n",
      "Processing chunk at position 0/79\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 118 tokens\n",
      "Processing chunk at position 0/118\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 73 tokens\n",
      "Processing chunk at position 0/73\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 48 tokens\n",
      "Processing chunk at position 0/48\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "‚è≥ Processing document 31/111 (27.9%)\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 85 tokens\n",
      "Processing chunk at position 0/85\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 62 tokens\n",
      "Processing chunk at position 0/62\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 110 tokens\n",
      "Processing chunk at position 0/110\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 28 tokens\n",
      "Processing chunk at position 0/28\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 58 tokens\n",
      "Processing chunk at position 0/58\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 1840 tokens\n",
      "Processing chunk at position 0/1840\n",
      "OVERLAP =  32\n",
      "Processing chunk at position 275/1840\n",
      "OVERLAP =  32\n",
      "Processing chunk at position 550/1840\n",
      "OVERLAP =  32\n",
      "Processing chunk at position 825/1840\n",
      "OVERLAP =  32\n",
      "Processing chunk at position 1100/1840\n",
      "OVERLAP =  32\n",
      "Processing chunk at position 1375/1840\n",
      "OVERLAP =  32\n",
      "Processing chunk at position 1650/1840\n",
      "OVERLAP =  32\n",
      "Created 7 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 553 tokens\n",
      "Processing chunk at position 0/553\n",
      "OVERLAP =  32\n",
      "Processing chunk at position 245/553\n",
      "OVERLAP =  32\n",
      "Processing chunk at position 490/553\n",
      "OVERLAP =  32\n",
      "Created 3 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 108 tokens\n",
      "Processing chunk at position 0/108\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 129 tokens\n",
      "Processing chunk at position 0/129\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 127 tokens\n",
      "Processing chunk at position 0/127\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "‚è≥ Processing document 41/111 (36.9%)\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 56 tokens\n",
      "Processing chunk at position 0/56\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 56 tokens\n",
      "Processing chunk at position 0/56\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 176 tokens\n",
      "Processing chunk at position 0/176\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 36 tokens\n",
      "Processing chunk at position 0/36\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 14 tokens\n",
      "Processing chunk at position 0/14\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 159 tokens\n",
      "Processing chunk at position 0/159\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 23 tokens\n",
      "Processing chunk at position 0/23\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 64 tokens\n",
      "Processing chunk at position 0/64\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 90 tokens\n",
      "Processing chunk at position 0/90\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 165 tokens\n",
      "Processing chunk at position 0/165\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "‚è≥ Processing document 51/111 (45.9%)\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 72 tokens\n",
      "Processing chunk at position 0/72\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 21 tokens\n",
      "Processing chunk at position 0/21\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 179 tokens\n",
      "Processing chunk at position 0/179\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 61 tokens\n",
      "Processing chunk at position 0/61\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 72 tokens\n",
      "Processing chunk at position 0/72\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 30 tokens\n",
      "Processing chunk at position 0/30\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 180 tokens\n",
      "Processing chunk at position 0/180\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 82 tokens\n",
      "Processing chunk at position 0/82\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 49 tokens\n",
      "Processing chunk at position 0/49\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 260 tokens\n",
      "Processing chunk at position 0/260\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "‚è≥ Processing document 61/111 (55.0%)\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 228 tokens\n",
      "Processing chunk at position 0/228\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 142 tokens\n",
      "Processing chunk at position 0/142\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 78 tokens\n",
      "Processing chunk at position 0/78\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 67 tokens\n",
      "Processing chunk at position 0/67\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 107 tokens\n",
      "Processing chunk at position 0/107\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 87 tokens\n",
      "Processing chunk at position 0/87\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 181 tokens\n",
      "Processing chunk at position 0/181\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 72 tokens\n",
      "Processing chunk at position 0/72\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 125 tokens\n",
      "Processing chunk at position 0/125\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 55 tokens\n",
      "Processing chunk at position 0/55\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "‚è≥ Processing document 71/111 (64.0%)\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 99 tokens\n",
      "Processing chunk at position 0/99\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 125 tokens\n",
      "Processing chunk at position 0/125\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 102 tokens\n",
      "Processing chunk at position 0/102\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 194 tokens\n",
      "Processing chunk at position 0/194\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 135 tokens\n",
      "Processing chunk at position 0/135\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 135 tokens\n",
      "Processing chunk at position 0/135\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 131 tokens\n",
      "Processing chunk at position 0/131\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 31 tokens\n",
      "Processing chunk at position 0/31\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 47 tokens\n",
      "Processing chunk at position 0/47\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 181 tokens\n",
      "Processing chunk at position 0/181\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "‚è≥ Processing document 81/111 (73.0%)\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 24 tokens\n",
      "Processing chunk at position 0/24\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 83 tokens\n",
      "Processing chunk at position 0/83\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 31 tokens\n",
      "Processing chunk at position 0/31\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 46 tokens\n",
      "Processing chunk at position 0/46\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 134 tokens\n",
      "Processing chunk at position 0/134\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 141 tokens\n",
      "Processing chunk at position 0/141\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 97 tokens\n",
      "Processing chunk at position 0/97\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 95 tokens\n",
      "Processing chunk at position 0/95\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 98 tokens\n",
      "Processing chunk at position 0/98\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 125 tokens\n",
      "Processing chunk at position 0/125\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "‚è≥ Processing document 91/111 (82.0%)\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 151 tokens\n",
      "Processing chunk at position 0/151\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 69 tokens\n",
      "Processing chunk at position 0/69\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 65 tokens\n",
      "Processing chunk at position 0/65\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 65 tokens\n",
      "Processing chunk at position 0/65\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 208 tokens\n",
      "Processing chunk at position 0/208\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 8 tokens\n",
      "Processing chunk at position 0/8\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 57 tokens\n",
      "Processing chunk at position 0/57\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 104 tokens\n",
      "Processing chunk at position 0/104\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 70 tokens\n",
      "Processing chunk at position 0/70\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 144 tokens\n",
      "Processing chunk at position 0/144\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "‚è≥ Processing document 101/111 (91.0%)\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 184 tokens\n",
      "Processing chunk at position 0/184\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 121 tokens\n",
      "Processing chunk at position 0/121\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 74 tokens\n",
      "Processing chunk at position 0/74\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 90 tokens\n",
      "Processing chunk at position 0/90\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 43 tokens\n",
      "Processing chunk at position 0/43\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 48 tokens\n",
      "Processing chunk at position 0/48\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 86 tokens\n",
      "Processing chunk at position 0/86\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 68 tokens\n",
      "Processing chunk at position 0/68\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 41 tokens\n",
      "Processing chunk at position 0/41\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 27 tokens\n",
      "Processing chunk at position 0/27\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "‚è≥ Processing document 111/111 (100.0%)\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 96 tokens\n",
      "Processing chunk at position 0/96\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "\n",
      "‚úÖ Embedding completed in 2.68s\n",
      "üìä Statistics:\n",
      "  - Documents processed: 111\n",
      "  - Total tokens: 14056\n",
      "  - Total chunks created: 127\n",
      "  - Average chunks per document: 1.14\n",
      "  - Processing speed: 5243.62 tokens/second\n",
      "  - Documents with errors: 0\n",
      "  - Success rate: 100.00%\n",
      "  - Embedding memory usage: 0.19 MB\n",
      "Processed /app/data/parsed/EPO/EPRTBJV2025000024001001/EPW1A9/EP24185369W1A9/EP24185369W1A9.json, total documents: 354\n",
      "/app/data/parsed/EPO/EPRTBJV2025000024001001/EPW1B8/EP18784075W1B8/EP18784075W1B8.json\n",
      "üìã Starting document embedding process for 1 documents\n",
      "‚è≥ Processing document 1/1 (100.0%)\n",
      "\n",
      "üìÑ Document 1 Preview:\n",
      "  - Content: THERMAL MANAGEMENT OF PHOTON-COUNTING DETECTORS\n",
      "  - Metadata: {'doc_id': 'EP18784075B8W1', 'language': 'en', 'country': 'EP', 'doc_number': '3610300', 'application_number': '18784075.6', 'publication_date': '20250611', 'ipc_classes': ['G01T   1/24        20060101AFI20210114BHEP', 'A61B   6/03        20060101ALI20210114BHEP', 'G01N  23/04        20180101ALI20210114BHEP', 'G01N  23/083       20180101ALI20210114BHEP', 'G01T   1/36        20060101ALI20210114BHEP'], 'file': 'EP18784075W1B8.xml', 'section': 'title'}\n",
      "  - Token count: 7\n",
      "  - Calculated chunk size: 7\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 7 tokens\n",
      "Processing chunk at position 0/7\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Chunks created: 1 (in 0.00s)\n",
      "  - First chunk: thermal management of photon - counting detectors...\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "  - Embeddings generated: 1 vectors of shape (384,)\n",
      "  - Embedding time: 0.01s (0.0060s per chunk)\n",
      "\n",
      "‚úÖ Embedding completed in 0.01s\n",
      "üìä Statistics:\n",
      "  - Documents processed: 1\n",
      "  - Total tokens: 7\n",
      "  - Total chunks created: 1\n",
      "  - Average chunks per document: 1.00\n",
      "  - Processing speed: 1111.87 tokens/second\n",
      "  - Documents with errors: 0\n",
      "  - Success rate: 100.00%\n",
      "  - Embedding memory usage: 0.00 MB\n",
      "Processed /app/data/parsed/EPO/EPRTBJV2025000024001001/EPW1B8/EP18784075W1B8/EP18784075W1B8.json, total documents: 355\n",
      "/app/data/parsed/EPO/EPRTBJV2025000024001001/EPW1B8/EP22732225W1B8/EP22732225W1B8.json\n",
      "üìã Starting document embedding process for 1 documents\n",
      "‚è≥ Processing document 1/1 (100.0%)\n",
      "\n",
      "üìÑ Document 1 Preview:\n",
      "  - Content: BRACKET SYSTEM\n",
      "  - Metadata: {'doc_id': 'EP22732225B8W1', 'language': 'en', 'country': 'EP', 'doc_number': '4356033', 'application_number': '22732225.2', 'publication_date': '20250611', 'ipc_classes': ['F16L   3/13        20060101AFI20221223BHEP', 'F16B   2/22        20060101ALI20221223BHEP', 'H02G   3/32        20060101ALI20221223BHEP'], 'file': 'EP22732225W1B8.xml', 'section': 'title'}\n",
      "  - Token count: 2\n",
      "  - Calculated chunk size: 2\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 2 tokens\n",
      "Processing chunk at position 0/2\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Chunks created: 1 (in 0.00s)\n",
      "  - First chunk: bracket system...\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "  - Embeddings generated: 1 vectors of shape (384,)\n",
      "  - Embedding time: 0.00s (0.0049s per chunk)\n",
      "\n",
      "‚úÖ Embedding completed in 0.01s\n",
      "üìä Statistics:\n",
      "  - Documents processed: 1\n",
      "  - Total tokens: 2\n",
      "  - Total chunks created: 1\n",
      "  - Average chunks per document: 1.00\n",
      "  - Processing speed: 395.78 tokens/second\n",
      "  - Documents with errors: 0\n",
      "  - Success rate: 100.00%\n",
      "  - Embedding memory usage: 0.00 MB\n",
      "Processed /app/data/parsed/EPO/EPRTBJV2025000024001001/EPW1B8/EP22732225W1B8/EP22732225W1B8.json, total documents: 356\n",
      "/app/data/parsed/EPO/EPRTBJV2025000024001001/EPW1B8/EP21207842W1B8/EP21207842W1B8.json\n",
      "üìã Starting document embedding process for 1 documents\n",
      "‚è≥ Processing document 1/1 (100.0%)\n",
      "\n",
      "üìÑ Document 1 Preview:\n",
      "  - Content: REFRACTIVE INDEX SENSOR DEVICE\n",
      "  - Metadata: {'doc_id': 'EP21207842B8W1', 'language': 'en', 'country': 'EP', 'doc_number': '4180796', 'application_number': '21207842.2', 'publication_date': '20250611', 'ipc_classes': ['G01N  21/41        20060101AFI20220425BHEP', 'G01N  21/552       20140101ALI20220425BHEP'], 'file': 'EP21207842W1B8.xml', 'section': 'title'}\n",
      "  - Token count: 5\n",
      "  - Calculated chunk size: 5\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 5 tokens\n",
      "Processing chunk at position 0/5\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Chunks created: 1 (in 0.00s)\n",
      "  - First chunk: refractive index sensor device...\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "  - Embeddings generated: 1 vectors of shape (384,)\n",
      "  - Embedding time: 0.01s (0.0063s per chunk)\n",
      "\n",
      "‚úÖ Embedding completed in 0.01s\n",
      "üìä Statistics:\n",
      "  - Documents processed: 1\n",
      "  - Total tokens: 5\n",
      "  - Total chunks created: 1\n",
      "  - Average chunks per document: 1.00\n",
      "  - Processing speed: 770.56 tokens/second\n",
      "  - Documents with errors: 0\n",
      "  - Success rate: 100.00%\n",
      "  - Embedding memory usage: 0.00 MB\n",
      "Processed /app/data/parsed/EPO/EPRTBJV2025000024001001/EPW1B8/EP21207842W1B8/EP21207842W1B8.json, total documents: 357\n",
      "/app/data/parsed/EPO/EPRTBJV2025000024001001/EPW1B8/EP21728115W1B8/EP21728115W1B8.json\n",
      "üìã Starting document embedding process for 1 documents\n",
      "‚è≥ Processing document 1/1 (100.0%)\n",
      "\n",
      "üìÑ Document 1 Preview:\n",
      "  - Content: SUPERFOOD FOR PROLONGING THE LONGEVITY OF AN INDIVIDUAL\n",
      "  - Metadata: {'doc_id': 'EP21728115B8W1', 'language': 'en', 'country': 'EP', 'doc_number': '4142762', 'application_number': '21728115.3', 'publication_date': '20250611', 'ipc_classes': ['A61K  36/185       20060101AFI20211105BHEP', 'A61K  36/31        20060101ALI20211105BHEP', 'A61K  36/52        20060101ALI20211105BHEP', 'A61K  36/54        20060101ALI20211105BHEP', 'A61K  36/73        20060101ALI20211105BHEP', 'A61K  36/889       20060101ALI20211105BHEP', 'A61P  39/00        20060101ALI20211105BHEP'], 'file': 'EP21728115W1B8.xml', 'section': 'title'}\n",
      "  - Token count: 12\n",
      "  - Calculated chunk size: 12\n",
      "x\n",
      "Starting tokenization...\n",
      "Tokenized text length: 12 tokens\n",
      "Processing chunk at position 0/12\n",
      "OVERLAP =  0\n",
      "Created 1 chunks\n",
      "y\n",
      "  - Chunks created: 1 (in 0.00s)\n",
      "  - First chunk: superfood for prolonging the longevity of an indiv...\n",
      "  - Processing batch 1/1\n",
      "‚úì Embedding completed\n",
      "x\n",
      "  - Embeddings generated: 1 vectors of shape (384,)\n",
      "  - Embedding time: 0.03s (0.0291s per chunk)\n",
      "\n",
      "‚úÖ Embedding completed in 0.03s\n",
      "üìä Statistics:\n",
      "  - Documents processed: 1\n",
      "  - Total tokens: 12\n",
      "  - Total chunks created: 1\n",
      "  - Average chunks per document: 1.00\n",
      "  - Processing speed: 409.23 tokens/second\n",
      "  - Documents with errors: 0\n",
      "  - Success rate: 100.00%\n",
      "  - Embedding memory usage: 0.00 MB\n",
      "Processed /app/data/parsed/EPO/EPRTBJV2025000024001001/EPW1B8/EP21728115W1B8/EP21728115W1B8.json, total documents: 358\n",
      "Total embedded chunks: 358\n",
      "\n",
      "Document 1:\n",
      "Text: audio signal encoder...\n",
      "Embedding shape: (384,)\n",
      "Metadata: {'doc_id': 'EP13899497B9W1', 'language': 'en', 'country': 'EP', 'doc_number': '3084761', 'application_number': '13899497.5', 'publication_date': '20250611', 'ipc_classes': ['G10L  19/038       20130101AFI20170426BHEP', 'G10L  19/07        20130101ALI20170426BHEP'], 'file': 'EP13899497W1B9.xml', 'section': 'title', 'chunk_index': 0, 'total_chunks': 1, 'source_doc_idx': 0}\n",
      "\n",
      "Document 2:\n",
      "Text: a processor - implemented method for encoding at least one audio signal, wherein the method comprise...\n",
      "Embedding shape: (384,)\n",
      "Metadata: {'doc_id': 'EP13899497B9W1', 'language': 'en', 'country': 'EP', 'doc_number': '3084761', 'application_number': '13899497.5', 'publication_date': '20250611', 'ipc_classes': ['G10L  19/038       20130101AFI20170426BHEP', 'G10L  19/07        20130101ALI20170426BHEP'], 'file': 'EP13899497W1B9.xml', 'section': 'claim', 'claim_number': '0001', 'chunk_index': 0, 'total_chunks': 3, 'source_doc_idx': 1}\n",
      "\n",
      "Document 3:\n",
      "Text: is of non - zero parity and when the number of minus signs of the components of the single vector of...\n",
      "Embedding shape: (384,)\n",
      "Metadata: {'doc_id': 'EP13899497B9W1', 'language': 'en', 'country': 'EP', 'doc_number': '3084761', 'application_number': '13899497.5', 'publication_date': '20250611', 'ipc_classes': ['G10L  19/038       20130101AFI20170426BHEP', 'G10L  19/07        20130101ALI20170426BHEP'], 'file': 'EP13899497W1B9.xml', 'section': 'claim', 'claim_number': '0001', 'chunk_index': 1, 'total_chunks': 3, 'source_doc_idx': 1}\n"
     ]
    }
   ],
   "source": [
    "for file_path in file_list:\n",
    "    print(file_path)\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "            # Extract documents (returns list of Document objects)\n",
    "            docs = extract_documents(data)\n",
    "            \n",
    "            # Process each document with metadata preservation\n",
    "            chunked_docs = embed_documents_with_metadata(docs)\n",
    "            all_documents.extend(chunked_docs)\n",
    "            \n",
    "        print(f\"Processed {file_path}, total documents: {len(all_documents)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {str(e)}\")\n",
    "\n",
    "print(f\"Total embedded chunks: {len(all_documents)}\")\n",
    "\n",
    "# Example of examining the first few documents\n",
    "for i, doc in enumerate(all_documents[:3]):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(f\"Text: {doc['text'][:100]}...\")\n",
    "    print(f\"Embedding shape: {doc['embedding'].shape}\")\n",
    "    print(f\"Metadata: {doc['metadata']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67990a4c-d10f-422d-b1ca-87e7f7d95de5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'squared ; determining the best leader class associated with the single potential code vector which generates the smallest associated distance ; and sorting components of the best leader class by the reverse ordering of the descending order of absolute values of the components of the single vector of parameters to generate an output lattice - quantized vector.',\n",
       " 'embedding': array([-5.95339723e-02,  5.56145906e-02, -5.51404022e-02, -3.64771783e-02,\n",
       "        -3.81343700e-02,  4.62850481e-02, -2.59227213e-02, -1.80207603e-02,\n",
       "        -1.10133621e-03, -8.75405967e-04,  1.88852716e-02,  3.99518460e-02,\n",
       "         3.90190408e-02, -1.90947379e-03, -9.95191932e-02, -7.86234345e-03,\n",
       "         1.70263350e-02,  9.65642035e-02, -6.24190159e-02, -4.64931428e-02,\n",
       "         6.05621003e-02, -7.02885091e-02, -2.86987349e-02,  1.04361326e-02,\n",
       "         6.36881143e-02, -2.62317099e-02,  4.23041061e-02,  5.52301221e-02,\n",
       "         9.72821340e-02, -8.66331086e-02,  1.62349530e-02,  2.15308517e-02,\n",
       "         9.86363366e-02,  5.77216223e-02, -9.02646109e-02,  7.37051070e-02,\n",
       "         4.94087748e-02, -5.86319529e-03, -3.80225945e-04,  7.89943784e-02,\n",
       "         1.42111721e-05,  3.39677893e-02,  1.50252301e-02, -1.14016794e-03,\n",
       "         1.84525677e-03,  2.67674886e-02,  3.34870033e-02, -3.97063754e-02,\n",
       "        -1.17530458e-01, -1.20492667e-01, -3.56696099e-02,  5.56953028e-02,\n",
       "        -6.86070174e-02,  3.68820466e-02, -1.39246574e-02, -4.38921899e-02,\n",
       "         1.98889021e-02, -7.87802935e-02,  2.75900774e-03, -2.50638016e-02,\n",
       "        -2.36980077e-02, -2.91234162e-02,  5.62011544e-03, -6.74771564e-03,\n",
       "         4.90664206e-02, -1.44342333e-02, -1.29769892e-02,  7.45801181e-02,\n",
       "        -2.09347927e-03,  6.73516020e-02,  3.10256295e-02, -5.20613268e-02,\n",
       "        -1.13277420e-01,  7.92612284e-02,  6.16943054e-02,  9.96665563e-03,\n",
       "         4.23038043e-02, -8.31929035e-03, -1.70332473e-02,  1.95989199e-02,\n",
       "        -3.71622555e-02,  2.62305457e-02, -6.07438423e-02,  5.70833609e-02,\n",
       "         6.36579767e-02,  1.12284049e-02, -2.32338365e-02, -1.64646306e-03,\n",
       "         4.88866903e-02,  1.22613106e-02,  1.19308718e-02,  4.91981953e-02,\n",
       "         1.54204173e-02,  6.76135570e-02, -5.18689640e-02,  6.88100918e-05,\n",
       "         1.15195170e-01, -4.96141948e-02,  4.22282256e-02,  5.28929792e-02,\n",
       "         6.92851320e-02, -2.20115893e-04, -7.32846837e-03, -7.69551471e-02,\n",
       "         6.30370304e-02, -4.58634794e-02,  6.82794154e-02,  2.54410561e-02,\n",
       "         3.20094861e-02, -9.63684469e-02, -2.18656864e-02, -5.71544096e-02,\n",
       "        -7.23527670e-02, -2.98434170e-03, -2.19835807e-03, -4.11072522e-02,\n",
       "         2.03994047e-02,  1.25936046e-01,  2.25203596e-02, -8.06030855e-02,\n",
       "         1.43430913e-02, -4.58552781e-03,  3.11890189e-02, -1.42448733e-03,\n",
       "         9.32808267e-04,  5.01757003e-02, -1.40361249e-01,  1.37003766e-33,\n",
       "         2.29854807e-02,  6.65197074e-02,  2.52864975e-02,  5.46036437e-02,\n",
       "         7.14757713e-03, -2.63559390e-02,  3.79983932e-02, -1.08272001e-01,\n",
       "         3.88178490e-02,  7.36028850e-02, -1.29329050e-02,  2.12799180e-02,\n",
       "         2.99549755e-02,  8.19536969e-02,  8.61601681e-02, -4.70263548e-02,\n",
       "        -3.46869230e-02,  1.39557244e-02, -2.64430419e-02, -3.41643393e-02,\n",
       "         7.46543407e-02, -6.37222966e-03, -3.14369332e-04, -7.55120069e-03,\n",
       "         4.97319736e-02, -1.70498323e-02,  2.58750897e-02, -7.49535635e-02,\n",
       "        -3.85578759e-02,  3.47461365e-02, -1.98384207e-02, -1.37736020e-03,\n",
       "        -4.82945070e-02,  1.01735722e-02,  5.56733496e-02,  2.32519656e-02,\n",
       "         1.22676753e-02, -2.33811121e-02,  1.94907505e-02, -8.94348323e-02,\n",
       "        -3.97334658e-02, -1.84697732e-02,  4.29639183e-02, -6.64254129e-02,\n",
       "         5.76806702e-02, -5.16439462e-03,  2.06205901e-02,  6.47111386e-02,\n",
       "         3.26060690e-02, -3.54606248e-02, -3.42914164e-02, -9.42268316e-03,\n",
       "         5.50243780e-02,  4.03912887e-02,  6.71794638e-02,  2.53044418e-03,\n",
       "         3.17921564e-02,  1.78728942e-02,  1.80505905e-02,  1.18101716e-01,\n",
       "        -3.86231169e-02,  3.32862325e-02, -3.63741852e-02, -1.01443585e-02,\n",
       "        -2.31878422e-02, -6.92466050e-02, -6.55462593e-02, -1.39033303e-01,\n",
       "         7.48404041e-02,  8.29700157e-02, -3.80522385e-02,  4.74968739e-02,\n",
       "        -4.60683368e-02, -8.18370953e-02,  5.51040992e-02, -5.67438900e-02,\n",
       "         3.35850678e-02, -7.46172741e-02,  6.17617974e-03, -6.47269711e-02,\n",
       "        -1.17490396e-01,  3.09242308e-02, -1.09893698e-02, -6.77808598e-02,\n",
       "        -2.39905156e-02, -1.01593770e-01, -6.29561301e-03, -5.94699271e-02,\n",
       "        -6.03041202e-02,  2.45003756e-02, -9.36079547e-02, -5.92263835e-03,\n",
       "         3.28879729e-02,  4.80900705e-02, -9.97937247e-02, -1.56012547e-33,\n",
       "        -5.36222793e-02,  1.00488789e-01,  3.72628774e-03,  6.28248826e-02,\n",
       "         1.61295049e-02, -3.10477614e-02, -1.19881621e-02, -3.03970985e-02,\n",
       "        -1.37482975e-02,  5.33956755e-03, -2.43279096e-02,  2.21762937e-02,\n",
       "         3.49688083e-02,  4.66296859e-02,  8.21187347e-02,  6.20310903e-02,\n",
       "         2.57299952e-02, -9.24968272e-02, -3.69066820e-02, -2.22134572e-02,\n",
       "         2.36952454e-02,  3.05224713e-02,  4.01534475e-02, -1.29830912e-02,\n",
       "         3.38159762e-02,  2.09549330e-02,  5.02782650e-02,  4.50096503e-02,\n",
       "        -1.17923087e-03, -2.70030778e-02,  5.23808748e-02, -7.70246834e-02,\n",
       "        -5.65511920e-02, -4.10029553e-02, -9.52665135e-02, -4.47857119e-02,\n",
       "         3.80616188e-02, -2.14462560e-02, -2.34054402e-02,  1.10383950e-01,\n",
       "        -7.45770376e-06,  1.16895325e-02, -2.46995222e-02,  2.73888893e-02,\n",
       "         3.21465842e-02,  4.96594124e-02,  4.38285545e-02, -6.02113607e-04,\n",
       "        -2.35464852e-02, -1.12366080e-02, -3.95875750e-03,  2.97862105e-02,\n",
       "        -7.22895563e-03,  1.06336817e-01, -1.70359015e-02, -3.42086918e-04,\n",
       "        -6.36052638e-02,  4.75271903e-02,  5.70564866e-02,  3.90496776e-02,\n",
       "         1.21747777e-02, -9.09607038e-02,  5.66071197e-02,  3.15680020e-02,\n",
       "        -5.75619750e-02, -4.71633226e-02, -3.63074727e-02,  5.02204113e-02,\n",
       "        -5.33854328e-02,  7.20884800e-02, -1.57371648e-02, -1.88775212e-02,\n",
       "        -7.91545864e-03, -1.52157750e-02, -6.61301538e-02, -5.42459935e-02,\n",
       "        -5.61179966e-02,  3.27006206e-02,  8.06078408e-03, -3.69844213e-02,\n",
       "         9.26197972e-03,  7.19360188e-02,  3.31356637e-02, -1.91440284e-02,\n",
       "        -1.46284113e-02,  1.31586089e-03,  1.23256832e-01,  7.51541033e-02,\n",
       "         7.19518512e-02, -1.98978167e-02,  2.99420208e-02, -5.37599111e-03,\n",
       "         5.03501995e-03,  5.69235459e-02, -1.21337119e-02, -3.30723395e-08,\n",
       "        -1.06254257e-01, -4.63002175e-02,  7.48127280e-03, -3.86828408e-02,\n",
       "         7.53936619e-02,  1.91686451e-02, -1.37869781e-03,  4.91531491e-02,\n",
       "         8.07740819e-03,  1.86624322e-02,  4.29537557e-02, -1.27596110e-02,\n",
       "        -1.01473972e-01, -1.72054693e-02,  6.44778311e-02,  4.95110527e-02,\n",
       "        -5.98129071e-02,  3.01159695e-02, -2.88515110e-02, -5.34033291e-02,\n",
       "         1.06762275e-02,  2.12721229e-02, -5.43430522e-02,  8.25501457e-02,\n",
       "        -9.37556252e-02, -4.56516305e-03, -1.20624742e-02, -4.38277908e-02,\n",
       "         3.92726883e-02,  6.43056780e-02, -7.20120501e-04,  6.02327101e-02,\n",
       "         6.34984002e-02, -5.79301827e-03, -3.40934321e-02,  4.31710631e-02,\n",
       "        -1.84356228e-01, -3.90786603e-02, -1.41786337e-02, -1.16239972e-02,\n",
       "        -5.56721389e-02, -8.00584734e-04, -2.21036002e-02,  1.26767950e-02,\n",
       "         1.05903789e-01,  5.68326004e-02,  1.25906207e-02, -1.94432382e-02,\n",
       "        -7.03466823e-03,  6.49275491e-03, -5.14047183e-02,  1.80161896e-03,\n",
       "        -5.36189303e-02, -5.58269322e-02, -1.60427857e-02,  3.27568986e-02,\n",
       "        -8.46587941e-02, -5.67369014e-02,  3.43273464e-03, -9.75299180e-02,\n",
       "         8.86076037e-03,  4.74317707e-02, -3.89991514e-02,  3.77094001e-02],\n",
       "       dtype=float32),\n",
       " 'metadata': {'doc_id': 'EP13899497B9W1',\n",
       "  'language': 'en',\n",
       "  'country': 'EP',\n",
       "  'doc_number': '3084761',\n",
       "  'application_number': '13899497.5',\n",
       "  'publication_date': '20250611',\n",
       "  'ipc_classes': ['G10L  19/038       20130101AFI20170426BHEP',\n",
       "   'G10L  19/07        20130101ALI20170426BHEP'],\n",
       "  'file': 'EP13899497W1B9.xml',\n",
       "  'section': 'claim',\n",
       "  'claim_number': '0001',\n",
       "  'chunk_index': 2,\n",
       "  'total_chunks': 3,\n",
       "  'source_doc_idx': 1}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_documents[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f1ecabef-4130-44b0-b0dc-da4c83d1071e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'a plurality of scale factors ; and applying the scale factor to the output lattice - quantized codevector.',\n",
       " 'embedding': array([ 3.75743280e-03, -2.34160628e-02, -7.15005025e-02, -1.68837663e-02,\n",
       "        -7.28141982e-03,  5.67938946e-02, -5.36125153e-02, -3.23771387e-02,\n",
       "        -1.56677645e-02, -2.38026790e-02,  3.40407491e-02, -2.54890732e-02,\n",
       "        -9.84252244e-03, -1.52092725e-02, -4.42799479e-02,  1.44423805e-02,\n",
       "        -7.25233257e-02,  8.47656131e-02, -3.66127118e-02,  2.49930541e-04,\n",
       "         4.34344076e-02, -5.25875725e-02, -6.99369460e-02, -2.12371945e-02,\n",
       "         9.17417407e-02,  4.19590548e-02, -1.71999000e-02,  3.53301018e-02,\n",
       "         9.13563818e-02, -2.94876713e-02,  2.89389789e-02,  7.54222944e-02,\n",
       "         1.14546843e-01,  7.84164220e-02, -2.17469465e-02,  2.62259934e-02,\n",
       "        -6.37745135e-04, -5.65790571e-02, -7.07795545e-02,  9.23098177e-02,\n",
       "         1.53841628e-02,  9.80806574e-02,  4.46067713e-02,  1.93364453e-02,\n",
       "        -8.35358351e-03, -2.96931732e-02,  1.62410717e-02,  8.07902834e-05,\n",
       "        -1.16550930e-01, -9.54138935e-02,  1.34650543e-02,  1.31780356e-01,\n",
       "        -6.97848499e-02,  4.71710935e-02, -1.02017252e-02, -1.02274582e-01,\n",
       "         6.93199709e-02, -5.51223271e-02, -1.17454259e-03, -4.43503447e-03,\n",
       "        -7.84963593e-02,  3.92585918e-02, -1.13672880e-03,  2.84425598e-02,\n",
       "         1.14425808e-01,  5.19519597e-02, -8.68652090e-02,  4.97369934e-03,\n",
       "        -4.12316434e-02,  3.94959040e-02, -5.72561808e-02, -9.11820978e-02,\n",
       "        -7.67507702e-02,  4.65031192e-02,  8.59677419e-02,  4.43644496e-03,\n",
       "        -1.91174783e-02, -2.65296344e-02,  3.03005651e-02,  2.29758974e-02,\n",
       "        -5.46620563e-02,  1.39319235e-02, -3.54903787e-02,  3.83605622e-02,\n",
       "         6.52659088e-02,  2.60621682e-02, -1.72395743e-02,  2.23244298e-02,\n",
       "         4.54171887e-03, -1.27690360e-02,  3.82143073e-02,  5.42909987e-02,\n",
       "         1.89869048e-03,  7.33170062e-02, -1.90625153e-02, -5.50747886e-02,\n",
       "         9.45822671e-02, -3.79495468e-04,  7.07145408e-02, -2.85679940e-03,\n",
       "         7.33508468e-02, -5.75439073e-02,  5.22809289e-02,  6.95832493e-03,\n",
       "        -3.64286043e-02, -2.64909919e-02,  8.08173046e-02,  1.02683820e-01,\n",
       "         4.09561582e-02, -7.88433328e-02, -1.03606144e-02, -4.94909175e-02,\n",
       "        -2.41406336e-02, -4.96999361e-02,  6.82065785e-02, -9.78022218e-02,\n",
       "         2.05497835e-02,  1.05983324e-01,  3.95499691e-02, -5.58460839e-02,\n",
       "         3.12882513e-02, -5.88437654e-02, -8.22975487e-02, -1.86738709e-03,\n",
       "        -3.11853029e-02,  2.52472423e-02, -1.17841378e-01, -1.00568988e-33,\n",
       "         2.72403825e-02,  8.33843183e-03,  1.06159979e-02,  3.83050391e-03,\n",
       "         1.66730843e-02,  1.53100872e-02, -3.27030830e-02, -9.18500200e-02,\n",
       "         1.72359738e-02,  7.63197988e-02,  9.12771840e-03,  7.62820169e-02,\n",
       "        -3.00020352e-02,  7.47232437e-02,  4.78309989e-02, -3.54353115e-02,\n",
       "        -7.65106902e-02, -1.66008796e-03, -2.80909277e-02, -2.56477222e-02,\n",
       "         5.84176891e-02, -1.82609744e-02, -4.69643297e-03,  4.02712636e-02,\n",
       "        -1.11477636e-02, -7.09190667e-02,  5.48914373e-02, -3.99641991e-02,\n",
       "        -1.00902319e-01,  2.42380314e-02, -4.37844582e-02, -4.31775041e-02,\n",
       "        -2.14370955e-02, -4.83548939e-02, -1.23654995e-02, -1.73623152e-02,\n",
       "         3.65256667e-02,  1.55750969e-02, -6.08838573e-02, -1.15426101e-01,\n",
       "        -1.93699542e-02,  7.35528534e-03,  2.37755869e-02, -7.18493089e-02,\n",
       "         5.30559868e-02,  4.26493672e-04,  6.77154437e-02,  2.19213851e-02,\n",
       "        -1.44607946e-02, -5.44042811e-02,  6.18590154e-02, -1.76518336e-02,\n",
       "         7.16982260e-02,  3.48635539e-02,  7.21997842e-02,  2.60065794e-02,\n",
       "         8.31957906e-02, -1.01678520e-02,  7.22141787e-02,  1.00221433e-01,\n",
       "        -9.79027376e-02,  2.13081483e-02,  2.16766330e-03, -7.20694615e-03,\n",
       "         2.05293428e-02,  2.75246333e-02, -3.68951410e-02, -1.01088397e-01,\n",
       "         4.05762158e-02,  5.12000658e-02, -1.55906398e-02,  8.12960565e-02,\n",
       "        -2.34979410e-02, -1.99610256e-02,  1.56753510e-02, -3.75082903e-02,\n",
       "         1.18471412e-02, -6.45038784e-02, -2.64021028e-02,  4.99045523e-03,\n",
       "        -1.15848795e-01,  2.21088585e-02, -9.20424331e-03, -3.93004380e-02,\n",
       "        -6.00343337e-03, -5.39878085e-02, -3.34769525e-02, -8.91347677e-02,\n",
       "        -1.12785563e-01,  4.93764766e-02, -6.68394044e-02, -5.81325591e-02,\n",
       "         7.36989751e-02, -1.48237795e-02, -9.26669389e-02,  3.71636511e-34,\n",
       "        -8.59797001e-02,  1.80379562e-02, -1.63597111e-02,  3.62938568e-02,\n",
       "        -1.63093396e-02, -6.42950311e-02,  3.61380987e-02,  1.56060783e-02,\n",
       "         7.60843307e-02, -2.52184793e-02, -9.39553697e-03, -2.50134878e-02,\n",
       "         8.57859664e-03,  2.73030475e-02,  8.35914835e-02,  4.22735363e-02,\n",
       "        -2.45971214e-02,  2.44087502e-02,  3.33011672e-02,  2.61348374e-02,\n",
       "         7.13259652e-02,  1.22608207e-02,  4.20393199e-02, -9.62004624e-03,\n",
       "        -7.00654229e-04, -1.21316677e-02, -2.63732895e-02,  3.75191607e-02,\n",
       "         3.47898789e-02, -1.00011259e-01,  4.37251255e-02, -7.39413276e-02,\n",
       "         1.81228351e-02, -1.32229505e-02, -1.73436087e-02, -4.72085401e-02,\n",
       "         3.65131125e-02, -3.78123522e-02, -3.98899354e-02,  5.17959967e-02,\n",
       "         2.48845965e-02,  3.96323949e-03, -4.10659574e-02,  3.18183415e-02,\n",
       "         7.54916146e-02,  2.13456526e-02,  5.52893020e-02, -1.15151135e-02,\n",
       "        -1.76727287e-02,  2.93413233e-02,  1.74088776e-02,  2.17929520e-02,\n",
       "         8.59863311e-03,  7.96073899e-02, -5.42150587e-02,  4.74314168e-02,\n",
       "        -3.51430923e-02,  3.10282446e-02,  9.05094761e-03, -7.40966387e-03,\n",
       "         1.74368452e-02, -8.85368809e-02,  7.33851045e-02, -9.66862962e-02,\n",
       "         9.63426568e-03, -6.43407330e-02,  1.32576237e-02,  2.77942866e-02,\n",
       "        -3.41002755e-02, -6.93801604e-03,  3.15238461e-02, -4.51847864e-03,\n",
       "         6.74573751e-03,  7.90625066e-03, -4.79230881e-02, -1.08202353e-01,\n",
       "        -5.63901849e-02,  8.58630333e-03,  2.69479863e-02,  5.28252758e-02,\n",
       "        -1.14581147e-02,  7.99781978e-02,  3.56375314e-02,  2.41505485e-02,\n",
       "         5.85004389e-02,  4.73334715e-02,  6.14157729e-02,  3.42378654e-02,\n",
       "         6.51850924e-02,  5.36533371e-02, -2.51910985e-02,  2.11756825e-02,\n",
       "         3.09643671e-02,  4.09506448e-02,  6.64332183e-04, -2.11161826e-08,\n",
       "        -6.69369400e-02, -2.14077942e-02, -5.20812497e-02, -7.66714290e-02,\n",
       "         2.16219332e-02, -1.39111644e-02,  4.22584228e-02, -1.13635194e-02,\n",
       "         1.27601586e-02, -9.01042670e-03,  1.67804107e-01, -3.24755646e-02,\n",
       "        -1.28500223e-01,  9.06275772e-03,  5.91388755e-02,  1.50278574e-02,\n",
       "        -3.47943716e-02, -2.79735178e-02, -2.08691172e-02, -2.74718925e-02,\n",
       "         8.93573463e-02,  7.25978240e-02, -5.95163889e-02,  7.63757527e-02,\n",
       "        -1.16175272e-01, -4.13943790e-02, -2.57879477e-02, -2.17228085e-02,\n",
       "         1.84721630e-02,  4.29521948e-02,  2.22281255e-02,  4.12228592e-02,\n",
       "         7.72781372e-02,  1.34633956e-02, -2.68931426e-02, -9.26329195e-03,\n",
       "        -6.69047087e-02,  6.75552525e-03,  6.33004308e-02, -2.32950598e-02,\n",
       "        -2.34951694e-02, -5.83088510e-02, -2.27373391e-02,  6.03333116e-03,\n",
       "         5.96266538e-02,  4.67894562e-02, -8.26816335e-02,  3.64997871e-02,\n",
       "        -1.11179836e-02,  3.58691625e-02, -5.26065426e-03,  2.66764890e-02,\n",
       "        -1.03544749e-01, -4.88990033e-03,  6.84418306e-02, -6.13468885e-02,\n",
       "        -4.10055332e-02, -2.68545672e-02, -9.57160164e-03, -3.68713513e-02,\n",
       "         9.67449509e-03,  7.33843539e-03,  1.06021110e-02,  7.99030811e-03],\n",
       "       dtype=float32),\n",
       " 'metadata': {'doc_id': 'EP13899497B9W1',\n",
       "  'language': 'en',\n",
       "  'country': 'EP',\n",
       "  'doc_number': '3084761',\n",
       "  'application_number': '13899497.5',\n",
       "  'publication_date': '20250611',\n",
       "  'ipc_classes': ['G10L  19/038       20130101AFI20170426BHEP',\n",
       "   'G10L  19/07        20130101ALI20170426BHEP'],\n",
       "  'file': 'EP13899497W1B9.xml',\n",
       "  'section': 'claim',\n",
       "  'claim_number': '0002',\n",
       "  'chunk_index': 2,\n",
       "  'total_chunks': 5,\n",
       "  'source_doc_idx': 2}}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_documents[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d015ec9b-3463-4736-81c1-e3d1331334dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
