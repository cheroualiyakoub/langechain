{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "bb133552-b468-428a-982d-e6362564085c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cl√© OpenRouter charg√©e depuis .env\n",
      "‚úÖ Setup Docker termin√©\n"
     ]
    }
   ],
   "source": [
    "# Setup pour environnement Docker - Version corrig√©e\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from typing import Dict, List, Optional, TypedDict, Annotated\n",
    "\n",
    "# Charger les variables d'environnement\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration avec ta cl√© OpenRouter\n",
    "openai_api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "base_url = \"https://openrouter.ai/api/v1\"\n",
    "\n",
    "if not openai_api_key:\n",
    "    print(\"‚ùå Erreur: OPENROUTER_API_KEY non trouv√©e dans .env\")\n",
    "else:\n",
    "    print(\"‚úÖ Cl√© OpenRouter charg√©e depuis .env\")\n",
    "    os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "    os.environ[\"OPENAI_API_BASE\"] = base_url\n",
    "\n",
    "print(\"‚úÖ Setup Docker termin√©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "5dda0121-3cfd-4905-9001-df8be488d80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    from langgraph.graph import StateGraph, END\n",
    "    from langchain_core.messages import HumanMessage, AIMessage, BaseMessage\n",
    "    from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "    from langchain_chroma import Chroma\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    from langchain.schema import Document\n",
    "    import chromadb\n",
    "    import langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "cb8c7303-c36b-4ff9-ad99-3d028e886d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "    from langchain.embeddings import HuggingFaceEmbeddings\n",
    "    \n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        model_kwargs={'device': 'cpu'}\n",
    "    )\n",
    "    \n",
    "    # LLM avec OpenRouter (√ßa marche)\n",
    "    llm = ChatOpenAI(\n",
    "        model=\"openai/gpt-4.1-nano\",\n",
    "        temperature=0.1,\n",
    "        openai_api_key=openai_api_key,\n",
    "        openai_api_base=base_url\n",
    "    )\n",
    "    \n",
    "    # Chroma avec embeddings locaux\n",
    "    vector_store = Chroma(\n",
    "        collection_name=\"patent_collection\",\n",
    "        embedding_function=embeddings,\n",
    "        persist_directory=\"./chroma_db\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "282d3b57-f023-4ee0-adc6-e70b3ab4d8e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichiers JSON import√©s :\n",
      "- EP13899497W1B9.json\n",
      "‚úÖ Fichier charg√© : EP13899497W1B9.json\n",
      "\n",
      "üìÑ Total de documents charg√©s : 9\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from langchain.schema import Document\n",
    "\n",
    "TEMPLATE_DIR = \"./\"\n",
    "template_files = [f for f in os.listdir(TEMPLATE_DIR) if f.endswith(\".json\")]\n",
    "\n",
    "docs = []\n",
    "print(\"Fichiers JSON import√©s :\")\n",
    "for fname in template_files:\n",
    "    print(f\"- {fname}\")\n",
    "    full_path = os.path.join(TEMPLATE_DIR, fname)\n",
    "    try:\n",
    "        with open(full_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "            # Indexer chaque claim s√©par√©ment\n",
    "            for claim in data.get(\"claims\", []):\n",
    "                docs.append(\n",
    "                    Document(\n",
    "                        page_content=f\"Claim {claim.get('claim_number', '')}: {claim.get('text', '')}\",\n",
    "                        metadata={\"source\": fname, \"claim_number\": claim.get(\"claim_number\", \"\")}\n",
    "                    )\n",
    "                )\n",
    "            # (Optionnel) Indexer aussi l'abstract et le titre\n",
    "            abstract = data.get(\"bibliographic_data\", {}).get(\"abstract\")\n",
    "            if abstract:\n",
    "                docs.append(\n",
    "                    Document(\n",
    "                        page_content=f\"Abstract: {abstract}\",\n",
    "                        metadata={\"source\": fname, \"type\": \"abstract\"}\n",
    "                    )\n",
    "                )\n",
    "            title = data.get(\"bibliographic_data\", {}).get(\"title\", {}).get(\"en\")\n",
    "            if title:\n",
    "                docs.append(\n",
    "                    Document(\n",
    "                        page_content=f\"Title: {title}\",\n",
    "                        metadata={\"source\": fname, \"type\": \"title\"}\n",
    "                    )\n",
    "                )\n",
    "        print(f\"‚úÖ Fichier charg√© : {fname}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur lors du chargement de {fname} : {e}\")\n",
    "\n",
    "print(f\"\\nüìÑ Total de documents charg√©s : {len(docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "82c42745-3bb4-48cd-822d-ecf97840a0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Splitter et vectoriser tous les templates\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "split_docs = splitter.split_documents(docs)\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"patent_collection\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chroma_db\"\n",
    ")\n",
    "vector_store.add_documents(split_docs)\n",
    "\n",
    "# 3. Cr√©er un Retriever avec m√©moire de conversation (nouvelle m√©thode)\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "f4df05a6-dc66-4ac1-b970-1fe0694ed585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def print_blue_bold_animated(text):\n",
    "#     \"\"\"Affiche le texte en bleu gras avec animation lettre par lettre\"\"\"\n",
    "#     if not text or str(text).strip() == \"\":\n",
    "#         print(\"‚ö†Ô∏è R√©ponse vide re√ßue.\")\n",
    "#         return\n",
    "    \n",
    "#     html_start = '<span style=\"color:#1976d2; font-weight:bold; font-size:1.1em\">'\n",
    "#     html_end = '</span>'\n",
    "#     s = \"\"\n",
    "#     display_id = str(uuid.uuid4())\n",
    "    \n",
    "#     for c in str(text):\n",
    "#         s += c\n",
    "#         display(HTML(html_start + s + html_end), display_id=display_id, update=True)\n",
    "#         time.sleep(0.002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250e80b9-8d27-4991-b382-3f124eebc1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Pose ta question sur les templates (ou 'quit'):  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in RootListenersTracer.on_chain_end callback: KeyError('output')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R√©ponse: The provided patent document excerpt emphasizes that the embodiments described are illustrative and not exhaustive. It states that other embodiments and variations that are apparent to those skilled in the art, based on the principles and practices disclosed, are intended to fall within the scope of the present disclosure. This includes adaptations and uses that follow the general principles, incorporating common knowledge and conventional technical means not explicitly disclosed in the document. The scope of the patent is therefore intended to cover not only the specific embodiments described but also any variations and modifications that a person skilled in the art could reasonably derive from the disclosure.\n",
      "Sources: ['_template_EP16731796W1B8_data_6.json', '_template_EP16731796W1B8_data_6.json', '_template_EP16731796W1B8_data_6.json']\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Pose ta question sur les templates (ou 'quit'):  What is the content of claim number 0001?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in RootListenersTracer.on_chain_end callback: KeyError('output')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R√©ponse: The content of claim number 0001 is not explicitly provided in the available text.\n",
      "Sources: ['_template_EP16731796W1B8_data_6.json', 'EP13899497W1B9.json', 'EP13899497W1B9.json']\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Pose ta question sur les templates (ou 'quit'):  List all claims.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in RootListenersTracer.on_chain_end callback: KeyError('output')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R√©ponse: The document contains only one claim, which is:\n",
      "\n",
      "**Claim 0001:**  \n",
      "*The foregoing description has provided by way of exemplary and non-limiting examples a full and informative description of the exemplary embodiment of this invention. However, various modifications and adaptations may become apparent to those skilled in the relevant arts in view of the foregoing description, when read in conjunction with the accompanying drawings and the appended claims.*\n",
      "Sources: ['EP13899497W1B9.json', 'EP13899497W1B9.json', 'EP13899497W1B9.json']\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# Classe pour stocker l'historique des messages\n",
    "class InMemoryHistory(BaseChatMessageHistory):\n",
    "    def __init__(self):\n",
    "        self.messages = []\n",
    "    \n",
    "    def add_messages(self, messages):\n",
    "        self.messages.extend(messages)\n",
    "    \n",
    "    def clear(self):\n",
    "        self.messages = []\n",
    "\n",
    "# Cr√©er l'historique\n",
    "chat_history = InMemoryHistory()\n",
    "\n",
    "# Template de prompt avec historique\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a patent expert. Use the Sources that you have from patent documents to answer the user's question as clearly and precisely as possible. If the answer is not in the context, say so.\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"system\", \"Contexte: {context}\")\n",
    "])\n",
    "\n",
    "# Cr√©er les cha√Ænes\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "# Ajouter l'historique\n",
    "conversational_chain = RunnableWithMessageHistory(\n",
    "    retrieval_chain,\n",
    "    lambda session_id: chat_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\"\n",
    ")\n",
    "\n",
    "# 4. Boucle de chat avec m√©moire de conversation\n",
    "session_id = \"session_1\"\n",
    "while True:\n",
    "    question = input(\"Pose ta question sur les templates (ou 'quit'): \")\n",
    "    if question.lower() in [\"quit\", \"exit\",\"q\"]:\n",
    "        break\n",
    "    \n",
    "    result = conversational_chain.invoke(\n",
    "        {\"input\": question},\n",
    "        config={\"configurable\": {\"session_id\": session_id}}\n",
    "    )\n",
    "    print(\"R√©ponse:\", result[\"answer\"])\n",
    "    print(\"Sources:\", [doc.metadata.get(\"source\", \"Unknown\") for doc in result.get(\"context\", [])])\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fdb025-aba8-45c0-875b-3a72aaf41833",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527331b8-33ed-4e0b-b09b-402b203a2289",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
